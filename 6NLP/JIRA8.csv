Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Duplicate),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Lucene Fields),Custom field (Lucene Fields),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Severity),Custom field (Severity),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Test and Documentation Plan),Custom field (Testcase included),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Efficient compression of small to medium stored fields,LUCENE-4226,12599008,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,16/Jul/12 17:56,09/May/16 18:39,30/Sep/19 08:38,08/Oct/12 09:35,,,,,,,,,,,4.1,6.0,,,core/index,,,3,,,,"I've been doing some experiments with stored fields lately. It is very common for an index with stored fields enabled to have most of its space used by the .fdt index file. To prevent this .fdt file from growing too much, one option is to compress stored fields. Although compression works rather well for large fields, this is not the case for small fields and the compression ratio can be very close to 100%, even with efficient compression algorithms.

In order to improve the compression ratio for small fields, I've written a {{StoredFieldsFormat}} that compresses several documents in a single chunk of data. To see how it behaves in terms of document deserialization speed and compression ratio, I've run several tests with different index compression strategies on 100,000 docs from Mike's 1K Wikipedia articles (title and text were indexed and stored):
 - no compression,
 - docs compressed with deflate (compression level = 1),
 - docs compressed with deflate (compression level = 9),
 - docs compressed with Snappy,
 - using the compressing {{StoredFieldsFormat}} with deflate (level = 1) and chunks of 6 docs,
 - using the compressing {{StoredFieldsFormat}} with deflate (level = 9) and chunks of 6 docs,
 - using the compressing {{StoredFieldsFormat}} with Snappy and chunks of 6 docs.

For those who don't know Snappy, it is compression algorithm from Google which has very high compression ratios, but compresses and decompresses data very quickly.

{noformat}
Format           Compression ratio     IndexReader.document time
————————————————————————————————————————————————————————————————
uncompressed     100%                  100%
doc/deflate 1     59%                  616%
doc/deflate 9     58%                  595%
doc/snappy        80%                  129%
index/deflate 1   49%                  966%
index/deflate 9   46%                  938%
index/snappy      65%                  264%
{noformat}

(doc = doc-level compression, index = index-level compression)

I find it interesting because it allows to trade speed for space (with deflate, the .fdt file shrinks by a factor of 2, much better than with doc-level compression). One other interesting thing is that {{index/snappy}} is almost as compact as {{doc/deflate}} while it is more than 2x faster at retrieving documents from disk.

These tests have been done on a hot OS cache, which is the worst case for compressed fields (one can expect better results for formats that have a high compression ratio since they probably require fewer read/write operations from disk).",,,,,,,,,,,,,,,,"29/Aug/12 00:00;jpountz;CompressionBenchmark.java;https://issues.apache.org/jira/secure/attachment/12542867/CompressionBenchmark.java","16/Jul/12 18:05;jpountz;CompressionBenchmark.java;https://issues.apache.org/jira/secure/attachment/12536684/CompressionBenchmark.java","05/Oct/12 00:19;jpountz;LUCENE-4226.patch;https://issues.apache.org/jira/secure/attachment/12547856/LUCENE-4226.patch","04/Oct/12 20:36;jpountz;LUCENE-4226.patch;https://issues.apache.org/jira/secure/attachment/12547815/LUCENE-4226.patch","04/Oct/12 00:06;jpountz;LUCENE-4226.patch;https://issues.apache.org/jira/secure/attachment/12547638/LUCENE-4226.patch","03/Oct/12 22:09;jpountz;LUCENE-4226.patch;https://issues.apache.org/jira/secure/attachment/12547607/LUCENE-4226.patch","26/Sep/12 00:00;jpountz;LUCENE-4226.patch;https://issues.apache.org/jira/secure/attachment/12546615/LUCENE-4226.patch","09/Sep/12 23:54;jpountz;LUCENE-4226.patch;https://issues.apache.org/jira/secure/attachment/12544420/LUCENE-4226.patch","29/Aug/12 00:00;jpountz;LUCENE-4226.patch;https://issues.apache.org/jira/secure/attachment/12542866/LUCENE-4226.patch","16/Jul/12 18:05;jpountz;LUCENE-4226.patch;https://issues.apache.org/jira/secure/attachment/12536682/LUCENE-4226.patch","16/Jul/12 18:05;jpountz;SnappyCompressionAlgorithm.java;https://issues.apache.org/jira/secure/attachment/12536683/SnappyCompressionAlgorithm.java",,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,2012-08-29 07:16:29.339,,,false,,,,,,,,,,,,,,,240253,,,Fri May 10 10:34:12 UTC 2013,New,,,,,,,"0|i00yrz:",3567,,,,,,,,,"16/Jul/12 18:05;jpountz;Patch (applies against trunk and does not include the snappy codec).

See org.apache.lucene.codecs.compressing.CompressedStoredFieldsFormat javadocs for the format description.

CompressionBenchmark.java and SnappyCompressionAlgorithm.java are the source files I used to compute differences in compression ratio and speed. To run it, you will need snappy-java from http://code.google.com/p/snappy-java/.

The patch is currently only for testing purposes: it hasn't been tested well and duplicates code from Lucene40's {{StoredFieldFormat}}.","29/Aug/12 00:00;jpountz;New patch as well as the code I used to benchmark.

Documents are still compressed into chunks, but I removed the ability to select the compression algorithm on a per-field basis in order to make the patch simpler and to handle cross-field compression.

I also added an index in front of compressed data using packed ints, so that uncompressors can stop uncompressing when enough data has been uncompressed.

The JDK only includes a moderately fast compression algorithm (deflate), but for this kind of use-case, we would probably be more interested in fast compression and uncompression algorithms such as LZ4 (http://code.google.com/p/lz4/) or Snappy (http://code.google.com/p/snappy/). Since lucene-core has no dependency, I ported LZ4 to Java (included in the patch, see o.a.l.util.compress).

LZ4 has a very fast uncompressor and two compression modes :
 - fast scan, which looks for the last offset in the stream that has at least 4 common bytes (using a hash table) and adds a reference to it,
 - high compression, which looks for the last 256 offsets in the stream that have at least 4 common bytes, takes the one that has the longest common sequence, and then performs trade-offs between overlapping matches in order to improve the compression ratio.

(In case you are curious about LZ4, I did some benchmarking with other compression algorithms in http://blog.jpountz.net/post/28092106032/wow-lz4-is-fast, unfortunately the high-compression Java impl is not included in the benchmark.)

I ran a similar benchmark as for my first patch, but this time I only compressed and stored the 1kb text field (the title field being too small was unfair for document-level compression with deflate). Here are the results :

{noformat}
Format           Chunk size  Compression ratio     IndexReader.document time
————————————————————————————————————————————————————————————————————————————
uncompressed                               100%                         100%
doc/deflate 1                               58%                         579%
doc/deflate 9                               57%                         577%
index/deflate 1          4K                 50%                        1057%
index/deflate 9          4K                 48%                        1037%
index/lz4 scan           4K                 70%                         329%
index/lz4 hc             4K                 66%                         321%
index/deflate 1           1                 60%                         457%
index/deflate 9           1                 59%                         454%
index/lz4 scan            1                 81%                         171%
index/lz4 hc              1                 79%                         176%
{noformat}

NOTE: chunk size = 1 means that there was only one document in the chunk (there is a compress+flush every time the byte size of documents is >= the chunk size).

NOTE: these number have been computed with the whole index fitting in the I/O cache. The performance should be more in favor of the compressing formats as soon as the index does not fit in the I/O cache anymore.

There are still a few nocommits in the patch, but it should be easy to get rid of them. I'd be very happy to have some feedback. :-)","29/Aug/12 07:16;dweiss;Very cool. I skimmed through the patch, didn't look too carefully. This caught my attention:
{code}
+  /**
+   * Skip over the next <code>n</code> bytes.
+   */
+  public void skipBytes(long n) throws IOException {
+    for (long i = 0; i < n; ++i) {
+      readByte();
+    }
+  }
{code}

you may want to use an array-based read here if there are a lot of skips; allocate a static, write-only buffer of 4 or 8kb once and just reuse it. A loop over readByte() is nearly always a performance killer, I've been hit by this too many times to count.

Also, lucene/core/src/java/org/apache/lucene/codecs/compressing/ByteArrayDataOutput.java -- there seems to be a class for this in
org.apache.lucene.store.ByteArrayDataOutput?
","29/Aug/12 08:14;eksdev;bq. but I removed the ability to select the compression algorithm on a per-field basis in order to make the patch simpler and to handle cross-field compression.

Maybe it is worth to keep it there for really short fields. Those general compression algorithms are great for bigger amounts of data, but for really short fields there is nothing like per field compression.   
Thinking about database usage, e.g. fields with low cardinality, or fields with restricted symbol set (only digits in long UID field for example).  Say zip code, product color...  is perfectly compressed using something with static dictionary approach (static huffman coder with escape symbol-s, at bit level, or plain vanilla dictionary lookup), and both of them are insanely fast and compress heavily. 

Even trivial utility for users is easily doable, index data without compression, get the frequencies from the term dictionary-> estimate e.g. static Huffman code table and reindex with this dictionary. 
","29/Aug/12 10:01;jpountz;Thanks Dawid and Eks for your feedback!

bq. allocate a static, write-only buffer of 4 or 8kb once and just reuse it

Right, sounds like a better default impl!

bq. ByteArrayDataOutput.java – there seems to be a class for this in org.apache.lucene.store.ByteArrayDataOutput?

I wanted to reuse this class, but I needed something that would grow when necessary (oal.store.BADO just throws an exception when you try to write past the end of the buffer). I could manage growth externally based on checks on the buffer length and calls to ArrayUtil.grow and BADO.reset but it was just as simple to rewrite a ByteArrayDataOutput that would manage it internally...

bq. Maybe it is worth to keep it there for really short fields. Those general compression algorithms are great for bigger amounts of data, but for really short fields there is nothing like per field compression.  Thinking about database usage, e.g. fields with low cardinality, or fields with restricted symbol set (only digits in long UID field for example). Say zip code, product color... is perfectly compressed using something with static dictionary approach (static huffman coder with escape symbol-s, at bit level, or plain vanilla dictionary lookup), and both of them are insanely fast and compress heavily.

Right, this is exactly why I implemented per-field compression first. Both per-field and cross-field compression have pros and cons. Cross-field compression allows less fine-grained tuning but on the other hand it would probably be a better default since the compression ratio would be better out of the box. Maybe we should implement both?

I was also thinking that some codecs such as this kind of per-field compression, but maybe even the bloom, memory, direct and pulsing postings formats might deserve a separate ""codecs"" module where we could put these non-default ""expert"" codecs.","29/Aug/12 11:12;rcmuir;{quote}
I was also thinking that some codecs such as this kind of per-field compression, but maybe even the bloom, memory, direct and pulsing postings formats might deserve a separate ""codecs"" module where we could put these non-default ""expert"" codecs.
{quote}

We have to do something about this soon!

Do you want to open a separate issue for that (it need not block this issue)?

I think we would try to get everything concrete we can out of core immediately
(maybe saving only the default codec for that release), but use the other
ones for testing. Still we should think about it.
","29/Aug/12 12:08;jpountz;bq. Do you want to open a separate issue for that (it need not block this issue)?

I created LUCENE-4340.","30/Aug/12 15:37;dsmiley;I just have a word of encouragement -- this is awesome!  Keep up the good work Adrien.","09/Sep/12 23:54;jpountz;Thanks for your kind words, David!

Here is a new version of the patch. I've though a lot about whether or not to let users configure per-field compression, but I think we should just try to provide something simple that improves the compression ratio by allowing cross-field and cross-document compression ;  People who have very specific needs can still implement their own {{StoredFieldsFormat}}.

Moreover I've had a discussion with Robert who argued that we should limit the number of classes that are exposed as a SPI because they add complexity (for example Solr needs to reload SPI registers every time it adds a core lib directory to the classpath). So I tried to make it simpler: there is no more {{CompressionCodec}} and people can choose between 3 different compression modes:
 - FAST, that uses LZ4's fast compressors and uncompressors (for indices that have a high update rate),
 - HIGH_COMPRESSION, that uses deflate (for people who want low compression ratios, no matter what the performance penalty is),
 - FAST_UNCOMPRESSION that spends more time compressing using LZ4's compress_HC method but still has very fast uncompression (for indices that have a reasonnable update rate and need good read performance).

I also added a test case and applied Dawid's advice to replace the default {{skipBytes}} implementation with a bulk-write into a write-only buffer.

Here is a new benchmark that shows how this new codec can help compress stored fields. This time, I indexed some access.log files generated by Apache HTTP server. A document consists of a line from the log file and is typically between 100 and 300 bytes. Because every line contains the date of the request, its path and the user-agent of the client, there is a lot of redundancy across documents.

{noformat}
Format            Chunk size  Compression ratio     IndexReader.document time
—————————————————————————————————————————————————————————————————————————————
uncompressed                               100%                         100%
doc/deflate 1                               90%                        1557%
doc/deflate 9                               90%                        1539%
index/FAST               512                50%                         197%
index/HIGH_COMPRESSION   512                44%                        1545%
index/FAST_UNCOMPRESSION 512                50%                         198%
{noformat}

Because documents are very small, document-level compression doesn't work well and only makes the .fdt file 10% smaller while loading documents from disk is more than 15 times slower on a hot OS cache.

However, with this kind of highly redundant input, {{CompressionMode.FAST}} looks very interesting as it divides the size of the .fdt file by 2 and only makes IndexReader.document twice slower.","10/Sep/12 10:25;jpountz;Otis shared a link to this issue on Twitter https://twitter.com/otisg/status/244996292743405571 and some people seem to wonder how it compares to ElasticSearch's block compression.

ElasticSearch's block compression uses a similar idea: data is compressed into blocks (with fixed sizes that are independent from document sizes). It is based on a CompressedIndexInput/CompressedIndexOutput: Upon closing, CompressedIndexOutput writes a metadata table at the end of the wrapped output that contains the start offset of every compressed block. Upon creation, a CompressedIndexInput first loads this metadata table into memory and can then use it whenever it needs to seek. This is probably the best way to compress small docs with Lucene 3.x.

With this patch, the size of blocks is not completely independent from document sizes: I make sure that documents don't spread across compressed blocks so that reading a document never requires more than one block to be uncompressed. Moreover, the LZ4 uncompressor (used by FAST and FAST_UNCOMPRESSION) can stop uncompressing whenever it has uncompressed enough data. So unless you need the last document of a compressed block, it is very likely that the uncompressor won't uncompress the whole block before returning.

Therefore I expect this StoredFieldsFormat to have a similar compression ratio to ElasticSearch's block compression (provided that similar compression algorithms are used) but to be a little faster at loading documents from disk.","26/Sep/12 00:00;jpountz;New version of the patch. It contains a few enhancements:
 - Merge optimization: whenever possible the StoredFieldsFormat tries to copy compressed data instead of uncompressing it into a buffer before compressing back to an index output,
 - New options for the stored fields index: there are 3 strategies that allow different memory/perf trade-offs:
 ** leaving it fully on disk (same as Lucene40, relying on the O/S cache),
 ** loading the position of the start of the chunk for every document into memory (requires up to 8 * numDocs bytes, no disk access),
 ** loading the position of the start of the chunk and the first doc ID it contains for every chunk (requires up to 12 * numChunks bytes, no disk access, interesting if you have large chunks of compressed data).
 - Improved memory usage and compression ratio (but a little slower) for CompressionMode.FAST (using packed ints).
 - Try to save 1 byte per field by storing the field number and the bits together.
 - More tests.

So in the end, this StoredFieldsFormat tries to make disk seeks less likely by:
 - giving the ability to load the stored fields index into memory (you never need to seek to find the position of the chunk that contains you document),
 - reducing the size of the fields data file (.fdt) so that the O/S cache can cache more documents.

Out of curiosity, I tested whether it could be faster for LZ4 to use intermediate buffers for compression and/or uncompression, and it is slower than accessing the index input/output directly (at least with MMapDirectory).

I hope I'll have something committable soon.","03/Oct/12 22:09;jpountz;New patch:
 - improved documentation,
 - I added CompressingCodec to the list of automatically tested codecs in test-framework,
 - a few bug fixes.

Please let me know if you would like to review this patch before I commit. Otherwise, I'll commit shortly...","03/Oct/12 22:20;rcmuir;im on the phone but i have some questions. give me a few :)","03/Oct/12 22:47;jpountz;Oh, I didn't mean THAT shortly :-)","03/Oct/12 22:55;rcmuir;Shouldnt ByteArrayDataInput override skip to just bump its 'pos'?

Can we plugin various schemes into MockRandomCodec?","03/Oct/12 22:59;rcmuir;OK MockRandom is just postings now... I think we should have a MockRandomCodec too!","04/Oct/12 00:03;jpountz;Almost the same patch. I removed ByteArrayDataInput.skipBytes(int) and removed ""throws IOException"" from ByteArrayDataInput.skipBytes(long).

bq. I think we should have a MockRandomCodec too!

Maybe we should fix it in a separate issue?","04/Oct/12 00:06;rcmuir;Yeah: i opened another issue to try to straighten this out. We can just bring these frankenstein codecs upto speed there.","04/Oct/12 01:59;rcmuir;I'm not a fan of the skipBytes on DataInput. Its not actually necessary or used for this patch?

And today DataInput is always forward-only, i dont like the ""may or may not be bidirectional depending if the impl throws UOE"".

I removed it locally and just left it on IndexInput, i think this is cleaner.","04/Oct/12 08:15;jpountz;bq. I'm not a fan of the skipBytes on DataInput. Its not actually necessary or used for this patch?

You're right! I needed it in the first versions of the patch when I reused Lucene40StoredFieldsFormat, but it looks like it's not needed anymore. Let's get rid of it!","04/Oct/12 20:36;jpountz;New patch that removes {{DataInput.skipBytes}}, this patch does not have any modifications in lucene-core anymore.","05/Oct/12 00:19;jpountz;Slightly modified patch in order not so seek when writing the stored fields index.","05/Oct/12 15:26;jpountz;I just committed to trunk. I'll wait for a couple of days to make sure Jenkins builds pass before backporting to 4.x. By the way, would it be possible to have one of the Jenkins servers to run lucene-core tests with -Dtests.codec=Compressing for some time?","05/Oct/12 18:03;simonw;bq. By the way, would it be possible to have one of the Jenkins servers to run lucene-core tests with -Dtests.codec=Compressing for some time?

FYI - http://builds.flonkings.com/job/Lucene-trunk-Linux-Java6-64-test-only-compressed/","05/Oct/12 21:48;jpountz;Thanks, Simon!","08/Oct/12 09:35;jpountz;lucene-core tests have passed the whole week-end so I just committed to branch 4.x as well. Thank you again for the Jenkins job, Simon.","16/Oct/12 14:35;hsn;is there example config provided?","16/Oct/12 14:36;simonw;@adrien I deleted the jenkins job for this.","16/Oct/12 14:50;jpountz;@radim you can have a look at CompressingCodec in lucene/test-framework
@Simon ok, thanks!","22/Mar/13 16:27;commit-tag-bot;[branch_4x commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1395491

LUCENE-4226: Efficient stored fields compression (merged from r1394578).

","10/May/13 10:34;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The test framework should report forked JVM PIDs at the start of test logs,LUCENE-4606,12623109,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,10/Dec/12 09:00,09/May/16 18:39,30/Sep/19 08:38,19/Dec/12 20:08,,,,,,,,,,,4.1,6.0,,,,,,0,,,,A follow-up to LUCENE-4603,,,,,,,,,,,,LUCENE-4603,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,296704,,,2012-12-10 09:00:42.0,New,,,,,,,"0|i14fdb:",233953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckIndex.fixIndex doesn't need a Codec,LUCENE-5141,12660094,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,26/Jul/13 16:12,09/May/16 18:39,30/Sep/19 08:38,28/Jul/13 16:17,,,,,,,,,,,4.5,6.0,,,,,,0,,,,CheckIndex.fixIndex takes a codec as an argument although it doesn't need one.,,,,,,,,,,,,,,,,"26/Jul/13 16:18;jpountz;LUCENE-5141.patch;https://issues.apache.org/jira/secure/attachment/12594418/LUCENE-5141.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-26 16:22:03.43,,,false,,,,,,,,,,,,,,,340286,,,Sat Oct 05 10:18:46 UTC 2013,New,,,,,,,"0|i1movb:",340604,,,,,,,,,"26/Jul/13 16:18;jpountz;Patch removing Codec from the arguments of CheckIndex.fixIndex.","26/Jul/13 16:22;mikemccand;+1","28/Jul/13 16:10;jira-bot;Commit 1507808 from [~jpountz] in branch 'dev/trunk'
[ https://svn.apache.org/r1507808 ]

LUCENE-5141: CheckIndex.fixIndex doesn't need a Codec.","28/Jul/13 16:26;jira-bot;Commit 1507812 from [~jpountz] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1507812 ]

LUCENE-5141: CheckIndex.fixIndex doesn't need a Codec.","05/Oct/13 10:18;jpountz;4.5 release -> bulk close",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update morfologik (polish stemmer) to 1.6.0,LUCENE-5089,12655994,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,03/Jul/13 10:05,09/May/16 18:39,30/Sep/19 08:38,03/Jul/13 12:18,,,,,,,,,,,4.5,6.0,,,,,,0,,,,,,,,,,,,,,,,SOLR-5126,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-07-03 12:14:51.621,,,false,,,,,,,,,,,,,,,336269,,,Sat Oct 05 10:19:04 UTC 2013,New,,,,,,,"0|i1m053:",336593,,,,,,,,,"03/Jul/13 12:14;jira-bot;Commit 1499352 from [~dawidweiss]
[ https://svn.apache.org/r1499352 ]

LUCENE-5089: Update to Morfologik 1.6.0. MorfologikAnalyzer and MorfologikFilter no longer support multiple ""dictionaries"" as there is only one dictionary available.","03/Jul/13 19:32;jira-bot;Commit 1499533 from [~dawidweiss]
[ https://svn.apache.org/r1499533 ]

LUCENE-5089: updated tag split character to be either + or |.","09/Aug/13 09:03;jira-bot;Commit 1512208 from [~dawidweiss] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1512208 ]

LUCENE-5089: Update to Morfologik 1.6.0 (Backport from trunk)
SOLR-5126: Update to Morfologik 1.7.1.
SOLR-5126: Update to Carrot2 1.8.0.","05/Oct/13 10:19;jpountz;4.5 release -> bulk close",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add small rounding to FuzzyQuery.floatToEdits,LUCENE-5469,12696919,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Not A Problem,,tallison@apache.org,tallison@apache.org,24/Feb/14 14:04,09/May/16 18:39,30/Sep/19 08:38,28/Feb/14 18:04,6.0,,,,,,,,,,,,,,core/search,,,0,easyfix,,,"I realize that FuzzyQuery.floatToEdits is deprecated, but I'd like to make a small fix for posterity.  Because of floating point issues, if a percentage leads to a number that is very close to a whole number of edits, our cast to int can improperly cause misses.

ddddd~0.8  will not match ""ddddX""
eeeee~0.8 will not match ""eeee"" or ""eeeeee""

This is a trivial part of the plan to reduce code duplication with LUCENE-5205.",,,,,,,,,,,,LUCENE-5496,,,,"24/Feb/14 14:07;tallison@apache.org;LUCENE-5469.patch;https://issues.apache.org/jira/secure/attachment/12630667/LUCENE-5469.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-24 19:37:31.271,,,false,,,,,,,,,,,,,,,375394,,,Fri Feb 28 18:04:21 UTC 2014,New,Patch Available,,,,,,"0|i1sotb:",375690,,,,,,,,,"24/Feb/14 14:07;tallison@apache.org;Fix with tests.  Chose to add .01f as hack.  Do we want genuine rounding?","24/Feb/14 19:37;rcmuir;I'm not really sure this is a bug. This is the way this crazy percentage always worked (a minimum bound, not inclusive!), and we just preserved that. I think instead we should remove this deprecated syntax from trunk completely?","24/Feb/14 20:20;tallison@apache.org;Sorry about this.  My fix was based on the idea that 80% of 5 should be the equivalent of an edit distance of 1, but you are absolutely right.  This behavior is entirely consistent with 3.5.

I tested a 10 letter word (salmonella) on noisy ocr'd data

4.x
~2 110 variants
~1 37 variants

0.9 no variants
0.89 34 variants
0.88 37 variants
0.80 37 variants
0.79 94 variants
0.78 94 variants
0.77 108 variants
0.74 110 variants

3.5
0.9 no variants
0.89 34 variants
0.88 37 variants
0.80 37 variants
0.79 94 variants
0.78 94 variants
0.77 108 variants
0.74 110 variants","24/Feb/14 20:25;tallison@apache.org;int edit distance is cleaner, and I think I've now convinced myself that although there are some gradations possible with percentage, it doesn't buy much.

Unless anyone is surprised by the above, I'll close this issue in a few days and modify 5205 to stop handling %.

Sorry about that!  Thank you, again.","28/Feb/14 11:56;tallison@apache.org;[~rcmuir], would the next step to removing FuzzyQuery.floatToEdits from trunk be to deprecate fuzzyMinSim(s) in the queryparsers and add fuzzyMaxEdits?","28/Feb/14 13:14;rcmuir;We deprecated this stuff already in 4.x, and issued warnings (e.g. in the queryparser syntax) that such syntax was deprecated in 4.x and will be removed in 5. We should nuke this stuff!","28/Feb/14 18:04;tallison@apache.org;will open another issue on nuking fuzzyminsim",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Improve ""ant eclipse"" to select right JRE for building",LUCENE-6174,12766646,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,uschindler,uschindler,10/Jan/15 15:33,09/May/16 18:38,30/Sep/19 08:38,12/Aug/15 08:09,,,,,,,,,,,5.4,6.0,,,general/build,,,0,,,,"Whenever I run ""ant eclipse"" the setting choosing the right JVM is lost and has to be reassigned in the project properties.

In fact the classpath generator writes a new classpath file (as it should), but this onl ycontains the ""default"" entry:

{code:xml}
<classpathentry kind=""con"" path=""org.eclipse.jdt.launching.JRE_CONTAINER""/>
{code}

Instead it should preserve something like:

{code:xml}
<classpathentry kind=""con"" path=""org.eclipse.jdt.launching.JRE_CONTAINER/org.eclipse.jdt.internal.debug.ui.launcher.StandardVMType/jdk1.8.0_25""/>
{code}

We can either path this by a Ant property via command line or user can do this with ""lucene/build.properties"" or per user. An alternative would be to generate the name ""jdk1.8.0_25"" by guessing from ANT's ""java.home"". If this name does not exist in eclipse it would produce an error and user would need to add the correct JDK.

I currently have the problem that my Eclipse uses Java 7 by default and whenever I rebuild the eclipse project, the change to Java 8 in trunk is gone.

When this is fixed, I could easily/automatically have the ""right"" JDK used by eclipse for trunk (Java 8) and branch_5x (Java 7).",,,,,,,,,,,,,,,,"11/Aug/15 11:00;dweiss;LUCENE-6174.patch;https://issues.apache.org/jira/secure/attachment/12749822/LUCENE-6174.patch","12/Aug/15 06:52;dweiss;capture-2.png;https://issues.apache.org/jira/secure/attachment/12750041/capture-2.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-08-11 11:00:47.134,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 12 08:07:56 UTC 2015,New,,,,,,,"0|i2486v:",9223372036854775807,,,,,,,,,"11/Aug/15 11:00;dweiss;I think this can be hardcoded in {{dot.classpath.xsl}} (to be 1.7 in branch_5x and 1.8 in trunk).

This declaration declares an ""execution environment"" which one can then define in Eclipse settings to be any available 1.8-compatible JVM:
{code}
  <classpathentry kind=""con"" path=""org.eclipse.jdt.launching.JRE_CONTAINER/org.eclipse.jdt.internal.debug.ui.launcher.StandardVMType/JavaSE-1.8""/>
{code}","11/Aug/15 19:57;dweiss;Let me know if you'd like me to add it, [~thetaphi], I can take care of this.","11/Aug/15 22:26;uschindler;If this works, I am fine with that. I tried this out locally and the names were more complicated (because it used the ""name"" I used locally.
But using those standards should be fine. So go ahead, I can try this!","11/Aug/15 22:30;uschindler;Looks fine! Go ahead. I did not find out that there is some ""generic"" selector in the GUI that selects those types with generic name!","12/Aug/15 06:51;dweiss;Yep, this is a generic ""jvm type"" selector. Which is nice because then you can select which JVM you're using at runtime (for all projects). I'll attach a screenshot with the info where it is in Eclipse.","12/Aug/15 07:59;uschindler;+1 to commit","12/Aug/15 07:59;jira-bot;Commit 1695438 from [~dawidweiss] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1695438 ]

LUCENE-6174: Improve 'ant eclipse' to select right JRE for building.","12/Aug/15 08:07;jira-bot;Commit 1695442 from [~dawidweiss] in branch 'dev/trunk'
[ https://svn.apache.org/r1695442 ]

LUCENE-6174: Improve 'ant eclipse' to select right JRE for building.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnicodeUtil.codePointCount microbenchmarks (wtf),LUCENE-4889,12639275,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,27/Mar/13 10:30,09/May/16 18:38,30/Sep/19 08:38,28/Mar/13 08:45,,,,,,,,,,,4.3,6.0,,,,,,0,,,,"This is interesting. I posted a link to a state-machine-based UTF8 parser/recognizer:
http://bjoern.hoehrmann.de/utf-8/decoder/dfa/

I spent some time thinking if the lookup table could be converted into a stateless computational function, which would avoid a table lookup (which in Java will cause an additional bounds check that will be hard to eliminate I think). This didn't turn out to be easy (it boils down to finding a simple function that would map a set of integers to its concrete permutation; a generalization of minimal perfect hashing).

But out of curiosity I though it'd be fun to compare how Lucene's codepoint counting compares to Java's built-in one (Decoder) and a sequence of if's.

I've put together a Caliper benchmark that processes 50 million unicode codepoints; one only ASCII, one Unicode. The results are interesting. On my win/I7:

{code}
 implementation dataType          ns linear runtime
         LUCENE  UNICODE 167359502.6 ===============
         LUCENE    ASCII 334015746.5 ==============================
NOLOOKUP_SWITCH  UNICODE 154294141.8 =============
NOLOOKUP_SWITCH    ASCII 119500892.8 ==========
    NOLOOKUP_IF  UNICODE  90149072.6 ========
    NOLOOKUP_IF    ASCII  29151411.4 ==
{code}

Disregard the switch lookup -- it's for fun only. But a sequence of if's is significantly faster than the current Lucene's table lookup, especially on ASCII input. And now compare this to Java's built-in decoder...

{code}
           JAVA  UNICODE   5753930.1 =
           JAVA    ASCII        23.8 =
{code}

Yes, it's the same benchmark. Wtf? I realize buffers are partially native and probably so is utf8 decoder but by so much?! Again, to put it in context:

{code}
 implementation dataType          ns linear runtime
         LUCENE  UNICODE 167359502.6 ===============
         LUCENE    ASCII 334015746.5 ==============================
           JAVA  UNICODE   5753930.1 =
           JAVA    ASCII        23.8 =
    NOLOOKUP_IF  UNICODE  90149072.6 ========
    NOLOOKUP_IF    ASCII  29151411.4 ==
NOLOOKUP_SWITCH  UNICODE 154294141.8 =============
NOLOOKUP_SWITCH    ASCII 119500892.8 ==========
{code}

Wtf? The code is here if you want to experiment.
https://github.com/dweiss/utf8dfa

I realize the Java version needs to allocate a temporary space buffer but if these numbers hold for different VMs it may actually be worth it...",,,,,,,,,,,,,,,,"27/Mar/13 12:42;dweiss;LUCENE-4889.patch;https://issues.apache.org/jira/secure/attachment/12575694/LUCENE-4889.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-27 11:00:47.202,,,false,,,,,,,,,,,,,,,319745,,,Fri May 10 10:33:40 UTC 2013,New,,,,,,,"0|i1j64n:",320086,,,,,,,,,"27/Mar/13 10:35;dweiss;Just to complete: didn't inspect jit assembly dumps, didn't check for dead code elimination (although I don't think it should happen here because of how Caliper is written).","27/Mar/13 11:00;uschindler;I think the ASCII-only performance is caused by line 211/212 and 632/633 in http://www.docjar.com/html/api/sun/nio/cs/UTF_8.java.html
If your buffer is only ASCII is has a very simple loop that simply copies the bytes. This may explain the speed, together with eliminated bounds checks by hotspot on this simple loop.","27/Mar/13 11:09;rcmuir;But the jdk decoder is buggy. Its easy to make a buggy fast decoder :)","27/Mar/13 11:16;dweiss;Just pushed a version that doesn't do a table lookup for ASCII.
{code}
implementation dataType          ns linear runtime
        LUCENE  UNICODE 167374240.6 ===============
        LUCENE    ASCII 333944799.0 ==============================
   LUCENE_MOD1  UNICODE 167449028.1 ===============
   LUCENE_MOD1    ASCII  77172139.4 ======
          JAVA  UNICODE   5755140.1 =
          JAVA    ASCII        23.9 =
   NOLOOKUP_IF  UNICODE  90220440.6 ========
   NOLOOKUP_IF    ASCII  29145155.3 ==
{code}

Cuts the time by 75% but still far above the Java decoder. So it's not a single loop I think, Uwe :)

Also: I'm not comparing full decoding, only codepoint counting. I also assumed valid utf8 (since it's what UnicodeUtil does anyway). Finally: I'm not advocating for changing it, I'm just saying it's interesting *by how much* these timings differ.
","27/Mar/13 11:25;rcmuir;I think we should change the counting function... its a little scary in that i think it will loop forever on bad input?

We could at least fix the table to not use -1 but a large negative value that stands a chance of creating AIOOBE","27/Mar/13 11:26;dweiss;I'll actually prepare a patch that replaces the current implementation of UnicodeUtil.codePointCount because it can spin forever on invalid input (yes it does say the behavior is undefined but I think we should just throw an exception on invalid input :).

Mike can run his magic perf. scripts and we'll see if it breaks anything. I doubt.","27/Mar/13 12:01;mikemccand;bq. I'll actually prepare a patch that replaces the current implementation of UnicodeUtil.codePointCount because it can spin forever on invalid input (yes it does say the behavior is undefined but I think we should just throw an exception on invalid input

+1 ... spinning forever on bad input is not nice!
","27/Mar/13 12:42;dweiss;A patch cleaning up codePointCount. The state array is still used in UTF8toUTF32 so I left it in, but replaced -1's with Integer.MIN_VALUE (not that it makes a difference here).","28/Mar/13 08:39;dweiss;http://i.stack.imgur.com/jiFfM.jpg [Double facepalm]

It couldn't be right, it was surreal. I checked the code again and indeed, there was a subtle bug in the Java code -- it was looping over the decode loop, all right, but it was never rewiding the input buffer after the first time, damn it. Corrected it shows sensible output:

{code}
implementation dataType    ms linear runtime
[current lucene]
        LUCENE  UNICODE 167.3 ==========
        LUCENE    ASCII 333.9 =====================
[patch]
   LUCENE_MOD1  UNICODE 103.0 ======
   LUCENE_MOD1    ASCII  77.2 ====
[if-based version but without assertions]
   NOLOOKUP_IF  UNICODE  90.2 =====
   NOLOOKUP_IF    ASCII  29.1 =
[java version]
          JAVA  UNICODE 465.6 ==============================
          JAVA    ASCII 103.1 ======
[no branching/counting, just pass over data]
      NO_COUNT  UNICODE  52.1 ===
      NO_COUNT    ASCII  26.0 =
{code}

I also did a non-benchmark loop in which everything is just counted 10 times.
{code}
time          codepoints   version data set
     1.676 <= [ 500000010] UNICODE LUCENE
     0.905 <= [ 500000010] UNICODE LUCENE_MOD1
     0.905 <= [ 500000010] UNICODE NOLOOKUP_IF
     4.686 <= [ 500000010] UNICODE JAVA

     3.339 <= [1000000000] ASCII LUCENE
     1.028 <= [1000000000] ASCII LUCENE_MOD1
     1.027 <= [1000000000] ASCII NOLOOKUP_IF
     1.591 <= [1000000000] ASCII JAVA
{code}

I'll commit the patch since it improves both the speed and the internal validation logic.
","10/May/13 10:33;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade randomizedtesting to 2.3.1,LUCENE-6899,12913851,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,17/Nov/15 20:00,09/May/16 18:38,30/Sep/19 08:38,18/Nov/15 08:53,,,,,,,,,,,5.4,6.0,,,,,,0,,,,"This has a bunch of internal and some external improvements, overview here:
https://github.com/randomizedtesting/randomizedtesting/releases",,,,,,,,,,,,,,,,"17/Nov/15 20:03;dweiss;LUCENE-6899.patch;https://issues.apache.org/jira/secure/attachment/12772816/LUCENE-6899.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-11-18 07:50:04.208,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 08:53:39 UTC 2015,New,,,,,,,"0|i2oj8v:",9223372036854775807,,,,,,,,,"18/Nov/15 07:50;jira-bot;Commit 1714945 from [~dawidweiss] in branch 'dev/trunk'
[ https://svn.apache.org/r1714945 ]

LUCENE-6899: upgrade randomizedtesting to version 2.3.1","18/Nov/15 08:53;jira-bot;Commit 1714952 from [~dawidweiss] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1714952 ]

LUCENE-6899: upgrade randomizedtesting to version 2.3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test runner should report the number of suites completed/ remaining,LUCENE-6413,12819627,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,09/Apr/15 10:23,09/May/16 18:38,30/Sep/19 08:38,09/Apr/15 10:29,,,,,,,,,,,5.3,6.0,,,,,,0,,,,"Pretty much as suggested by Shawn on the dev list.
{code}
Suite: org.apache.lucene.util.TestNamedSPILoader
Completed [2/414] on J3 in 0.59s, 3 tests

Suite: org.apache.lucene.util.TestBitDocIdSetBuilder
Completed [3/414] on J0 in 0.29s, 2 tests

Suite: org.apache.lucene.index.TestStressIndexing2
Completed [4/414] on J1 in 2.06s, 3 tests
{code}

The number of individual tests cannot be printed (it's not available globally to the runner until the suite is actually executed).

There is no ETA remaining for similar reasons (the variance on each suite's execution time is unpredictable).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-04-09 10:25:51.883,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 26 13:05:44 UTC 2015,New,,,,,,,"0|i2d0wv:",9223372036854775807,,,,,,,,,"09/Apr/15 10:25;jira-bot;Commit 1672281 from [~dawidweiss] in branch 'dev/trunk'
[ https://svn.apache.org/r1672281 ]

LUCENE-6413: Test runner should report the number of suites completed/ remaining.","09/Apr/15 10:27;jira-bot;Commit 1672282 from [~dawidweiss] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1672282 ]

LUCENE-6413: Test runner should report the number of suites completed/ remaining.","09/Apr/15 10:31;jpountz;Woohoo! Thanks Dawid!","09/Apr/15 10:34;dweiss;You're very welcome although I am so far behind with other changes I'd like to make to the RR that there's hardly any reason to celebrate  (yet :).","09/Apr/15 17:55;elyograg;This is *_awesome_*.  It won't make the tests go any faster, but now I will know whether I can walk away from the running tests to work on something else.","26/Aug/15 13:05;shalinmangar;Bulk close for 5.3.0 release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopDocs.merge should use updateTop instead of pop / add,LUCENE-6878,12909802,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,djelinski,djelinski,02/Nov/15 22:11,09/May/16 18:38,30/Sep/19 08:38,03/Nov/15 14:35,6.0,,,,,,,,,,5.4,6.0,,,core/search,,,0,,,,"The function TopDocs.merge uses PriorityQueue in a pattern: pop, update value (ref.hitIndex++), add. JavaDocs for PriorityQueue.updateTop say that using this function instead should be at least twice as fast.",,,,,,,,,,,,,,,,"02/Nov/15 22:14;djelinski;LUCENE-6878.patch;https://issues.apache.org/jira/secure/attachment/12770175/LUCENE-6878.patch","03/Nov/15 23:32;djelinski;speedtest.tar.gz;https://issues.apache.org/jira/secure/attachment/12770450/speedtest.tar.gz",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-11-03 10:59:23.799,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 13:01:03 UTC 2015,New,,,,,,,"0|i2nuhb:",9223372036854775807,,,,,,,,,"03/Nov/15 10:59;mikemccand;+1

I know we are discussing how to benchmark this change but I don't think that's needed before committing ... this is a good change ... it's only needed to satisfy curiosity :)","03/Nov/15 11:53;toke;In light of my own recent experiments with PriorityQueue (SOLR-6828), I'll note that microbenchmarks are exceedingly simple to screw up, especially in Java. I ended up doing comparative testing with pre-generated test inputs, multiple runs, discarding the first runs, alternating between the implementation multiple times and removing outliers. And the results are still not very stable.","03/Nov/15 14:18;jira-bot;Commit 1712298 from [~jpountz] in branch 'dev/trunk'
[ https://svn.apache.org/r1712298 ]

LUCENE-6878: Speed up TopDocs.merge.","03/Nov/15 14:30;jira-bot;Commit 1712299 from [~jpountz] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1712299 ]

LUCENE-6878: Speed up TopDocs.merge.","03/Nov/15 14:35;jpountz;bq. I know we are discussing how to benchmark this change but I don't think that's needed before committing

Agreed, I just committed the change.

Daniel: I am marking the issue resolved since it was committed, but feel free to comment on it about your findings about potential performance improvements.","03/Nov/15 23:32;djelinski;I merged 64 ScoreDoc lists, 100k docs each, and took top 100k results, in 3 different score distributions. For time calculations, each test was repeated 60 times, and I averaged the results of 10 subsequent runs, discarding any outliers. For the number of lessThan calls in random case, I run the test 3 times and took an average. The number of lessThan calls for case 1 and 2 is constant.
I tested score lists generated using 3 different methods.
1) All scores equal to 1. This is the case where the patch made a greatest difference, mostly because of tie breaks in lessThan methods. Results:
Without the patch - 2.66 msec per merge call, 1600057 calls to lessThan
With the patch - 0.32 msec per merge call, 200071 calls to lessThan. 
Overall, ~88% savings on time and lessThan calls
2) Each list contains scores 100000,99999,....,1
Without the patch - 3.5 msec per merge call, 1100063 calls to lessThan
With the patch - 2.6 msec per merge call, 1005390 calls to lessThan
Overall, ~25% savings on time, 9% savings on lessThan calls
3) Each list starts with doc with score 100000, score of other docs is calculated as score of previous doc minus Math.random()
Without the patch - 3.5 msec per merge call, ~1156500 calls to lessThan
With the patch, 2.7 msec per merge call, ~960500 calls to lessThan
Overall, ~23% savings on time, 17% savings on lessThan calls.

In the random case the gain is much less than the advertised double speed, but it's still a net improvement.
I attached the code I used to measure the speed, in case anyone is interested. Fair warning, it's not pretty.","04/Nov/15 13:01;jpountz;These results make sense to me given the change. Thank you Daniel!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NormValueSource unable to read long field norm,LUCENE-5398,12688617,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,peng,peng,13/Jan/14 21:26,09/May/16 18:38,30/Sep/19 08:38,31/Jan/14 12:22,4.6,,,,,,,,,,4.7,6.0,,,core/query/scoring,,,0,,,,"Previous Lucene implementation store field norms of all documents in memory, float values are therefore encoded into byte to minimize memory consumption.
Recent release no longer have this constraint (see LUCENE-5078, and discussion at http://lucene.markmail.org/message/jtwit3pwu5oiqr2h), users are encouraged to implement their own encodeNormValue() to encode them into/decode from any type including int, byte and long, to fulfil their request for precision.
But the legacy NormValueSource still typecast any long encoding into byte, as seen in line 74 in the java file, making any TFIDFSimilarity using more accurate encoding useless.
It should be removed for the greater good.",Ubuntu 12.04,3600,3600,,0%,3600,3600,,,,,,,,,"29/Jan/14 11:16;mikemccand;LUCENE-5398.patch;https://issues.apache.org/jira/secure/attachment/12625848/LUCENE-5398.patch","18/Jan/14 00:34;peng;NormValueSource.java;https://issues.apache.org/jira/secure/attachment/12623756/NormValueSource.java","29/Jan/14 04:02;peng;TestValueSourcesWithNonByteNormEncoding.java;https://issues.apache.org/jira/secure/attachment/12625785/TestValueSourcesWithNonByteNormEncoding.java",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-01-29 11:00:31.144,,,false,,,,,,,,,,,,,,,367636,,,Fri Jan 31 18:36:33 UTC 2014,New,,,,,,,"0|i1rd9z:",367943,,,,,,,,,"18/Jan/14 00:34;peng;Removed.

Run junit TestValueSources without problem. This thing should be trivial and doesn't require a test case for non-byte situation","29/Jan/14 04:02;peng;I have attached a simple test case to show the issue. The only difference:
Similarity at index-time and search-time are replaced with a TFIDFSimilarity implementation that use integer norm encoding (instead of byte).","29/Jan/14 11:00;mikemccand;Thanks Peng, I'll have a look.  It's clear that cast to (byte) is a holdover from before TFIDFSim only accepted 1 byte norms.","29/Jan/14 11:16;mikemccand;Thanks Peng; I simplified the test a bit and folded the fix into the attached patch.

I think it's ready!","29/Jan/14 17:30;peng;At you service, I've read your book.","31/Jan/14 12:16;jira-bot;Commit 1563119 from [~mikemccand] in branch 'dev/trunk'
[ https://svn.apache.org/r1563119 ]

LUCENE-5398: remove invalid byte cast in NormValueSource, since TFIDFSimilarity now allows for norms larger than one byte","31/Jan/14 12:21;jira-bot;Commit 1563120 from [~mikemccand] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1563120 ]

LUCENE-5398: remove invalid byte cast in NormValueSource, since TFIDFSimilarity now allows for norms larger than one byte","31/Jan/14 12:22;mikemccand;Thanks Peng!","31/Jan/14 18:36;jira-bot;Commit 1563209 from [~rcmuir] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1563209 ]

LUCENE-5398: don't use 3.x codec in this test",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The test framework should report forked JVM PIDs upon heartbeats,LUCENE-4603,12623065,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,09/Dec/12 18:54,09/May/16 18:38,30/Sep/19 08:38,10/Dec/12 08:59,,,,,,,,,,,4.1,6.0,,,general/test,,,0,,,,"This would help in getting a stack trace of a hung JVM before the timeout and/or in killing the offending JVM.

RR issue:
https://github.com/carrotsearch/randomizedtesting/issues/135
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-12-09 19:12:19.256,,,false,,,,,,,,,,,,,,,296657,,,Mon Dec 10 08:48:09 UTC 2012,New,,,,,,,"0|i14e9j:",233774,,,,,,,,,"09/Dec/12 18:59;dweiss;I've fixed this in version 2.0.5. It reports forked JVM PID, no stack trace -- there's another issue for that but it's more complicated than ""just"" PIDs. I used the method mentioned by Uwe and dump the entire PID string (VM name, whatever it's going to be). On hotspot this should be PID@host. On other vendor VMs -- we'll see.

I'll update as soon as version 2.0.5 propagates through Maven repositories.","09/Dec/12 19:12;uschindler;I have seen the randomizedtesting/junit4 commit. One addition: Could we also print all pidStrings on startup, too? Sometimes tests don't heartbeat, but I still want to signal them. So along the messages:

{noformat}
[junit4:junit4] <JUnit4> says aloha! Master seed: 47E240512F1F7A09
[junit4:junit4] Executing 328 suites with 2 JVMs.
{noformat}

it could print:

{noformat}
J0: pidString; J1: pidString;...
{noformat}","09/Dec/12 19:16;dweiss;Sure, added to the list of issues to fix. I'll respin in a week, perhaps there'll be more fixes by then, ok?","10/Dec/12 08:44;commit-tag-bot;[trunk commit] Dawid Weiss
http://svn.apache.org/viewvc?view=revision&revision=1419261

LUCENE-4603: Upgrade randomized testing to version 2.0.5: print forked JVM PIDs on heartbeat from hung tests (Dawid Weiss)


","10/Dec/12 08:48;commit-tag-bot;[branch_4x commit] Dawid Weiss
http://svn.apache.org/viewvc?view=revision&revision=1419263

LUCENE-4603: Upgrade randomized testing to version 2.0.5: print forked JVM PIDs on heartbeat from hung tests (Dawid Weiss)



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
query is always null in countDocsWithClass() of SimpleNaiveBayesClassifier,LUCENE-5466,12696770,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,koji,koji,koji,23/Feb/14 02:27,09/May/16 18:38,30/Sep/19 08:38,17/Mar/14 19:12,,,,,,,,,,,4.7.1,4.8,6.0,,modules/classification,,,0,,,,,,,,,,,,,,,,,,,,"23/Feb/14 02:31;koji;LUCENE-5466.patch;https://issues.apache.org/jira/secure/attachment/12630524/LUCENE-5466.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-28 00:49:40.788,,,false,,,,,,,,,,,,,,,375245,,,Wed Apr 02 15:04:52 UTC 2014,New,,,,,,,"0|i1snw7:",375541,,,,,,,,,"23/Feb/14 02:31;koji;I think query must be set before calling countDocsWithClass() in train() method.","28/Feb/14 00:49;jira-bot;Commit 1572793 from [~okoji091] in branch 'dev/trunk'
[ https://svn.apache.org/r1572793 ]

LUCENE-5466: query is always null in countDocsWithClass() of SimpleNaiveBayesClassifier.","28/Feb/14 01:03;jira-bot;Commit 1572798 from [~okoji091] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1572798 ]

LUCENE-5466: query is always null in countDocsWithClass() of SimpleNaiveBayesClassifier.","17/Mar/14 19:12;rcmuir;reopening for 4.7.1 backport","17/Mar/14 19:12;jira-bot;Commit 1578522 from [~rcmuir] in branch 'dev/branches/lucene_solr_4_7'
[ https://svn.apache.org/r1578522 ]

LUCENE-5466: query is always null in countDocsWithClass() of SimpleNaiveBayesClassifier","02/Apr/14 15:04;sarowe;Bulk close 4.7.1 issues",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FSA NoOutputs should implement merge() allowing duplicate keys,LUCENE-4887,12639204,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,ryantxu,ryantxu,ryantxu,26/Mar/13 21:40,09/May/16 18:37,30/Sep/19 08:38,27/Mar/13 04:15,,,,,,,,,,,4.3,6.0,,,,,,0,,,,The NoOutput Object throws NotImplemented if you try to add the same input twice.  This can easily be implemented,,,,,,,,,,,,,,,,"26/Mar/13 21:42;ryantxu;LUCENE-4887.patch;https://issues.apache.org/jira/secure/attachment/12575588/LUCENE-4887.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-26 22:56:02.788,,,false,,,,,,,,,,,,,,,319674,,,Fri May 10 10:32:41 UTC 2013,,,,,,,,"0|i1j5ov:",320015,,,,,,,,,"26/Mar/13 21:42;ryantxu;merge as:
{code:java}
  @Override
  public Object merge(Object first, Object second) {
    assert first == NO_OUTPUT;
    assert second == NO_OUTPUT;
    return NO_OUTPUT;
  }
{code}","26/Mar/13 22:56;mikemccand;+1

This makes FST act like a Set, ie adding the same input more than once is indistinguishable from adding that input only once.","27/Mar/13 04:15;ryantxu;Added in r1461409

I hit this issue trying to have the FST act as a Set<String>","10/May/13 10:32;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexDocument methods should not use wildcards in their return types,LUCENE-4519,12614171,,Task,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,31/Oct/12 13:37,09/May/16 18:37,30/Sep/19 08:38,04/Nov/12 22:19,,,,,,,,,,,6.0,,,,,,,0,,,,{{public Iterable<? extends IndexableField> indexableFields()}} should be replaced with {{public Iterable<IndexableField> indexableFields()}}.,,,,,,,,,,,,,,,,"04/Nov/12 13:03;jpountz;LUCENE-4519.patch;https://issues.apache.org/jira/secure/attachment/12552021/LUCENE-4519.patch","31/Oct/12 13:42;jpountz;LUCENE-4519.patch;https://issues.apache.org/jira/secure/attachment/12551535/LUCENE-4519.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-11-04 12:09:58.533,,,false,,,,,,,,,,,,,,,253361,,,Sun Nov 04 22:19:07 UTC 2012,New,,,,,,,"0|i0dknb:",77274,,,,,,,,,"31/Oct/12 13:42;jpountz;Patch. I probably need a review from the generics policeman. :-)","04/Nov/12 12:09;uschindler;I agree with this, the wildcard is useless here and not really user-friendly. I think it was added to prevent some unchecked casts from different Field types, which can still be done, if the called methods have correct generics.

I am not so happy with the FilterIterator change, but I agree that it is needed here, otherwise it gets unchecked casts in the predicate.","04/Nov/12 12:15;uschindler;Can you add javadocs to FilterIterator, so the generics are more clear. It is confusing which one is the inner iterator's type and which one the outer's. I would propose to rename the params to be more clear, too. Like FilterIterator<T, InnerT>.","04/Nov/12 13:03;jpountz;New patch with improved javadocs. Does it look better?","04/Nov/12 13:54;uschindler;+1","04/Nov/12 22:19;jpountz;Committed to trunk (r1405639).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support running the same test suite multiple times in parallel,LUCENE-4360,12606139,,New Feature,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,04/Sep/12 19:39,09/May/16 18:36,30/Sep/19 08:38,05/Sep/12 10:05,,,,,,,,,,,4.0,6.0,,,general/test,,,0,,,,"The current ""test execution multiplier"" or:
{code}
-Dtests.iters=N
{code}
generates multiple tests (method executions) under a test class (suite). All these tests, however, are bound to a single class so they must run sequentially and on a single JVM (because of how JUnit works -- nesting of rules, class hooks, etc.).

Mark pointed out that if somebody has a multi-core CPU then it'd be nice to be able to run a single suite in parallel, possibly in combination with tests.iters (so that a single test method is executed X times on Y parallel JVMs).

This is surprisingly easy with the randomized runner because it currently accepts ""duplicate"" suite names and will load-balance them in a normal way. So, if one has Y cores (JVMs) then providing a suite name X times will result in X executions, balanced across Y JVMs.

The only problem is how to ""multiply"" suite names. This can be done in a number of ways, starting from a custom resource collection wrapper and ending at a built-in code in the runner itself. I think the custom collection wrapper approach would be interesting, I'll explore this direction.",,,,,,,,,,,,,,,,"04/Sep/12 19:49;dweiss;quickhack.patch;https://issues.apache.org/jira/secure/attachment/12543733/quickhack.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-05 10:54:05.618,,,false,,,,,,,,,,,,,,,239823,,,Fri May 10 10:32:49 UTC 2013,New,,,,,,,"0|i00u73:",2824,,,,,,,,,"04/Sep/12 19:49;dweiss;This is a quick hack that multiplies execution of all tests by N-fold by just repeating the same fileset a few times.

A proof of concept, but working.","05/Sep/12 08:33;dweiss;Any ideas what to call this parameter? It's not the same as tests.iters because it multiplies entire suites. I came up with:
{code}
-Dtests.multiply
-Dtests.duplicate

or the short equivalents:

-Dtests.mply
-Dtests.dups
{code}","05/Sep/12 10:03;dweiss;Example of use:
{code}
ant test-core -Dtests.dups=3 -Dtestcase=TestCharFilter

<JUnit4> says kaixo! Master seed: 4CE0022F3CDFF5D7
Your default console's encoding may not display certain unicode glyphs: windows-1252
Executing 3 suites with 3 JVMs.

Suite: org.apache.lucene.analysis.TestCharFilter
OK      0.05s J1 | TestCharFilter.testCharFilter1
OK      0.01s J1 | TestCharFilter.testCharFilter11
OK      0.00s J1 | TestCharFilter.testCharFilter2
OK      0.00s J1 | TestCharFilter.testCharFilter12
Completed on J1 in 0.35s, 4 tests

Suite: org.apache.lucene.analysis.TestCharFilter
OK      0.04s J2 | TestCharFilter.testCharFilter1
OK      0.01s J2 | TestCharFilter.testCharFilter11
OK      0.00s J2 | TestCharFilter.testCharFilter2
OK      0.01s J2 | TestCharFilter.testCharFilter12
Completed on J2 in 0.35s, 4 tests

Suite: org.apache.lucene.analysis.TestCharFilter
OK      0.04s J0 | TestCharFilter.testCharFilter1
OK      0.02s J0 | TestCharFilter.testCharFilter11
OK      0.00s J0 | TestCharFilter.testCharFilter2
OK      0.00s J0 | TestCharFilter.testCharFilter12
Completed on J0 in 0.36s, 4 tests

JVM J0:     0.67 ..     1.56 =     0.89s
JVM J1:     0.67 ..     1.55 =     0.88s
JVM J2:     0.67 ..     1.57 =     0.90s
Execution time total: 1.58 sec.
Tests summary: 3 suites, 12 tests
{code}

In combination with tests.iters:
{code}
ant test-core -Dtests.dups=2 -Dtests.iters=2 -Dtestcase=TestCharFilter

<JUnit4> says íHola! Master seed: 592442048D98540B
Your default console's encoding may not display certain unicode glyphs: windows-1252
Executing 2 suites with 2 JVMs.

Suite: org.apache.lucene.analysis.TestCharFilter
OK      0.04s J1 | TestCharFilter.testCharFilter12 {#0 seed=[592442048D98540B:5E8780464DC70845]}
OK      0.00s J1 | TestCharFilter.testCharFilter12 {#1 seed=[592442048D98540B:EAD13CBA7905C369]}
OK      0.00s J1 | TestCharFilter.testCharFilter11 {#0 seed=[592442048D98540B:4AC9FEB2457325F4]}
OK      0.00s J1 | TestCharFilter.testCharFilter11 {#1 seed=[592442048D98540B:FE9F424E71B1EED8]}
OK      0.00s J1 | TestCharFilter.testCharFilter1 {#0 seed=[592442048D98540B:643276B2D133E783]}
OK      0.00s J1 | TestCharFilter.testCharFilter1 {#1 seed=[592442048D98540B:D064CA4EE5F12CAF]}
OK      0.00s J1 | TestCharFilter.testCharFilter2 {#0 seed=[592442048D98540B:D339FCA65F87D49C]}
OK      0.01s J1 | TestCharFilter.testCharFilter2 {#1 seed=[592442048D98540B:676F405A6B451FB0]}
Completed on J1 in 0.34s, 8 tests

Suite: org.apache.lucene.analysis.TestCharFilter
OK      0.05s J0 | TestCharFilter.testCharFilter12 {#0 seed=[592442048D98540B:5E8780464DC70845]}
OK      0.00s J0 | TestCharFilter.testCharFilter12 {#1 seed=[592442048D98540B:EAD13CBA7905C369]}
OK      0.00s J0 | TestCharFilter.testCharFilter11 {#0 seed=[592442048D98540B:4AC9FEB2457325F4]}
OK      0.00s J0 | TestCharFilter.testCharFilter11 {#1 seed=[592442048D98540B:FE9F424E71B1EED8]}
OK      0.00s J0 | TestCharFilter.testCharFilter1 {#0 seed=[592442048D98540B:643276B2D133E783]}
OK      0.00s J0 | TestCharFilter.testCharFilter1 {#1 seed=[592442048D98540B:D064CA4EE5F12CAF]}
OK      0.00s J0 | TestCharFilter.testCharFilter2 {#0 seed=[592442048D98540B:D339FCA65F87D49C]}
OK      0.00s J0 | TestCharFilter.testCharFilter2 {#1 seed=[592442048D98540B:676F405A6B451FB0]}
Completed on J0 in 0.35s, 8 tests

JVM J0:     0.66 ..     1.49 =     0.82s
JVM J1:     0.66 ..     1.28 =     0.62s
Execution time total: 1.49 sec.
Tests summary: 2 suites, 16 tests
{code}

Note each JVM will start from the same master seed. Each test will work in its own working directory and JVM  though so it can be useful for performing stress testing of a single suite.","05/Sep/12 10:54;siren;Cool! This is a really nice addition!","05/Sep/12 10:56;dweiss;Thanks! It was Mark's request, actually. I had all the infrastructure ready, I just needed that duplicating resource collection and this was trivial.","22/Mar/13 16:43;commit-tag-bot;[branch_4x commit] Dawid Weiss
http://svn.apache.org/viewvc?view=revision&revision=1381124

LUCENE-4360: Support running the same test suite multiple times in parallel.
","10/May/13 10:32;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thread pool's threads may escape in TestCodecLoadingDeadlock,LUCENE-6666,12843437,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,08/Jul/15 07:46,09/May/16 18:36,30/Sep/19 08:38,08/Jul/15 07:50,,,,,,,,,,,5.3,6.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-07-08 07:48:59.412,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 26 13:05:59 UTC 2015,New,,,,,,,"0|i2gzav:",9223372036854775807,,,,,,,,,"08/Jul/15 07:48;jira-bot;Commit 1689803 from [~dawidweiss] in branch 'dev/trunk'
[ https://svn.apache.org/r1689803 ]

LUCENE-6666: (damn, how evil!) Thread pool's threads may escape in TestCodecLoadingDeadlock","08/Jul/15 07:50;jira-bot;Commit 1689804 from [~dawidweiss] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1689804 ]

LUCENE-6666: Thread pool's threads may escape in TestCodecLoadingDeadlock","08/Jul/15 08:54;jira-bot;Commit 1689819 from [~dawidweiss] in branch 'dev/trunk'
[ https://svn.apache.org/r1689819 ]

LUCENE-6666: reverting this from trunk as the code doesn't use a separate thread killer thread (thanks Uwe).","08/Jul/15 09:02;uschindler;Thanks!","26/Aug/15 13:05;shalinmangar;Bulk close for 5.3.0 release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
_TestUtil should be able to generate random longs,LUCENE-4148,12560786,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,15/Jun/12 14:51,09/May/16 18:36,30/Sep/19 08:38,19/Jun/12 14:29,,,,,,,,,,,4.0-ALPHA,6.0,,,general/test,,,0,,,,"It would be helpful in TestPackedInts at least, in order to generate random values (as a workaround, we currently generate a random int between 0 and {{min(Integer.MAX_VALUE, PackedInts.maxValue(bitsPerValue)}}). Moreover, it would help to fix {{nextInt}} for large ranges (calling {{nextInt(random, -10, Integer.MAX_VALUE)}} or even {{nextInt(random, 0, Integer.MAX_VALUE)}} currently fails because the range of values is {{> Integer.MAX_VALUE}}.",,,,,,,,,,,,,,,,"18/Jun/12 13:49;jpountz;LUCENE-4148.patch;https://issues.apache.org/jira/secure/attachment/12532410/LUCENE-4148.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-15 15:04:11.788,,,false,,,,,,,,,,,,,,,243817,,,Tue Jun 19 14:29:38 UTC 2012,New,,,,,,,"0|i04g33:",23871,,,,,,,,,"15/Jun/12 15:04;dweiss;Try RandomInts from randomizedtesting, it does what you need:
{code}
  /** 
   * A random integer from <code>min</code> to <code>max</code> (inclusive).
   */
  public static int     randomIntBetween(Random r, int min, int max) {
    assert max >= min : ""max must be >= min: "" + min + "", "" + max;
    long range = (long) max - (long) min;
    if (range < Integer.MAX_VALUE) {
      return min + r.nextInt(1 + (int) range);
    } else {
      return min + (int) Math.round(r.nextDouble() * range);
    }
  } 
{code}","15/Jun/12 15:06;rcmuir;can we just switch _TestUtil.nextInt to use this then?","15/Jun/12 15:17;jpountz;bq. Try RandomInts from randomizedtesting

Thanks, it fixes the problem for large range of ints, but I would also like to be able to generate random longs (> Integer.MAX_VALUE) in a fixed range. For {{PackedInts}}, I always need longs between 0 and a power of 2, but I thought it would be more useful to add a general-purpose {{long nextLong(Random, long, long)}} to {{_TestUtil}}.

Since {{Random.nextLong(long)}} doesn't exist, it would be tempting to implement it by ourselves by using the same logic as {{Random.nextInt(int)}} (described here: http://docs.oracle.com/javase/7/docs/api/java/util/Random.html#nextInt(int) ), but I am afraid there would be copyright issues, so we should probably do it otherwise ( ? ).

By the way, the code of {{randomIntBetween}} makes me curious, are all values as likely when the range of values is {{>= Integer.MAX_VALUE}} (ie. when the else block gets executed)?","15/Jun/12 16:58;dweiss;@Robert: sure we could (noting the subtle difference that the range is inclusive on both min and max).

As for distribution... I think it should be uniform over all of the range's values assuming r.nextDouble() yields uniform value from [0,1]? It's simple scaling after all. I didn't really consider nuances of floating point representation here but I think within 2^32 a double should cover everything fairly well. ","17/Jun/12 22:49;jpountz;It seems to me that {{min}} and {{max}} are inclusive in both methods.

@Dawid could RandomInts have a similar method to generate longs? Something like 
{code}
  public static long randomLongBetween(Random r, long min, long max);
{code}","18/Jun/12 08:48;dweiss;I didn't check _TestUtil, to be honest -- I think it did have a bug with range overflow passed to nextInt (which is not a problem in practice, but was my example on how randomized testing can explore unexpected things). 

Anyway, as for randomLongBetween -- here you'd need to be more careful about double's representation so simple scaling won't work (there are ranges of double that transform to NaNs for example). I am not so strong in numerical analysis to be able to prove something is a correct solution to the problem. My (naive) take at this would be to calculate the range and then if split into a few cases:

1) if it doesn't exceed a positive int, use nextInt()
2) if it doesn't exceed a positive long, use nextLong()
3) if it does exceed positive long, use BigDecimal? :) 

Maybe this can be done faster without keeping the uniform distribution requirement so strong (i.e. use a modulo from a combination of two concatenated longs as BigDecimal or something like that).","18/Jun/12 13:49;jpountz;Here is a patch proposition:
 * {{_TestUtil.nextInt}} forwards calls to {{RandomInts.randomIntBetween}}, as Robert suggested,
 * A new {{_TestUtil.nextLong}} uses {{Random.nextInt(int)}} when the range does not exceed {{Integer.MAX_VALUE}} and scales {{Double.nextDouble}} into the expected range of values otherwise using {{BigDecimal}} arithmetic.

I also updated a few tests to use this new method (especially a call to {{Math.abs(random.nextLong())}} in {{TestIndexInput}}, which is buggy since {{random.nextLong()}} might return Long.MIN_VALUE whose absolute value cannot be represented by a long).","18/Jun/12 16:10;dweiss;This looks good to me, although I'm wondering how much randomness do we really need -- those BigIntegers are worrying. I think this can be improved later if somebody wishes to dive into it.

Btw. an interesting random number generator -- 
http://home.southernct.edu/~pasqualonia1/ca/report.html","19/Jun/12 14:29;jpountz;Committed (r1351718 on trunk and r1351728 on branch 4.x). Thanks Robert and Dawid!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port BasicAutomata.stringUnion from Brics to Lucene,LUCENE-3832,12544438,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,28/Feb/12 15:44,09/May/16 18:35,30/Sep/19 08:38,29/Jun/12 10:04,,,,,,,,,,,4.0-BETA,6.0,,,core/FSTs,,,0,,,,Brics has my code to build Automaton from a set of sorted strings in one step (Daciuk/Mihov's algorithm again). This should be easily portable to Lucene and is quite useful.,,,,,,,,,,,,,,,,"28/Jun/12 09:04;dweiss;LUCENE-3832.patch;https://issues.apache.org/jira/secure/attachment/12533791/LUCENE-3832.patch","28/Feb/12 16:01;dweiss;LUCENE-3832.patch;https://issues.apache.org/jira/secure/attachment/12516345/LUCENE-3832.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-28 16:03:41.387,,,false,,,,,,,,,,,,,,,229675,,,Wed Jul 11 23:08:39 UTC 2012,New,,,,,,,"0|i04i0v:",24185,,,,,,,,,"28/Feb/12 16:01;dweiss;A patch with ported stringunion.","28/Feb/12 16:03;mikemccand;+1

Then we can remove the class from test-framework and cutover all uses to this new one?","28/Feb/12 16:31;dweiss;Huh? Sorry, I don't get you -- what do you mean?","28/Feb/12 16:39;uschindler;In test-framework is a helper method to build a Daciuk/Mihov automaton.

With the patch, this somehow makes TermsFilter from contrib obsolete or should we maybe port that to an AutomatonQuery / MTQWF(AutomatonQuery)? If it simply subclasses AQ it could be more performant if the array has terms which are following each other in terms index.","28/Feb/12 16:42;mikemccand;Sorry, what I meant was: we already have (and use, from tests only) this algorithm, in {{lucene/test-framework/src/java/org/apache/lucene/util/automaton/DaciukMihovAutomatonBuilder.java}}.

I agree we should promote it to core: it seems quite useful!

Actually I think there are slight differences vs the attached patch (looks like Robert cutover to CharsRef/BytesRef), so I guess we need to reconcile those... or maybe just move the existing one from test-framework to StringUnionOperations (if there are no *important* differences :) ).","28/Feb/12 19:05;dweiss;I don't know if the original version is a derivative of what I placed in the patch or not (I wrote the one originally in brics). There will be ordering differences between the one based on char and UTF8 so it's not exactly the same; somebody should perhaps look at it from a wider perspective. If you have spare cycles, feel free to take over -- this was really a 5 minute effort and I can't take a deeper look at it at the moment.","29/Feb/12 03:46;rcmuir;The version in test-framework was something i quickly hacked together 
(converted Dawid's implementation to binary order) because some tests 
added for blocktree (TestTermsEnum?) were really slow without it.
","29/Feb/12 07:25;dweiss;We should then just move Robert's port instead of what I pulled from Brics. UTF8 order is omnipresent in Lucene so it should really be that (possibly with a method accepting an iterator/iterable of char sequences and sorting internally?).","29/Feb/12 13:45;mikemccand;bq. We should then just move Robert's port instead of what I pulled from Brics.

+1, as long as we can reconcile that the diffs are not ""meaningful"" :)  Maybe you fixed a bug since we added this to Lucene!

EG a small diff is your version does the .sort() for the caller, but Lucene's version requires caller do the .sort()...","06/Mar/12 02:40;rcmuir;removing 3.6, automaton package doesnt exist there.","28/Jun/12 09:04;dweiss;A patch moving DaciukMihovAutomatonBuilder to automata package and hiding it effectively under BasicAutomata.makeStringUnion.

Minor cleanups.","28/Jun/12 09:04;dweiss;Passes tests for me, I'll commit it in soon if there are no objections.","28/Jun/12 09:44;rcmuir;I don't like the one that takes CharSequence and makes a new arraylist and sorts :)","28/Jun/12 09:57;dweiss;I don't like many things. For example the fact that it accepts UTF-8 but returns an automaton with codepoints on transitions -- this isn't intuitive ;)

I think that method fits well with other methods in that class (accepting strings). Maybe it's a good idea to cutover to utf8 entirely but just having Collection<utf8> and everything else on char sequences seems odd to me.","28/Jun/12 10:01;rcmuir;{quote}
For example the fact that it accepts UTF-8 but returns an automaton with codepoints on transitions – this isn't intuitive
{quote}

Whats unintuitive? UTF-8 and UTF-32 share the same order.","28/Jun/12 10:02;dweiss;You pass an array of bytes and get codepoint (ints) as transitions.","28/Jun/12 10:04;rcmuir;So how is passing an array of shorts better?","28/Jun/12 10:09;dweiss;My whining doesn't mean I want to change this because I realize most of the data in Lucene already comes as UTF8 and it wouldn't make sense to convert it back and forth. This doesn't change my feeling that it isn't intuitive when you look at the method's signature.

Of course it's explained in the javadoc what it actually does, but so is conversion/ sorting in that other method that you don't like (and that I think is generally useful considering the rest of this package).","29/Jun/12 09:48;dweiss;It'd be possible to avoid the sort and still make makeUnionOfStrings(Collection<String>) possible if we also exposed Utf16AsUtf8 comparator... Then the automaton builder could accept either byte[] or char[] and just do its job without any additional overhead of copying. 

Ok, I'll remove that method if you don't like it. If there's a need we can revert it using the trick above.","11/Jul/12 23:08;hossman;hoss20120711-manual-post-40alpha-change",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make empty doc values impls singletons,LUCENE-5834,12728369,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,19/Jul/14 10:43,09/May/16 18:35,30/Sep/19 08:38,21/Jul/14 11:32,,,,,,,,,,,4.10,6.0,,,,,,0,,,,Making these empty instances singletons would allow to use {{unwrapSingleton}} to check if they are single-valued.,,,,,,,,,,,,,,,,"19/Jul/14 10:45;jpountz;LUCENE-5834.patch;https://issues.apache.org/jira/secure/attachment/12656720/LUCENE-5834.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-19 12:54:41.181,,,false,,,,,,,,,,,,,,,406445,,,Mon Jul 21 11:33:11 UTC 2014,New,,,,,,,"0|i1xxxz:",406465,,,,,,,,,"19/Jul/14 10:45;jpountz;Patch.","19/Jul/14 12:54;rcmuir;+1","21/Jul/14 11:31;jira-bot;Commit 1612251 from [~jpountz] in branch 'dev/trunk'
[ https://svn.apache.org/r1612251 ]

LUCENE-5834: Empty sorted set and numeric doc values are now singletons.","21/Jul/14 11:33;jira-bot;Commit 1612252 from [~jpountz] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1612252 ]

LUCENE-5834: Empty sorted set and numeric doc values are now singletons.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generalize how IndexSearcher parallelizes collection execution,LUCENE-6294,12777511,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,25/Feb/15 12:41,09/May/16 18:35,30/Sep/19 08:38,27/Feb/15 17:38,,,,,,,,,,,5.1,6.0,,,,,,0,,,,"IndexSearcher takes an ExecutorService that can be used to parallelize collection execution. This is useful if you want to trade throughput for latency.

However, this executor service will only be used if you search for top docs. In that case, we will create one collector per slide and call TopDocs.merge in the end. If you use search(Query, Collector), the executor service will never be used.

But there are other collectors that could work the same way as top docs collectors, eg. TotalHitCountCollector. And maybe also some of our users' collectors. So maybe IndexSearcher could expose a generic way to take advantage of the executor service?",,,,,,,,,,,,,,LUCENE-5299,,"25/Feb/15 12:48;jpountz;LUCENE-6294.patch;https://issues.apache.org/jira/secure/attachment/12700749/LUCENE-6294.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-02-26 04:44:43.968,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 00:30:34 UTC 2015,New,,,,,,,"0|i2614n:",9223372036854775807,,,,,,,,,"25/Feb/15 12:48;jpountz;Here is a patch that demonstrates the idea. This does not change any API on Collector since not all collectors could work this way, but adds a CollectorManager object which can create collectors and merge them. I cut over top docs collection to this new API and also added IndexSearcher.count to exercise it.","26/Feb/15 04:44;dsmiley;I didn't look it over in great detail but I like it.  At first I was hoping that there might be a Collector subclass to declare it's parallel-izability with the reduce method but then realized it wouldn't look good since the factory method to create itself wouldn't feel right.","26/Feb/15 22:22;jpountz;Thanks for the feedback David!","27/Feb/15 09:43;mikemccand;+1, I like this approach.","27/Feb/15 16:23;rjernst;+1

In the javadocs for {{IndexSearcher.search}} I think you mean ""In contrast to"" instead of ""On the contrary to""?","27/Feb/15 17:06;jira-bot;Commit 1662751 from [~jpountz] in branch 'dev/trunk'
[ https://svn.apache.org/r1662751 ]

LUCENE-6294: Generalize how IndexSearcher parallelizes collection execution.","27/Feb/15 17:38;jpountz;Thanks David, Mike and Ryan for the reviews!","27/Feb/15 17:40;jira-bot;Commit 1662761 from [~jpountz] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1662761 ]

LUCENE-6294: Generalize how IndexSearcher parallelizes collection execution.","27/Feb/15 18:42;shikhar;This is great. I saw some improvements when testing LUCENE-5299 with the addition of a configurable parallelism throttle at the search request level using a semaphore, that might be useful to have here too. I.e. being able to cap how many segments are concurrently searched. That can help ensure resources for concurrent search requests, or reduce context switching if using an unbounded pool.","27/Feb/15 18:56;jpountz;I think a better approach than the semaphore would be to just cap the number of slices of your searcher (see IndexSearcher.slices).","27/Feb/15 19:13;shikhar;Makes sense! Seems to be already customizable by overriding that method.","27/Feb/15 19:25;shikhar;When slicing differently than segment-per-slice, it'd probably be desirable to distribute segments by size across the slices, rather than all large segments ending up in one slice to be searched sequentially.","15/Apr/15 00:30;thelabdude;Bulk close after 5.1 release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade of RandomizedRunner to version 2.2.0,LUCENE-6862,12908642,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,28/Oct/15 17:47,09/May/16 18:35,30/Sep/19 08:38,29/Oct/15 09:47,,,,,,,,,,,5.4,6.0,,,,,,0,,,,,,,,,,,,,,,,,,,,"28/Oct/15 18:32;dweiss;LUCENE-6862.patch;https://issues.apache.org/jira/secure/attachment/12769374/LUCENE-6862.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-10-28 18:43:39.569,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 29 09:47:12 UTC 2015,New,,,,,,,"0|i2nn9j:",9223372036854775807,,,,,,,,,"28/Oct/15 18:32;dweiss;Patch, spatial4j uses a dependency (spatial4j-0.5-tests.jar) that in turn seems to be referencing methods from another version of RandomizedTest -- can't commit because the build currently breaks with no such method. [~dsmiley@mac.com] could you take a look?
{code}
  2> NOTE: reproduce with: ant test  -Dtestcase=Geo3dShapeSphereModelRectRelationTest -Dtests.method=testGeoBBoxRect -Dtests.seed=54887386AAEC5BEF -Dtests.slow=true -Dtests.locale=fi -Dtests.timezone=Pacific/Pitcairn -Dtests.asserts=true -Dtests.file.encoding=UTF-8
[19:19:25.647] ERROR   0.04s J1 | Geo3dShapeSphereModelRectRelationTest.testGeoBBoxRect <<<
   > Throwable #1: java.lang.NoSuchMethodError: com.spatial4j.core.shape.RandomizedShapeTest.assertEquals(Ljava/lang/Object;Ljava/lang/Object;)V
   > 	at __randomizedtesting.SeedInfo.seed([54887386AAEC5BEF:702DDBBE8EDF25B1]:0)
   > 	at com.spatial4j.core.shape.RandomizedShapeTest.randomPointIn(RandomizedShapeTest.java:257)
{code}","28/Oct/15 18:43;dsmiley;Thorny issue.  Geo3dShape (in Lucene spatial) implements Shape (a Spatial4j interface), and I've always thought it ideally would exist and be provided by Spatial4j, and thus be tested there too.  This is the only bit of integration there right now.  It's been on my todo list.  I think I vote to mark these tests as \@Ignore for now, but Geo3dRptTest can stay as it's not impacted and fundamentally is a Lucene (spatial) thing.  Feel free to do so with a comment, obviously.","28/Oct/15 18:48;dweiss;I can wait until you upgrade RR in spatial4j -- not a problem with me. Unless it's going to take a long time, in which  case I can mark those offending tests as ignored.","29/Oct/15 04:47;dsmiley;I suggest marking them as ignored -- all the more motivation for me to get on with moving Geo3dShape to Spatial4j.","29/Oct/15 09:38;jira-bot;Commit 1711203 from [~dawidweiss] in branch 'dev/trunk'
[ https://svn.apache.org/r1711203 ]

LUCENE-6862: Upgrade of RandomizedRunner to version 2.2.0","29/Oct/15 09:47;jira-bot;Commit 1711205 from [~dawidweiss] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1711205 ]

LUCENE-6862: Upgrade of RandomizedRunner to version 2.2.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test output should include timestamps (start/end for each test/ suite).,LUCENE-4189,12597428,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,04/Jul/12 09:57,09/May/16 18:35,30/Sep/19 08:38,05/Jul/12 10:49,,,,,,,,,,,4.0-BETA,6.0,,,,,,0,,,,"This adds more verboseness to the output -- should this be optional (overrideable using local properties but defaulting to 'off')?

{code}
   [junit4] [11:54:50.259] Suite: org.apache.lucene.index.TestDeletionPolicy
   [junit4] [11:54:53.706] Completed in 3.45s, 6 tests
   [junit4]  
   [junit4] [11:54:53.709] Suite: org.apache.lucene.util.TestVirtualMethod
   [junit4] [11:54:53.725] Completed in 0.02s, 2 tests
   [junit4]  
   [junit4] [11:54:53.728] Suite: org.apache.lucene.index.TestRollingUpdates
   [junit4] [11:54:55.700] Completed in 1.97s, 2 tests
   [junit4]  
   [junit4] [11:54:55.721] Suite: org.apache.lucene.index.TestIndexWriterExceptions
   [junit4] [11:55:02.394] Completed in 6.67s, 24 tests
   [junit4]  
   [junit4] [11:55:02.398] Suite: org.apache.lucene.index.TestNoDeletionPolicy
   [junit4] [11:55:02.548] Completed in 0.15s, 4 tests
...
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-07-04 10:37:48.974,,,false,,,,,,,,,,,,,,,243776,,,Wed Jul 11 23:13:26 UTC 2012,New,,,,,,,"0|i04ftz:",23830,,,,,,,,,"04/Jul/12 10:37;rcmuir;{quote}
should this be optional (overrideable using local properties but defaulting to 'off')?
{quote}

please!","11/Jul/12 23:13;hossman;hoss20120711-manual-post-40alpha-change",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ComplexPhraseQueryParser.ComplexPhraseQuery does not display slop in toString(),LUCENE-6817,12896492,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,25/Sep/15 20:57,09/May/16 18:35,30/Sep/19 08:38,06/Oct/15 12:50,,,,,,,,,,,5.4,6.0,,,,,,0,,,,"This one is quite simple (I think) -- ComplexPhraseQuery doesn't display the slop factor which, when the result of parsing is dumped to logs, for example, can be confusing.

I'm heading for a weekend out of office in a few hours... so in the spirit of not committing and running away ( :) ), if anybody wishes to tackle this, go ahead.
",,,,,,,,,,,,,,,,"28/Sep/15 11:50;iorixxx;LUCENE-6817.patch;https://issues.apache.org/jira/secure/attachment/12764002/LUCENE-6817.patch","28/Sep/15 11:49;iorixxx;LUCENE-6817.patch;https://issues.apache.org/jira/secure/attachment/12764001/LUCENE-6817.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-10-06 12:49:08.963,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 12:50:18 UTC 2015,New,,,,,,,"0|i2lkwf:",9223372036854775807,,,,,,,,,"06/Oct/15 12:49;jira-bot;Commit 1707043 from [~dawidweiss] in branch 'dev/trunk'
[ https://svn.apache.org/r1707043 ]

LUCENE-6817: ComplexPhraseQueryParser.ComplexPhraseQuery does not display slop in toString().","06/Oct/15 12:49;jira-bot;Commit 1707044 from [~dawidweiss] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1707044 ]

LUCENE-6817: ComplexPhraseQueryParser.ComplexPhraseQuery does not display slop in toString().","06/Oct/15 12:50;dweiss;Thanks Ahmet!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement toString() method in TermsFilter,LUCENE-4009,12552026,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,tcostermans,tcostermans,22/Apr/12 08:59,09/May/16 18:35,30/Sep/19 08:38,03/Dec/12 13:41,3.5,3.6,,,,,,,,,4.1,6.0,,,modules/other,,,0,,,,"LUCENE-1049 introduced a enhanced implementation of the toString() method in the BooleanFilter clause.
This was an improvement, however I'm still seeing a lot Lucene filter classes not overriding the toString method resulting in a toString returning the classname and the hashcode of the object. 
This can be useful sometimes, but it's totally not useful in my case.
I want to see the properties set in the filters so I know which Lucene query was created.

Now:
BooleanFilter(+BooleanFilter(BooleanFilter(+org.apache.lucene.search.TermsFilter@ea81ba60 +org.apache.lucene.search.TermsFilter@26ea3cbc) BooleanFilter(+org.apache.lucene.search.TermsFilter@df621f09 +org.apache.lucene.search.TermsFilter@2f712446)))

Wanted behavior:
BooleanFilter(+BooleanFilter(BooleanFilter(+inStock:Y +barCode:12345678) BooleanFilter(+isHeavy:N +isDamaged:Y)))
",,,,,,,,,,,,,,,,"30/Apr/12 04:11;cmale;LUCENE-4009.patch;https://issues.apache.org/jira/secure/attachment/12525038/LUCENE-4009.patch","22/Apr/12 09:45;tcostermans;LUCENE-4009__Improved_previous_implementation_of__toString()_method_in_TermsFilter_+_added.patch;https://issues.apache.org/jira/secure/attachment/12523676/LUCENE-4009__Improved_previous_implementation_of__toString%28%29_method_in_TermsFilter_%2B_added.patch","22/Apr/12 09:07;tcostermans;LUCENE-4009__implemented_toString()_method_in_TermsFilter_.patch;https://issues.apache.org/jira/secure/attachment/12523673/LUCENE-4009__implemented_toString%28%29_method_in_TermsFilter_.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-04-22 09:15:16.364,,,false,,,,,,,,,,,,,,,236917,,,Fri May 10 10:34:02 UTC 2013,New,Patch Available,,,,,,"0|i04gyf:",24012,,,,,,,,,"22/Apr/12 09:07;tcostermans;Patch + test","22/Apr/12 09:15;cmale;Hey Tim,

bq. however I'm still seeing a lot Lucene filter classes not overriding the toString

Know of any more? If so, want to put together a bigger patch and we'll tackle them here?","22/Apr/12 09:45;tcostermans;Discard previous patch. Only import this one. Improved implementation of toString() in TermsFilter + added a test for it.  ","22/Apr/12 09:49;tcostermans;Hey Chris,

Basically all the subclasses of org.apache.lucene.search.Filter should implement their own toString() method.
Maybe this can be enforced by adding this to the Filter class.
{code}
@Override
public abstract String  toString();
{code}
I was going to investigate this, but this will be something for next week.","30/Apr/12 04:11;cmale;Hi Tim

I've simplified the patch a little, can you check you're okay with it? If so, I'll commit.","01/Dec/12 10:12;t.costermans;Hi Chris,
Sorry for the delay, I didn't notice your comment.
I'm ok with your changes.
You can commit this.
","03/Dec/12 13:37;mikemccand;Thanks Tim, I'll merge & commit ...","03/Dec/12 13:41;mikemccand;Thanks Tim!","03/Dec/12 13:42;commit-tag-bot;[trunk commit] Michael McCandless
http://svn.apache.org/viewvc?view=revision&revision=1416509

LUCENE-4009: improve TermsFilter.toString

","03/Dec/12 13:46;commit-tag-bot;[branch_4x commit] Michael McCandless
http://svn.apache.org/viewvc?view=revision&revision=1416513

LUCENE-4009: improve TermsFilter.toString

","22/Mar/13 16:12;commit-tag-bot;[branch_4x commit] Michael McCandless
http://svn.apache.org/viewvc?view=revision&revision=1416513

LUCENE-4009: improve TermsFilter.toString
","10/May/13 10:34;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collapse identical catch branches in try-catch statements,LUCENE-6193,12769148,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,shalinmangar,shalinmangar,shalinmangar,21/Jan/15 23:19,09/May/16 18:34,30/Sep/19 08:38,21/Jan/15 23:50,,,,,,,,,,,5.1,6.0,,,,,,0,,,,We are on Java 7+ so we can reduce verbosity by collapsing identical catch statements into one. We did the same for solr in SOLR-7014.,,,,,,,,,,,,SOLR-7014,,,,"21/Jan/15 23:20;shalinmangar;LUCENE-6193.patch;https://issues.apache.org/jira/secure/attachment/12693741/LUCENE-6193.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-01-21 23:48:36.744,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 00:30:37 UTC 2015,,,,,,,,"0|i24n4v:",9223372036854775807,,,,,,,,,"21/Jan/15 23:20;shalinmangar;The only places where I did not make these changes are where the catch blocks have different comments or the code wasn't ASL.

The following were excluded:
# org.apache.lucene.analysis.core.TestFactories
# org.apache.lucene.index.TestReaderClosed
# org.apache.lucene.queryparser.flexible.messages.NLS (one instance)
# org.egothor.stemmer.Diff (license different from ASL)
# org.tartarus.snowball.SnowballProgram (license different from ASL)","21/Jan/15 23:48;jira-bot;Commit 1653707 from shalin@apache.org in branch 'dev/trunk'
[ https://svn.apache.org/r1653707 ]

LUCENE-6193: Collapse identical catch branches in try-catch statements","21/Jan/15 23:49;jira-bot;Commit 1653709 from shalin@apache.org in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1653709 ]

LUCENE-6193: Collapse identical catch branches in try-catch statements","22/Jan/15 07:34;uschindler;Hi,
FYI: If you had in the past the multiple catch statements around reflection errors, all those exceptions have new superclass in Java 7: ReflectiveOperationException. This can sometimes simplify the code more! This applies in most cases around where you have stuff like ClassNotFoundException, IllegalAccessException, InstantiationException, InvocationTargetException, NoSuchFieldException, NoSuchMethodException, all handled by identical catch.

Just catching ReflectiveOperationException around reflection access is in most cases all you need now.","22/Jan/15 15:21;jira-bot;Commit 1653909 from shalin@apache.org in branch 'dev/trunk'
[ https://svn.apache.org/r1653909 ]

LUCENE-6193: Use ReflectiveOperationException to simplify catch clauses around reflection errors","22/Jan/15 15:22;jira-bot;Commit 1653911 from shalin@apache.org in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1653911 ]

LUCENE-6193: Use ReflectiveOperationException to simplify catch clauses around reflection errors","22/Jan/15 15:24;shalinmangar;Thanks Uwe! I changed LockStressTest and JavascriptCompiler to use ReflectiveOperationException.","15/Apr/15 00:30;thelabdude;Bulk close after 5.1 release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix IndexWriter working together with EmptyTokenizer and EmptyTokenStream (without CharTermAttribute), fix BaseTokenStreamTestCase",LUCENE-4656,12625845,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,jpountz,jpountz,03/Jan/13 12:50,09/May/16 18:34,30/Sep/19 08:38,03/Jan/13 23:45,4.0,,,,,,,,,,4.1,6.0,,,modules/analysis,,,0,,,,TestRandomChains can fail because EmptyTokenizer doesn't have a CharTermAttribute and doesn't compute the end offset (if the offset attribute was added by a filter).,,,,,,,,,,,,,,,,"03/Jan/13 16:13;uschindler;LUCENE-4656-IW-bug.patch;https://issues.apache.org/jira/secure/attachment/12563085/LUCENE-4656-IW-bug.patch","03/Jan/13 16:52;uschindler;LUCENE-4656-IW-fix.patch;https://issues.apache.org/jira/secure/attachment/12563092/LUCENE-4656-IW-fix.patch","03/Jan/13 16:49;uschindler;LUCENE-4656-IW-fix.patch;https://issues.apache.org/jira/secure/attachment/12563091/LUCENE-4656-IW-fix.patch","03/Jan/13 18:42;uschindler;LUCENE-4656.patch;https://issues.apache.org/jira/secure/attachment/12563126/LUCENE-4656.patch","03/Jan/13 17:32;uschindler;LUCENE-4656.patch;https://issues.apache.org/jira/secure/attachment/12563107/LUCENE-4656.patch","03/Jan/13 17:28;uschindler;LUCENE-4656.patch;https://issues.apache.org/jira/secure/attachment/12563105/LUCENE-4656.patch","03/Jan/13 17:17;uschindler;LUCENE-4656.patch;https://issues.apache.org/jira/secure/attachment/12563096/LUCENE-4656.patch","03/Jan/13 15:11;jpountz;LUCENE-4656.patch;https://issues.apache.org/jira/secure/attachment/12563077/LUCENE-4656.patch","03/Jan/13 12:55;jpountz;LUCENE-4656.patch;https://issues.apache.org/jira/secure/attachment/12563070/LUCENE-4656.patch","03/Jan/13 17:16;rcmuir;LUCENE-4656_bttc.patch;https://issues.apache.org/jira/secure/attachment/12563095/LUCENE-4656_bttc.patch",,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,2013-01-03 12:55:11.228,,,false,,,,,,,,,,,,,,,302391,,,Sat Jan 05 20:28:17 UTC 2013,New,,,,,,,"0|i170jz:",249057,,,,,,,,,"03/Jan/13 12:55;rcmuir;I think this might be a broken assert in BaseTokenStreamTestCase?

    assertTrue(""has no CharTermAttribute"", ts.hasAttribute(CharTermAttribute.class));

Why do we have that? This assert can be safely removed i think.","03/Jan/13 12:55;jpountz;Patch. I wasn't sure whether to add a CharTermAttribute to EmptyTokenizer or to try fixing BaseTokenStreamTestCase but I couldn't think of a non-trivial tokenizer that wouldn't have a CharTermAttribute so I left the assertion that checks that a token stream always has a CharTermAttribute.","03/Jan/13 12:58;jpountz;bq. Why do we have that?

It feels strange to me that a non-trivial TokenStream could have no CharTermAttribute?","03/Jan/13 12:58;rcmuir;I think we should fix BaseTokenStreamTestCase but your patch fixes other things about this Tokenizer?

Like it fixes its end() to work and makes it consume from its reader.","03/Jan/13 13:02;rcmuir;{quote}
It feels strange to me that a non-trivial TokenStream could have no CharTermAttribute?
{quote}

I dont think analysis components should add attributes they dont really need: it should only 
be the minimal ones they need to work. so I think that extends to this empty one?

","03/Jan/13 13:09;uschindler;I am happy with both solutions. I agree, that only the really used attributes should be added. There are other tokenstream that dont have a CTA, e.g. NumericTokenStream. For the indexer, only BytesRefAttribute is mandatory (I am not sure it is really *mandatory*, but it is the only one that it is consumed to get the term text).","03/Jan/13 13:12;uschindler;We should at least test EmptyTokenizer with IndexWrite.

By the way: EmptyTokenizer is wrong name, as it needs no input reader. It should *only* extend TokenStream. That's the real bug here!","03/Jan/13 13:13;rcmuir;There is an EmptyTokenStream in src/java that is correct.

EmptyTokenizer is in test-framework, for some crazy test reason. If its not being used lets remove it!","03/Jan/13 15:11;jpountz;Alternative patch that fixes BaseTokenStreamTestCase. I needed to add a quick hack to add a TermToBytesRefAttribute when the tokenstream doesn't have one so that TermsHashPerField doesn't complain that it can't find this attribute when indexing.","03/Jan/13 15:27;uschindler;bq. I needed to add a quick hack to add a TermToBytesRefAttribute when the tokenstream doesn't have one so that TermsHashPerField doesn't complain that it can't find this attribute when indexing.

But this is a bug. If you use the non-test EmptyTokenStream from the analysis-common package and add it to a document it will fail, right? So we should fix IndexWriter to handle that case?","03/Jan/13 15:36;jpountz;bq. So we should fix IndexWriter to handle that case?

How would IndexWriter handle token streams with no TermToBytesRefAttribute?
 - fail if the tokens stream happens to have tokens? (incrementToken returns true at least once)
 - index empty terms?","03/Jan/13 15:51;rcmuir;I'm not really certain what it should do.

it does seem a little funky that even though incrementToken returned false we start() consumer anyway though.","03/Jan/13 15:56;uschindler;The problem is here that the attributes are initialized after construction of the Tokenizer before the consumer starts to consume the tokens. The bug in IndexWriter is that it fails, when the initial getAttribute fails. Maybe it should just initialize the bytesRef attribute to be NULL and fail later if really tokens are emitted.

Lucene 3.x indexed empty terms.","03/Jan/13 16:13;uschindler;Here a patch showing the bug in the public class EmptyTokenStream from analysis-common working together with IndexWriter:

{noformat}
[junit4:junit4] ERROR   0.33s | TestEmptyTokenStream.testIndexWriter_LUCENE4656 <<<
[junit4:junit4]    > Throwable #1: java.lang.IllegalArgumentException: This AttributeSource does not have the attribute 'org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute'.
[junit4:junit4]    >    at __randomizedtesting.SeedInfo.seed([3B209861053849AF:D7B239E3D4067832]:0)
[junit4:junit4]    >    at org.apache.lucene.util.AttributeSource.getAttribute(AttributeSource.java:303)
[junit4:junit4]    >    at org.apache.lucene.index.TermsHashPerField.start(TermsHashPerField.java:119)
[junit4:junit4]    >    at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:109)
[junit4:junit4]    >    at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:272)
[junit4:junit4]    >    at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:250)
[junit4:junit4]    >    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:376)
[junit4:junit4]    >    at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
[junit4:junit4]    >    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1131)
[junit4:junit4]    >    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1112)
[junit4:junit4]    >    at org.apache.lucene.analysis.miscellaneous.TestEmptyTokenStream.testIndexWriter_LUCENE4656(TestEmptyTokenSt{noformat}

It also has a test that assertTokenStreamContents actually works, which it doesnt at the moment, because it asserts that the CTA is available. But NumericTokenStream *and* this one both dont have this attribute.","03/Jan/13 16:49;uschindler;Here the fix that solves the DocInverterPerField issue (it also removes the horrible for(;;) loop where the first clause is a ""if ... break"".

Now only BaseTokenStreamTestCase should be able to handle the missing attribute. It should *only* complain when actually tokens are emitted.","03/Jan/13 16:52;uschindler;Better patch, ueses do...while, which is more readable.","03/Jan/13 17:16;rcmuir;here's a patch for BaseTokenStreamTestCase. I think it should work for this EmptyTokenizer too.","03/Jan/13 17:17;uschindler;New patch merged with Adrien's. I am not sure if the Fix in BaseTokenStreamTestCase is correct, because if you pass the String[] you expect tokens and the fix is different like the one for offsets or positionincrements.","03/Jan/13 17:24;uschindler;Thanks Robert! I will merge again and I took the issue!","03/Jan/13 17:28;uschindler;Patch merged with Robert's.","03/Jan/13 17:32;uschindler;Add a check that the document is really in IW after indexing.","03/Jan/13 17:37;rcmuir;Slightly related to the BaseToken changes, i think its confusing how we use output.length (from the String[]) also as the number of expected tokens.

we could clear this up with something like:
{noformat}
@@ -114,21 +114,32 @@
   public static void assertTokenStreamContents(...
     assertNotNull(output);
+    final int numExpected = output.length;
{noformat}

and then use this in the for loop and such.

additionally i've often sent the wrong number of parameters when writing tests because you are passing huge parallel arrays.
so something like this could save some trouble:

{noformat}
     TypeAttribute typeAtt = null;
     if (types != null) {
       assertTrue(""has no TypeAttribute"", ts.hasAttribute(TypeAttribute.class));
       typeAtt = ts.getAttribute(TypeAttribute.class);
+      assertEquals(""wrong number of types"", numExpected, types.length);
     }
{noformat}

We don't have to do these changes here. it just reminded me of it looking at this stuff.
","03/Jan/13 17:51;rcmuir;I would also say that we dont need EmptyTokenizer in test-framework. 
Its only there because 2 places use it, and both in a bogus way (in my opinion):
1. core/TestDocument
2. queryparsers

we should first fix TestDocument, its test does not care if the tokenstream is empty or anything:
{noformat}
Index: src/test/org/apache/lucene/document/TestDocument.java
===================================================================
--- src/test/org/apache/lucene/document/TestDocument.java	(revision 1428441)
+++ src/test/org/apache/lucene/document/TestDocument.java	(working copy)
@@ -20,7 +20,7 @@
 import java.io.StringReader;
 import java.util.List;
 
-import org.apache.lucene.analysis.EmptyTokenizer;
+import org.apache.lucene.analysis.CannedTokenStream;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
@@ -318,7 +318,7 @@
   // LUCENE-3616
   public void testInvalidFields() {
     try {
-      new Field(""foo"", new EmptyTokenizer(new StringReader("""")), StringField.TYPE_STORED);
+      new Field(""foo"", new CannedTokenStream(), StringField.TYPE_STORED);
       fail(""did not hit expected exc"");
     } catch (IllegalArgumentException iae) {
       // expected
{noformat}

The queryparser test looks outdated, like its some test about when an Analyzer returns null?
Maybe the test can just be removed, but if we apply this patch, we could move EmptyTokenizer 
from test-framework/src/java to queryparser/src/test at least as an improvement, since it is kinda funky.
","03/Jan/13 17:52;uschindler;I would really like to remove that horrible piece of sh* :-)","03/Jan/13 18:01;jpountz;Uwe, I just ran all Lucene tests with your patch and they passed, so +1. +1 to removing EmptyTokenizer too.","03/Jan/13 18:03;uschindler;In trunk it is not even used in core! Only in 4.x's TestDocument!","03/Jan/13 18:05;uschindler;Patch with Tokenizer completely removed!

In trunk its completely useless in core (not used), in moudles/qp it is just used to supply a Tokenizer that returns no tokens at all (not null, just empty tokenstream). MockTokenizer is fine for this, too.","03/Jan/13 18:13;uschindler;Sorry was wrong patch.","03/Jan/13 18:29;uschindler;Correct patch. TestDocument actually used it, too - sorry.
This patch also makes the QueryParser Analyzer correctly reuse. The trick is to simply override initReader() in the Analyzer to return an empty one and close the original reader.","03/Jan/13 18:30;uschindler;I also cleaned up some imports in affected files. I will commit this later and backport.","03/Jan/13 23:45;uschindler;I committed the latest patch. Thanks Adrien and Robert!","05/Jan/13 20:27;commit-tag-bot;[branch_4x commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1428675

Merged revision(s) 1428671 from lucene/dev/trunk:
LUCENE-4656: Fix regression in IndexWriter to work with empty TokenStreams that have no TermToBytesRefAttribute (commonly provided by CharTermAttribute), e.g., oal.analysis.miscellaneous.EmptyTokenStream. Remove EmptyTokenizer from test-framework.
","05/Jan/13 20:28;commit-tag-bot;[trunk commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1428671

LUCENE-4656: Fix regression in IndexWriter to work with empty TokenStreams that have no TermToBytesRefAttribute (commonly provided by CharTermAttribute), e.g., oal.analysis.miscellaneous.EmptyTokenStream. Remove EmptyTokenizer from test-framework.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test duration statistics from multiple test runs should be reused (locally).,LUCENE-4654,12625817,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,03/Jan/13 07:48,09/May/16 18:34,30/Sep/19 08:38,04/Mar/13 12:42,,,,,,,,,,,4.2,6.0,,,,,,0,,,,"This is trivial to accomplish: when somebody (or jenkins) runs tests multiple times the execution statistics could be reused to improve load balancing on the local machine (local hardware and settings) in favor of the precached values currently version in the svn repo.

At this moment we already do this, but keep the stats under build/ and every ant clean effectively removes them. I could move those stats under an svn-ignored folder elsewhere so that these stats are not lost and reused for balancing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-04 12:44:14.262,,,false,,,,,,,,,,,,,,,302363,,,Fri May 10 10:34:08 UTC 2013,New,,,,,,,"0|i170dz:",249030,,,,,,,,,"04/Mar/13 12:42;dweiss;I've implemented this and committed it in along with an update to precomputed statistics.

This will *not* make your tests a whole lot faster because there is job-stealing by default anyway so unless you really hit a long test scheduled at the very end of a series of per-jvm tests it won't make a difference.

Obviously it won't make any difference if you're running with a single JVM either.

What may make a (small) difference is for people with a lot of cores or, paradoxically, a few cores on slower machines. I think it's an improvement over the existing version anyway so it's for the better.
","04/Mar/13 12:44;commit-tag-bot;[trunk commit] Dawid Weiss
http://svn.apache.org/viewvc?view=revision&revision=1452276

LUCENE-4654: Test duration statistics from multiple test runs should be reused (locally).
","04/Mar/13 12:54;commit-tag-bot;[branch_4x commit] Dawid Weiss
http://svn.apache.org/viewvc?view=revision&revision=1452277

LUCENE-4654: Test duration statistics from multiple test runs should be reused (locally).
","10/May/13 10:34;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split monster tests in Test2BSortedDocValues out into their own suites so that they can be run in parallel,LUCENE-6516,12834375,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,sarowe,sarowe,sarowe,01/Jun/15 21:13,09/May/16 18:33,30/Sep/19 08:38,02/Jun/15 15:34,,,,,,,,,,,5.3,6.0,,,,,,0,,,,"The two monster tests in {{Test2BSortedDocValues}} each take a long time to run, e.g. from [http://jenkins.sarowe.net/job/Lucene-core-nightly-monster-trunk/23/consoleText]:

{noformat}
   [junit4] HEARTBEAT J0 PID(29118@localhost): 2015-05-31T07:41:18, stalled for 5231s at: Test2BSortedDocValues.test2BOrds
[...]
   [junit4] HEARTBEAT J0 PID(29118@localhost): 2015-05-31T08:04:18, stalled for 1329s at: Test2BSortedDocValues.testFixedSorted
{noformat}

If each of these tests were in its own suite, then when run with multiple JVMs, they could be run in parallel rather than serially.

When I do this locally using 4 JVMs, the Lucene core nightly+monster tests complete about 20-25 minutes faster (~95 minutes vs. ~120 minutes).",,,,,,,,,,,,,,,,"01/Jun/15 21:19;sarowe;LUCENE-6516.patch;https://issues.apache.org/jira/secure/attachment/12736665/LUCENE-6516.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-06-01 21:18:13.927,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 26 13:05:53 UTC 2015,New,Patch Available,,,,,,"0|i2fh8f:",9223372036854775807,,,,,,,,,"01/Jun/15 21:18;dweiss;You have a, ehm, non-standard, slightly above average, machine, Steve ;D

Just curious if any mere mortal will be able to run these in parallel!... ","01/Jun/15 21:19;sarowe;Patch that splits the two tests into their own suites: {{Test2BSortedDocValuesFixedSorted}} and {{Test2BSortedDocValuesOrds}}.

If there are no objections I'll commit tomorrow.","01/Jun/15 21:39;sarowe;bq. You have a, ehm, non-standard, slightly above average, machine, Steve ;D

True: Intel 3GHz Core i7-5960X Haswell-E (8-real-cores); 64GB DDR4 RAM (running @2400MHz); Intel 750-series NVMe SSD.

bq. Just curious if any mere mortal will be able to run these in parallel!...

[~mikemccand] reported that -Dtests.heapsize=30g was required to run some (I think only one?) of the monster tests, so to run them at all one needs lots-o-RAM; mere mortality is insufficient to run them serially even.

It shouldn't harm serial monster test runners to split these out, I don't think?  IMHO every monster test should be in its own suite, to enable parallel execution.
","02/Jun/15 07:13;dweiss;Yeah, ok. Ideally it'd be nice to pick the number of concurrent processors automatically based on the amount of ram one has, but this is still on the todo list...","02/Jun/15 15:27;jira-bot;Commit 1683128 from [~steve_rowe] in branch 'dev/trunk'
[ https://svn.apache.org/r1683128 ]

LUCENE-6516: Split monster tests in Test2BSortedDocValues out into their own suites so that they can be run in parallel","02/Jun/15 15:33;jira-bot;Commit 1683129 from [~steve_rowe] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1683129 ]

LUCENE-6516: Split monster tests in Test2BSortedDocValues out into their own suites so that they can be run in parallel (merged trunk r1683128)","26/Aug/15 13:05;shalinmangar;Bulk close for 5.3.0 release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make top-level default ant task print -projecthelp,LUCENE-4432,12609155,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,uschindler,uschindler,26/Sep/12 00:46,09/May/16 18:33,30/Sep/19 08:38,26/Sep/12 13:03,,,,,,,,,,,4.0,4.1,6.0,,general/build,,,0,,,,"The top-level ant default task runs test, which is confusing. Instead it should alias ""-projecthelp"". This is easily possible with a trick: invoke ant's main class with <java fork=""false""/>.

Solr has a good default task, it may also use the same trick, but I don't care here.

Lucene's default task is jar-core... Maybe move the above task to common-build.xml and let all default targets depend on ""-projecthelp"" task from common-build.xml?",,,,,,,,,,,,,,,,"26/Sep/12 01:40;uschindler;LUCENE-4432.patch;https://issues.apache.org/jira/secure/attachment/12546631/LUCENE-4432.patch","26/Sep/12 01:20;uschindler;LUCENE-4432.patch;https://issues.apache.org/jira/secure/attachment/12546626/LUCENE-4432.patch","26/Sep/12 00:47;uschindler;LUCENE-4432.patch;https://issues.apache.org/jira/secure/attachment/12546623/LUCENE-4432.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-09-26 00:52:17.948,,,false,,,,,,,,,,,,,,,239751,,,Fri May 10 10:34:36 UTC 2013,New,,,,,,,"0|i00tr3:",2752,,,,,,,,,"26/Sep/12 00:47;uschindler;Simple task for root only:

{noformat}
C:\Users\Uwe Schindler\Projects\lucene\trunk-lusolr1>ant
Buildfile: C:\Users\Uwe Schindler\Projects\lucene\trunk-lusolr1\build.xml

-projecthelp:
        [-] Buildfile: C:\Users\Uwe Schindler\Projects\lucene\trunk-lusolr1\build.xml

Main targets:

 clean                        Clean Lucene and Solr
 clean-idea                   Removes all IntelliJ IDEA configuration files
 clean-jars                   Clean local jars
 clean-maven-build            Remove
 compile                      Compile Lucene and Solr
 documentation                Generate Lucene and Solr Documentation
 eclipse                      Setup Eclipse configuration
 generate-maven-artifacts     Generate Maven Artifacts for Lucene and Solr
 get-maven-poms               Copy Maven POMs from dev-tools/maven/ to ${maven-build-dir}/
 idea                         Setup IntelliJ IDEA configuration
 ivy-bootstrap                Download and install Ivy in the users ant lib dir
 jar-checksums                Recompute SHA1 checksums for all JAR files.
 jenkins-clover               Runs nightly clover builds on Jenkins
 jenkins-hourly               Runs the Jenkins hourly test runs
 jenkins-maven-nightly        Runs the nightly Maven build on Jenkins, including artifact deployment
 nightly-smoke                Builds an unsigned release and smoke tests it
 pitest                       Test both Lucene and Solr
 precommit                    Run basic checks before committing
 rat-sources                  Runs rat across all sources and tests
 remove-maven-artifacts       Removes all Lucene/Solr Maven artifacts from the local repository
 resolve                      Resolves all dependencies
 run-maven-build              Runs the Maven build using automatically generated POMs
 test                         Test both Lucene and Solr
 test-help                    Test runner help
 validate                     Validate dependencies, licenses, etc.
 validate-maven-dependencies  Validates maven dependencies, licenses, etc
Default target: -projecthelp

BUILD SUCCESSFUL
Total time: 0 seconds

C:\Users\Uwe Schindler\Projects\lucene\trunk-lusolr1>{noformat}","26/Sep/12 00:52;rcmuir;Lets do it for the root definitely.

as far as things like lucene/ we just have to be more careful.

Specifically make sure BUILD.txt is correct if we change it (it currently 
instructs you to just type 'ant' to compile the core jar file).
","26/Sep/12 01:20;uschindler;Fixes the top-level build file to also print info for documentation-lint","26/Sep/12 01:22;uschindler;Robert: I set this for Lucene 4.1 and 5.0. If you like it in the release, I can backport - the current ""ant"" behaviour is totally confusing for newcomers.","26/Sep/12 01:31;hossman;I know rmuir opened another issue to hide/disable ""internal"" targets so that the only ones -projecthelp outputs are things users should actually run, so ""ant -projecthelp"" should be fairly useful moving forward, but instead of making the default target just fork ""ant -projecthelp"" wouldn't it be better to have it echo out a summary of the handful of _really_ important tasks? eg: ivy-bootstrap, test, clean.

i mean ... stuff like the jenkins-\* targets will always need to be ""public"" so people can run them, but if the goal is to make ""ant"" provide a useful intro to people about the targets available, why don't we just use {{<echo>}} to tell people about the really important ones, and let them run ""ant -projecthelp"" themselves?

Something like...

{noformat}
> ant
[echo]
[echo]  Welcome to Apache Lucene
[echo]
[echo]  For new developers, the important targets you should 
[echo]  be aware of are...
[echo]
[echo]  Tasks you may need to run once...
[echo]    ant ivy-bootstrap
[echo]    ant idea (setups up IDE configurations for IDEA)
[echo]    ant eclipses (setups up IDE configurations for IDEA)
[echo]
[echo]  Tasks that you may run frequently...
[echo]    ant test (to run all tests)
[echo]    ant clean (...)
[echo]    ...
[echo]
[echo]  To see a full list of build options run ""ant -projecthelp""
[echo]
[echo]  Please review BUILD.txt for more information
{noformat}



","26/Sep/12 01:40;uschindler;Minor tweak to make the default target also useful in common-build.xml (passes build file directly).

Hoss: We can still add some <echo/> above (please renamed with taskname=""help"" attribute) above the projecthelp. I would filter the projecthelp by removing ""more internal targets""'s description. Ant -projecthelp only shows targets with a description set.

I dont want to maintain a second list of ""targets"" that gets easily outdated. The important ones should be in projecthelp, all others (jenkins internal ones just there for convenience) can be hidden as described above.","26/Sep/12 01:48;rcmuir;I agree: I think the ideal situation is the list of targets printed is the ones we test in jenkins and know work (LUCENE-4416).
This way users are not confused. 

Then there is really no maintenance effort: in fact we could even imagine a future ""test"" of some sort in jenkins that ensures they work (maybe AntUnit).
I especially don't want manual lists that might contain targets that no longer even exist.

Other targets like jenkins targets are really only of interest to committers: we can either remove their description or filter on some key like ""internal"" or whatever.

Targets that don't ever work by themselves and are only intended to be called by other targets should be hidden in all cases (and prefixed with a hyphen).
","26/Sep/12 13:03;uschindler;I committed the current approach for trunk to fix it 4.0, too!

We should make the other features like Hoss suggested in the separate issue about cleaning up tasks. For now I only take care of root build.xml does *not* run test by default. This confuses users completely that download the solr.src.gz and type ""ant"" after extracting (so we must fix for 4.0, too).

Committed trunk revision: 1390453
Committed 4.x revision: 1390455","26/Sep/12 14:01;uschindler;Fixed in 4.0, too.","22/Mar/13 16:32;commit-tag-bot;[branch_4x commit] Uwe Schindler
http://svn.apache.org/viewvc?view=revision&revision=1390455

Merged revision(s) 1390453 from lucene/dev/trunk:
LUCENE-4432: Make top-level default ant task print -projecthelp
","10/May/13 10:34;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade morfologik-stemming to version 2.1.0,LUCENE-7040,12941044,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,22/Feb/16 13:30,09/May/16 18:33,30/Sep/19 08:38,23/Feb/16 09:06,,,,,,,,,,,6.0,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-02-22 17:24:55.538,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 22 17:24:55 UTC 2016,New,,,,,,,"0|i2t5lz:",9223372036854775807,,,,,,,,,"22/Feb/16 17:24;jira-bot;Commit ed90697dd2db5233b46cbf6902a7186538864102 in lucene-solr's branch refs/heads/master from [~dawid.weiss]
[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ed90697 ]

SOLR-8711: Upgrade Carrot2 clustering dependency to 3.12.0
SOLR-8710: Upgrade morfologik-stemming to version 2.1.0
LUCENE-7040: Upgrade morfologik-stemming to version 2.1.0
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve concurrency for FacetsConfig,LUCENE-6909,12915974,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,sanne,sanne,25/Nov/15 00:12,09/May/16 18:33,30/Sep/19 08:38,25/Nov/15 16:36,5.3,,,,,,,,,,5.5,6.0,,,core/other,,,0,,,,"The design of {{org.apache.lucene.facet.FacetsConfig}} encourages reuse of a single instance across multiple threads, yet the current synchronization model is too strict as it doesn't allow for concurrent read operations.

I'll attach a trivial patch which removes the contention point.
",,,,,,,,,,,,,,,,"25/Nov/15 00:19;sanne;0001-LUCENE-6909-Allow-efficient-concurrent-usage-of-a-Fa.patch;https://issues.apache.org/jira/secure/attachment/12774207/0001-LUCENE-6909-Allow-efficient-concurrent-usage-of-a-Fa.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-11-25 10:20:25.165,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 25 17:52:39 UTC 2015,New,Patch Available,,,,,,"0|i2owbj:",9223372036854775807,,,,,,,,,"25/Nov/15 00:19;sanne;Trivial patch.

The synchronization isn't needed on `getDimConfig` because it's reading from a ConcurrentMap.

Synchronization is still needed on setters, but that's not a performance concern as the usage pattern is supposedly to configure the fields once and then reuse the instance mostly reading.","25/Nov/15 10:20;mikemccand;Thanks [~sanne], I think it's safe to remove {{synchronized}} from {{getDimConfig}}, but why change the type declaration for {{fieldTypes}}?  Can't it remain a {{Map}} (no usage requires specific methods from {{ConcurrentHashMap}}?).","25/Nov/15 15:29;sanne;Hi [~mikemccand]! Thanks for checking.
Yes, of course that first changed line is not required. I just felt it was useful to make it explicit to the reader that these are concurrent maps. Just a matter of style, feel free to revert that if it doesn't fit the Lucene style? Or should I provide an alternative patch?","25/Nov/15 16:24;mikemccand;Thanks [~sanne], I'll put the first part back and commit the 2nd part!","25/Nov/15 16:34;jira-bot;Commit 1716476 from [~mikemccand] in branch 'dev/trunk'
[ https://svn.apache.org/r1716476 ]

LUCENE-6909: remove unnecessary synchronized keyword","25/Nov/15 16:35;jira-bot;Commit 1716477 from [~mikemccand] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1716477 ]

LUCENE-6909: remove unnecessary synchronized keyword","25/Nov/15 17:52;sanne;Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JAR resolution/ cleanup should be done automatically for ant clean/ eclipse/ resolve.,LUCENE-4115,12559436,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,05/Jun/12 20:02,09/May/16 18:32,30/Sep/19 08:38,26/Jun/12 11:48,,,,,,,,,,,4.0-BETA,6.0,,,,,,0,,,,"I think we should add the following target deps:

ant clean [depends on] clean-jars
ant resolve [depends on] clean-jars
ant eclipse [depends on] resolve, clean-jars
ant idea [depends on] resolve, clean-jars

This eliminates the need to remember about cleaning up stale jars which users complain about (and I think they're right about it). The overhead will be minimal since resolve is only going to copy jars from cache. Eclipse won't have a problem with updated JARs if they end up at the same location.

If there are no objections I will fix this in a few hours.",,,,,,,,,,,,,,,,"26/Jun/12 10:15;dweiss;LUCENE-4115.patch;https://issues.apache.org/jira/secure/attachment/12533458/LUCENE-4115.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-05 20:08:52.807,,,false,,,,,,,,,,,,,,,243850,,,Wed Jul 11 23:15:33 UTC 2012,New,,,,,,,"0|i04gaf:",23904,,,,,,,,,"05/Jun/12 20:08;sarowe;+1

bq. (possibly ant idea too?)

I think ant idea too should depend on clean-jars resolve, if for no other reason than to immediately fail on attempting to link to stale jars when the idea config is out of sync.","05/Jun/12 20:20;sarowe;{quote}
ant eclipse [depends on] resolve, clean-jars
ant idea [depends on] resolve, clean-jars
{quote}

Shouldn't the order be clean-jars,resolve ? (Maybe I'm missing something...)","05/Jun/12 20:26;dweiss;This doesn't matter, it'll sort topologically anyway (if resolve depends on clean-jars).  But you're right -- I was just being intentionally sloppy ;)","05/Jun/12 20:28;jkrupan;The other case I was interested in is if I do ""ant clean test"", it is smart enough to go and fetch the new jars, but I have to manually do the resolve. And it would be nice if ant eclipse was triggered in that case as well. In other words, anything that caused jars to be updated should do resolve and eclipse.

","05/Jun/12 20:34;sarowe;bq. In other words, anything that caused jars to be updated should do resolve and eclipse.

-1

ant eclipse, ant idea, etc. should never be called by the standard build.

Why?  I don't use eclipse.  I don't want it cluttering my build dir/slowing down my build.
(Your version: ""I don't use IntelliJ.  I don't want it cluttering my build dir/slowing down my build."")","05/Jun/12 20:38;dweiss;If you invoke 'ant clean test' this will fetch new jars after this patch. It will not prepare any IDE settings (because of what Steve mentioned).","05/Jun/12 21:00;jkrupan;Sounds good. Just to be sure I followed it all, if I do ""ant clean eclipse test"" after ""svn co"", all of the jar processing should happen as desired.
","05/Jun/12 21:29;dweiss;Yep (and you only need to call ant eclipse if the JARs change; you'll see when they change because Eclipse won't be able to find the required library JARs and will show a configuration/ build error).","05/Jun/12 21:40;dweiss;Suggested patch. Note jar resync in 'ant resolve' for any submodule. This is intentional and will prevent stale jars even if somebody works on a submodule only.","07/Jun/12 13:38;markrmiller@gmail.com;Looks like Windows does not like this one.

BUILD FAILED
C:\Jenkins\workspace\Lucene-Solr-4.x-Windows-Java7-64\build.xml:29: The following error occurred while executing this line:
C:\Jenkins\workspace\Lucene-Solr-4.x-Windows-Java7-64\lucene\build.xml:448: The following error occurred while executing this line:
C:\Jenkins\workspace\Lucene-Solr-4.x-Windows-Java7-64\lucene\common-build.xml:618: The following error occurred while executing this line:
C:\Jenkins\workspace\Lucene-Solr-4.x-Windows-Java7-64\lucene\common-build.xml:286: Unable to delete file C:\Jenkins\workspace\Lucene-Solr-4.x-Windows-Java7-64\lucene\test-framework\lib\junit4-ant-1.5.0.jar","07/Jun/12 19:53;dweiss;Can you check what's causing the JAR file to be locked?

- what target did you issue (it can be that ant's own url class loader holds a lock)?
- was Eclipse/IntelliJ running in the background?
- was there any executing test/ other class in the background (these would all hold a lock)?

We can always add failonerror=false to delete but not able to delete that jar signals a problem and it'd be nice to know where it comes from.","07/Jun/12 19:58;rcmuir;This is creating a lot of noise. Can we back it out until there is a solution that works on windows?","07/Jun/12 20:02;dweiss;Sorry -- I'm on mobile and couldn't check properly (didn't know it fails on jenkins). Robert will revert this commit and I'll see what the problem is tomorrow. Quite likely a class loader holds a lock on a jar file. I don't know why yet.","07/Jun/12 20:03;markrmiller@gmail.com;bq. what target did you issue 

I'm getting this from Uwe's jenkins emails to the devs list rather than my own machine. I don't think there is any IDE involved there.

A quick test in my vm shows it happening at the end of running ant test though.","07/Jun/12 20:08;dweiss;Yeah, sorry Mark -- didn't check it thoroughly, I'm on mobile.","08/Jun/12 10:28;rcmuir;Another possibility (didnt investigate if it has options that would work for us) is the sync=true option for retrieve:

http://ant.apache.org/ivy/history/trunk/use/retrieve.html

Just at a glance there could be some problems: sha1/license/notice files, and solr/lib which is 'shared' across solrj and core dependencies. 

But maybe we could still utilize this...","08/Jun/12 10:53;dweiss;I've checked that -- sync on retrieve deletes everything from a folder (there is no exclusion pattern to be applied). Besides it won't solve the locking problem on windows (assuming something keeps a lock on a jar to be deleted it'd fail anyway).

A true nice solution would be to revisit the issue where classpaths are constructed to ivy cache directly (they're always correct then) and just use copying for packaging.","08/Jun/12 20:44;hossman;bq. A true nice solution would be to revisit the issue where classpaths are constructed to ivy cache directly (they're always correct then) and just use copying for packaging.

seems like that might introduce some risk of the classpath(s) used by developers/jenkins for running tests deviating from the ones people would get if they use the binary distributions (particularly solr users who don't know/understand java classpaths and just copy the example & lib dirs as a starting point).
","08/Jun/12 21:04;dweiss;Why would this be so? I mean -- the risk of users messing up their classpath with lib/*.jar is pretty much the same compared to an ivy classpath from cache + ivy classpath from cache copied to lib/ at distribution time?","08/Jun/12 21:33;hossman;I'm not sure i'm following you.

right now, we know that when you run ""ant test"" (or ""java -jar start.jar"" in solr) you getting the exact same classpath and set of jars as someone who downloads a binary dist you might build from your checkout -- because the classpath you are using comes from the lib dirs, built by ivy.  there is only one places jars are ""copied"" from the ivy cache to the lib dir(s).

if instead we have ant <classpath/> declarations that use ivy features to build up classpaths pointed directly at jars in the ivy cache, and independently we have <copy> directives building up the lib/ dirs that make it into the binary dists, then isn't there a risk that (overtime) those will diverge in a way we might not notice because all the testing will only be done with the ivy generated classpaths?

(maybe i'm wrong ... maybe the ivy classpath stuff works diff then i understand ... i'm just raising it as a potential concern)","08/Jun/12 21:40;dweiss;I know why the jars are locked - it's because junit4-ant* is taskdef'ed from ant and url jar loader will keep it open. Makes sense. A workaround is to ignore failed deletes (I don't see a cleaner solution other than the one mentioned above -- construct the classpath from ivy cache directly).

I'll post a patch but I'm away most of the time until Sunday so if somebody is willing to commit and monitor jenkins for sanity -- feel free to do so.","08/Jun/12 21:41;dweiss;Corrected patch, checked the build on windows, didn't fail for me. ","08/Jun/12 21:46;dweiss;Hoss -- I've been doing some experiments with ivy (and I know very little about it) but it seems that you can use ivy to build both <path> elements and <fileset> elements from the same configuration. This means you would construct an identical path for any java invocation/ taskdef, etc. and a fileset for <copy> to lib/.

I agree with you 100% that we shouldn't be doing any copying or filtering manually -- it has to be the same fileset.

http://ant.apache.org/ivy/history/trunk/use/cachepath.html
http://ant.apache.org/ivy/history/trunk/use/cachefileset.html

We could even use the fileset only and just put it under a custom <path id=""...""> element. 

Note that at the moment this isn't consistent either -- if there is a jar dependency upgrade people are left with unused jars under lib/* and this can (and does) cause headaches until you realize you need to clean up that folder (which is a reason for this issue, actually).","08/Jun/12 21:48;dweiss;One thing I didn't know how to work around with direct ivy caches was jar checksums -- I think it is a good idea to keep these but then I don't know how they could be verified if we use ivy caches directly.","08/Jun/12 23:09;hossman;bq. I agree with you 100% that we shouldn't be doing any copying or filtering manually – it has to be the same fileset. 

Right ... that's the crux of my concern.

bq. Note that at the moment this isn't consistent either – if there is a jar dependency upgrade people are left with unused jars under lib/* and this can (and does) cause headaches until you realize you need to clean up that folder (which is a reason for this issue, actually).

but our checksum validation warns you of that .. the reason for this issue was not that people didn't know they had bad jars (the build already told them that) the point was to try and make dealing with it automatic. 

bq. One thing I didn't know how to work around with direct ivy caches was jar checksums – I think it is a good idea to keep these but then I don't know how they could be verified if we use ivy caches directly.

yeah ... the checksum validation is really huge ... we definitely should sacrifice that.

----

Which leads me to a strawman suggestion: what if instead of making all these ant targets depend on ""ant clean-jars"" we add an optional build property that tells the checksum validation code to try to remove any jar that doesn't have a checksum file?  values for the property could indicate:
 * don't try to delete but warn of existence
 * don't try to delete and fail because of existence (current behavior)
 * try to delete, fail if delete fails (new default)
 * try to delete, warn & don't fail if delete fails (new default if windows) 
...in the cases where deletion failure is non-fatal, the code could still register a deleteOnExit() for the files as a fallback (which should work on windows right? by that point windows will have closed the file handle for the jar?)

if we did that, then (i think) the worst case scenario for windows dev/jenkins users after ivy config changes would be that the *first* build attempt might fail because of a jar that couldn't be deleted (because it was in use), but that file should be deleted when the JVM exists, and after that the build should start working.

right?

","09/Jun/12 12:42;uschindler;bq. One thing I didn't know how to work around with direct ivy caches was jar checksums – I think it is a good idea to keep these but then I don't know how they could be verified if we use ivy caches directly.

You can get the ivy:cachefileset and iterate it using a hand-written <script> task or similar?","09/Jun/12 12:47;uschindler;bq. I know why the jars are locked - it's because junit4-ant* is taskdef'ed from ant and url jar loader will keep it open. Makes sense. A workaround is to ignore failed deletes (I don't see a cleaner solution other than the one mentioned above – construct the classpath from ivy cache directly).

I thought all taskdefs are using ivy:cachepath? I changed (or I hope to have changed) at least all those everywhere. Everything ANT needs to build with custom tasks should come from ivy directly. Those JARs never need to be in any binary distribution.","09/Jun/12 12:49;uschindler;For examples see classpath of generate-webpage, cpp-tasks,... Those are all tasks I created during the migration.","09/Jun/12 17:55;dweiss;Good point, Uwe -- the problematic jars are junit4 and randomized runner -- these are required for tests though... so I think it's in the gray area somewhere because they will need to be distributed (if tests are distributed) and at the same time they could be loaded from ivy cache? 

What's your opinion on this?","09/Jun/12 18:12;rcmuir;I'm not sure we should really go to the trouble for this issue.

why can't developers just 'ant clean-jars' after they svn update? Really we are just talking about committers etc tracking development branches here: they should be subscribed to the commits@ list and watching what is happening. Jar upgrades should be no surprise.

The problem with doing all this automagic stuff is it makes things too automagic: what if i just want to plop in a jar today to test it before dealing with maven etc? This works completely fine today: but if we start rm'ing jars in 'resolve' then this becomes impossible.

if anything gets done on this issue, then personally i want it triggered by a property that is completely defaulted to 'off'. If you want it on, put it in your ~/build.properties etc.","09/Jun/12 18:26;dweiss;I don't agree -- the current situation in which you need to remember about cleaning up jars and resolving again is confusing. It should be syncing automatically with respect to the contents of ivy files. If you need a temporary jar, oh well, modify the ivy file. Given the tradeoff between the two situations I'm opting for having an automagic sync that the need to remember to do cleanups manually every time.

Currently we 'ant resolve' on ant test anyway so it's half-way to full syncing to me.

A middle ground would be to have ant clean/eclipse/idea resync everything and 'ant test' and 'ant resolve' not do anything special, but like I said -- I think it should be consistent with ivy files. For better or worse, that's the reason we have them -- to specify dependencies.

","09/Jun/12 19:33;rcmuir;{quote}
For better or worse, that's the reason we have them – to specify dependencies.
{quote}

Not exactly true. in the current build, what is in lib/ specifies the dependencies.

ivy files are just being used as jar-downloaders.
","10/Jun/12 17:20;dweiss;Hmm... I don't think the use of ivy should stop at being just a jar-downloader. I have mixed feelings about transitive dependency management (this applies to both ivy and maven) so I'm not suggesting we jump into it full time, but the current build system would really benefit (time and I/O wise) if we used direct ivy cache references instead of copying jars to lib/ folders. And I don't share your concerns about it -- I don't think it makes things more complicated or more magical.
","26/Jun/12 10:15;dweiss;A new version of this patch that doesn't attempt to delete *.jar files on per-module resolve (does it from the top-level resolve once). 

JUnit4 dependency is picked from ivy cache now. There is no redundancy as the same ivy module config is used and this will prevent jar locking.","26/Jun/12 11:48;dweiss;I've committed this one in (checked on Windows and Linux, doesn't seem to be causing any problems anymore).

It isn't ideal but better than what's currently in.","11/Jul/12 23:15;hossman;hoss20120711-manual-post-40alpha-change",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade ant dependency to 1.8.2,LUCENE-4112,12559367,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,05/Jun/12 12:12,09/May/16 18:32,30/Sep/19 08:38,05/Jun/12 12:31,,,,,,,,,,,4.0-ALPHA,6.0,,,,,,0,,,,ivy defines 1.7.1 and we require 1.8.2.,,,,,,,,,,,,,,,,"05/Jun/12 12:12;dweiss;LUCENE-4112.patch;https://issues.apache.org/jira/secure/attachment/12530940/LUCENE-4112.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,243853,,,2012-06-05 12:12:22.0,New,,,,,,,"0|i04gb3:",23907,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose buffer size in BufferedIndexOutput,LUCENE-4837,12637299,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,simonw,simonw,simonw,15/Mar/13 23:38,09/May/16 18:32,30/Sep/19 08:38,16/Mar/13 07:02,4.1,4.2,,,,,,,,,4.3,6.0,,,core/store,,,0,,,,We expose a getter on BufferedIndexInput so for consistency it makes sense to expose it on BufferedIndexOutput as well. Wrappers like RateLimiter can also take advantage of this information to adjust their buffer size.,,,,,,,,,,,,,,,,"15/Mar/13 23:39;simonw;LUCENE-4837.patch;https://issues.apache.org/jira/secure/attachment/12573977/LUCENE-4837.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-16 15:01:54.554,,,false,,,,,,,,,,,,,,,317791,,,Fri May 10 10:32:53 UTC 2013,New,Patch Available,,,,,,"0|i1iu2f:",318132,,,,,,,,,"15/Mar/13 23:39;simonw;simple patch","16/Mar/13 15:01;commit-tag-bot;[branch_4x commit] Simon Willnauer
http://svn.apache.org/viewvc?view=revision&revision=1457212

LUCENE-4837: Expose buffer size in BufferedIndexOutput
","16/Mar/13 15:01;commit-tag-bot;[trunk commit] Simon Willnauer
http://svn.apache.org/viewvc?view=revision&revision=1457211

LUCENE-4837: Expose buffer size in BufferedIndexOutput
","10/May/13 10:32;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix junitcompat tests (so that they're not triggered when previous errors occur),LUCENE-5238,12670046,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,23/Sep/13 12:40,09/May/16 18:32,30/Sep/19 08:38,24/Sep/13 08:31,,,,,,,,,,,4.6,6.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-24 08:28:01.452,,,false,,,,,,,,,,,,,,,349876,,,Tue Sep 24 08:30:13 UTC 2013,New,,,,,,,"0|i1obtj:",350174,,,,,,,,,"23/Sep/13 12:41;dweiss;http://builds.flonkings.com/job/Lucene-trunk-Linux-Java7-64-test-only/58093/console
{code}
- org.apache.lucene.util.junitcompat.TestFailIfUnreferencedFiles.testFailIfUnreferencedFiles
- org.apache.lucene.util.junitcompat.TestFailIfDirectoryNotClosed.testFailIfDirectoryNotClosed
{code}
","24/Sep/13 08:28;jira-bot;Commit 1525815 from [~dawidweiss] in branch 'dev/trunk'
[ https://svn.apache.org/r1525815 ]

LUCENE-5238: Fix junitcompat tests (so that they're not triggered when previous errors occur).","24/Sep/13 08:30;jira-bot;Commit 1525816 from [~dawidweiss] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1525816 ]

LUCENE-5238: Fix junitcompat tests (so that they're not triggered when previous errors occur).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade RandomizedTesting to 2.1.17,LUCENE-6812,12895418,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,22/Sep/15 12:00,09/May/16 18:32,30/Sep/19 08:38,22/Sep/15 12:32,,,,,,,,,,,5.4,6.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-09-22 12:29:26.479,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 12:31:54 UTC 2015,New,,,,,,,"0|i2le9z:",9223372036854775807,,,,,,,,,"22/Sep/15 12:29;jira-bot;Commit 1704600 from [~dawidweiss] in branch 'dev/trunk'
[ https://svn.apache.org/r1704600 ]

LUCENE-6812: Upgrade RandomizedTesting to 2.1.17","22/Sep/15 12:31;jira-bot;Commit 1704601 from [~dawidweiss] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1704601 ]

LUCENE-6812: Upgrade RandomizedTesting to 2.1.17",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Out of date code examples,LUCENE-4788,12633247,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,shaie,drzhonghao,drzhonghao,20/Feb/13 18:30,09/May/16 18:32,30/Sep/19 08:38,21/Feb/13 13:27,4.1,,,,,,,,,,4.2,6.0,,,modules/facet,,,0,,,,"The following API documents have code examples:
http://lucene.apache.org/core/4_1_0/facet/org/apache/lucene/facet/index/OrdinalMappingAtomicReader.html
http://lucene.apache.org/core/4_1_0/facet/org/apache/lucene/facet/index/OrdinalMappingAtomicReader.html
""// merge the old taxonomy with the new one.
 OrdinalMap map = DirectoryTaxonomyWriter.addTaxonomies();""

The two code examples call the DirectoryTaxonomyWriter.addTaxonomies method. Lucene 3.5 has that method, according to its document:
http://lucene.apache.org/core/old_versioned_docs/versions/3_5_0/api/all/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.html

However, lucene 4.1 does not have such a method, according to its document：
http://lucene.apache.org/core/4_1_0/facet/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.html
Please update the code examples to reflect the latest implementation.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-02-21 13:27:36.993,,,false,,,,,,,,,,,,,,,313743,,,Fri May 10 10:33:00 UTC 2013,New,,,,,,,"0|i1i547:",314088,,,,,,,,,"21/Feb/13 13:27;shaie;Committed to 4.2 and trunk.

Thanks Hao Zhong!","21/Feb/13 13:38;commit-tag-bot;[branch_4x commit] Shai Erera
http://svn.apache.org/viewvc?view=revision&revision=1448634

LUCENE-4788: fix code example in jdoc
","10/May/13 10:33;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change the semantics of @Slow on tests,LUCENE-4181,12596362,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,29/Jun/12 08:53,09/May/16 18:32,30/Sep/19 08:38,29/Jun/12 10:49,,,,,,,,,,,4.0-ALPHA,6.0,,,,,,0,,,,,,,,,,,,,,,,,,,,"29/Jun/12 10:28;dweiss;LUCENE-4181.patch;https://issues.apache.org/jira/secure/attachment/12533966/LUCENE-4181.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,243784,,,Fri Jun 29 10:49:21 UTC 2012,New,,,,,,,"0|i04fvr:",23838,,,,,,,,,"29/Jun/12 10:28;dweiss;Applied @Slow on the slowest tests.","29/Jun/12 10:48;dweiss;Before (full tests, solr and lucene):
{noformat}
BUILD SUCCESSFUL
Total time: 11 minutes 29 seconds
{noformat}
After:
{noformat}
BUILD SUCCESSFUL
Total time: 7 minutes 45 seconds
{noformat}
","29/Jun/12 10:49;dweiss;I'll commit it in since there was just one test marked as @Slow and it doesn't affect the default behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadoc error when run in private access level,LUCENE-6571,12838107,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,cpoerschke,caomanhdat,caomanhdat,16/Jun/15 09:20,09/May/16 18:32,30/Sep/19 08:38,24/Aug/15 20:18,5.2,,,,,,,,,,5.4,6.0,,,,,,0,,,,Javadoc error when run in private access level.,,,,,,,,,,,,,,,,"16/Jun/15 09:22;caomanhdat;LUCENE-6571.patch;https://issues.apache.org/jira/secure/attachment/12739827/LUCENE-6571.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-08-05 11:32:17.433,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 24 20:27:55 UTC 2015,New,Patch Available,,,,,,"0|i2g3dr:",9223372036854775807,,,,,,,,,"16/Jun/15 09:22;caomanhdat;Here is patch for this trivial bug.","05/Aug/15 11:32;cpoerschke;Hi [~caomanhdat] - just applied your patch to my local trunk working copy and then ran {{ant javadocs -Djavadoc.access=private}} on it.

Seeing some warnings, will check further, might be due to local setup or so.

{code}
  [javadoc] /mydirectory/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java:386: warning - Tag @link: can't find setIndexingChain(IndexingChain) in org.apache.lucene.index.IndexWriterConfig
  [javadoc] /mydirectory/lucene/core/src/java/org/apache/lucene/util/fst/FST.java:119: warning - Tag @see: can't find shouldExpand(Builder, UnCompiledNode) in org.apache.lucene.util.fst.FST
  [javadoc] /mydirectory/lucene/core/src/java/org/apache/lucene/util/fst/FST.java:114: warning - Tag @see: can't find shouldExpand(Builder, UnCompiledNode) in org.apache.lucene.util.fst.FST
  [javadoc] /mydirectory/lucene/core/src/java/org/apache/lucene/util/fst/FST.java:109: warning - Tag @see: can't find shouldExpand(Builder, UnCompiledNode) in org.apache.lucene.util.fst.FST
{code}
","05/Aug/15 12:08;cpoerschke;With respect to the {{LiveIndexWriterConfig.java:386}} warning, we have currently

IndexWriterConfig.java
{code}
public final class IndexWriterConfig extends LiveIndexWriterConfig {
  ...
  IndexWriterConfig setIndexingChain(IndexingChain indexingChain) {
    if (indexingChain == null) {
      throw new IllegalArgumentException(""indexingChain must not be null"");
    }
    this.indexingChain = indexingChain;
    return this;
  }

  @Override
  IndexingChain getIndexingChain() {
    return indexingChain;
  }
{code}

LiveIndexWriterConfig.java
{code}
public class LiveIndexWriterConfig {
  ...
  protected volatile IndexingChain indexingChain;
  /**
   * Returns the indexing chain set on
   * {@link IndexWriterConfig#setIndexingChain(IndexingChain)}.
   */
  IndexingChain getIndexingChain() {
    return indexingChain;
  }
{code}

i.e. {{@link IndexWriterConfig#setIndexingChain}} in the base class references a non-public function in the derived class.

I wonder if the {{@Override getIndexingChain()}} function in the derived class could be removed and {{setIndexingChain}} be relocated to the base class? And might {{indexingChain}} be made {{private}} instead of {{protected}} then also?","05/Aug/15 14:27;mikemccand;The problem with moving {{setIndexingChain}} to {{LiveIndexWriterConfig}} is this is not something you can change ""live"" in an {{IndexWriter}}: it only takes effect when IW first starts up.

We could also remove the setter entirely (hardwire IW to {{DefaultIndexingChain}})?  It's already package private, is insanely expert, and devs playing with this are clearly expert enough to modify Lucene's sources to do their experiments.","05/Aug/15 15:59;jpountz;bq. We could also remove the setter entirely (hardwire IW to DefaultIndexingChain)? It's already package private, is insanely expert, and devs playing with this are clearly expert enough to modify Lucene's sources to do their experiments.

+1 It is weird to have a pkg-private method being part of our API.","14/Aug/15 15:32;cpoerschke;Created LUCENE-6738 for removal of the setter and also removal of the overriding getter since it just 'repeats' the base class getter.","19/Aug/15 20:56;cpoerschke;Removing setIndexingChain via LUCENE-6738 removed the LiveIndexWriterConfig.java:386 warning.

The FST.java warnings remain
{code}
FST.java:119: warning - Tag @see: can't find shouldExpand(Builder, UnCompiledNode) in org.apache.lucene.util.fst.FST
FST.java:114: warning - Tag @see: can't find shouldExpand(Builder, UnCompiledNode) in org.apache.lucene.util.fst.FST
FST.java:109: warning - Tag @see: can't find shouldExpand(Builder, UnCompiledNode) in org.apache.lucene.util.fst.FST
{code}
but
{code}
package org.apache.lucene.util.fst;
...
-import org.apache.lucene.util.fst.Builder.UnCompiledNode;
...
-   * @see #shouldExpand(Builder, UnCompiledNode)
+   * @see #shouldExpand(Builder, Builder.UnCompiledNode)
...
    * @see Builder.UnCompiledNode#depth
    */
-  private boolean shouldExpand(Builder<T> builder, UnCompiledNode<T> node) {
+  private boolean shouldExpand(Builder<T> builder, Builder.UnCompiledNode<T> node) {
...
{code}
changes would make the warning go away.","21/Aug/15 19:09;mikemccand;+1 to the FST fixes!  Thanks [~cpoerschke].","24/Aug/15 19:43;jira-bot;Commit 1697491 from [~cpoerschke] in branch 'dev/trunk'
[ https://svn.apache.org/r1697491 ]

LUCENE-6571: fix some private access level javadoc errors and warnings","24/Aug/15 19:56;jira-bot;Commit 1697496 from [~cpoerschke] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1697496 ]

LUCENE-6571: fix some private access level javadoc errors and warnings (merge in revision 1697491 from trunk)","24/Aug/15 20:18;cpoerschke;Thanks [~caomanhdat]!","24/Aug/15 20:27;cpoerschke;For the log, the changes committed above were for trunk and branch_5x but additional residual {{ant javadocs -Djavadoc.access=private}} errors remain, for example jflex generated code gives errors such as
{code}
... WikipediaTokenizerImpl.java:426: error: bad use of '>'
/** zzAtEOF == true <=> the scanner is at the EOF */
                      ^
{code}

On https://github.com/jflex-de/jflex/issues there is [#182|https://github.com/jflex-de/jflex/issues/182] re: doclint but a quick search found no equivalent issue re: javadocs (with private access level).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow storing test execution statistics in an external file,LUCENE-4168,12596076,,Test,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,27/Jun/12 12:21,09/May/16 18:31,30/Sep/19 08:38,27/Jun/12 12:45,,,,,,,,,,,4.0-ALPHA,6.0,,,general/test,,,0,,,,"Override on the build server to calculate stats during runs, then update the cache in the repo from time to time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,243797,,,2012-06-27 12:21:46.0,New,,,,,,,"0|i04fyn:",23851,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
we should change the suggested search in the demo docs because the lucene code base is full of swear words,LUCENE-5233,12669790,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,hossman,hossman,hossman,21/Sep/13 01:28,09/May/16 18:31,30/Sep/19 08:38,25/Sep/13 18:04,,,,,,,,,,,4.5,6.0,,,,,,0,,,,"the javadocs for the lucene demo say...

bq. You'll be prompted for a query. Type in a swear word and press the enter key. You'll see that the Lucene developers are very well mannered and get no results. Now try entering the word ""string"". That should return a whole bunch of documents. The results will page at every tenth result and ask you whether you want more results.

...but thanks to files like ""KStemData*.java"" and ""Top50KWiki.utf8"" i was *really* hard pressed to find an (english) swear word that didn't result in a match in any of the files in the lucene code base (and i have a pretty extensive breadth of knowledge of profanity)

We should change this paragraph to refer to something that is total giberish (""supercalifragilisticexpialidocious"")... or maybe just ""nocommit""

(side note: since this para exists in the javadoc package comments, it will get picked up when they index the source -- so we should include an HTML comment in the middle of whatever word is picked)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-21 12:26:28.596,,,false,,,,,,,,,,,,,,,349718,,,Sat Oct 05 10:19:25 UTC 2013,New,,,,,,,"0|i1oauf:",350016,,,,,,,,,"21/Sep/13 12:26;mikemccand;bq.  (and i have a pretty extensive breadth of knowledge of profanity)

LOL :)

+1 to fix the demo docs!","25/Sep/13 17:57;jira-bot;Commit 1526247 from hossman@apache.org in branch 'dev/trunk'
[ https://svn.apache.org/r1526247 ]

LUCENE-5233: tweak demo example search string to something that isn't in the code base","25/Sep/13 18:02;jira-bot;Commit 1526248 from hossman@apache.org in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1526248 ]

LUCENE-5233: tweak demo example search string to something that isn't in the code base (merge r1526247)","25/Sep/13 18:04;hossman;I went with...

{noformat}""superca<!-- need to break up word in a way that is not visibile so it doesn't cause this ile to match a search on this word -->lifragilisticexpialidocious""{noformat}","26/Sep/13 15:47;jira-bot;Commit 1526585 from [~jpountz] in branch 'dev/branches/lucene_solr_4_5'
[ https://svn.apache.org/r1526585 ]

LUCENE-5233: tweak demo example search string to something that isn't in the code base (merge r1526248)","05/Oct/13 10:19;jpountz;4.5 release -> bulk close",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade randomizedtesting to 2.3.2,LUCENE-6924,12919683,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,07/Dec/15 13:11,09/May/16 18:31,30/Sep/19 08:38,07/Dec/15 13:24,,,,,,,,,,,5.5,6.0,,,,,,0,,,,https://github.com/randomizedtesting/randomizedtesting/releases/tag/release%2F2.3.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-12-07 13:12:46.556,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 13:15:22 UTC 2015,New,,,,,,,"0|i2pj6n:",9223372036854775807,,,,,,,,,"07/Dec/15 13:12;jira-bot;Commit 1718345 from [~dawidweiss] in branch 'dev/trunk'
[ https://svn.apache.org/r1718345 ]

LUCENE-6924: Upgrade randomizedtesting to 2.3.2.","07/Dec/15 13:15;jira-bot;Commit 1718347 from [~dawidweiss] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1718347 ]

LUCENE-6924: Upgrade randomizedtesting to 2.3.2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve IndexWriter.addIndexes(IndexReader) javadocs,LUCENE-4164,12595747,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,shaie,shaie,shaie,25/Jun/12 07:02,09/May/16 18:31,30/Sep/19 08:38,26/Jun/12 07:31,,,,,,,,,,,4.0-ALPHA,6.0,,,core/index,,,0,,,,"IndexWriter.addIndexes(IndexReader) needs some improvements:

* It falsely states that this method blocks any add/delete documents attempts.

* It merges all input IndexReaders at once, and not e.g. like MergePolicy does at mergeFactor steps. Therefore I think it'd be good to clarify it to the user, and also encourage him to call this method several times if he has many IndexReaders to merge.
** And while at it, mentioning that the IR can be opened with termIndexInterval=-1 since we don't need it during merge will be good -- saves RAM.

I'll attach a patch shortly.",,,,,,,,,,,,,,,,"25/Jun/12 07:12;shaie;LUCENE-4164.patch;https://issues.apache.org/jira/secure/attachment/12533265/LUCENE-4164.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,243801,,,Tue Jun 26 07:31:29 UTC 2012,New,Patch Available,,,,,,"0|i04fzj:",23855,,,,,,,,,"25/Jun/12 07:12;shaie;Patch with proposed javadoc changes.","26/Jun/12 07:31;shaie;Committed javadoc changes to trunk and 4x.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sorter API: Make NumericDocValuesSorter able to sort in reverse order,LUCENE-4904,12640802,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,shaie,jpountz,jpountz,04/Apr/13 16:42,09/May/16 18:31,30/Sep/19 08:38,10/Apr/13 10:56,,,,,,,,,,,4.3,6.0,,,,,,0,newdev,,,Today it is only able to sort in ascending order.,,,,,,,,,,,,,,,,"10/Apr/13 03:21;shaie;LUCENE-4904.patch;https://issues.apache.org/jira/secure/attachment/12577944/LUCENE-4904.patch","09/Apr/13 18:13;shaie;LUCENE-4904.patch;https://issues.apache.org/jira/secure/attachment/12577846/LUCENE-4904.patch","09/Apr/13 17:51;shaie;LUCENE-4904.patch;https://issues.apache.org/jira/secure/attachment/12577843/LUCENE-4904.patch","09/Apr/13 15:13;shaie;LUCENE-4904.patch;https://issues.apache.org/jira/secure/attachment/12577811/LUCENE-4904.patch",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2013-04-04 18:59:16.002,,,false,,,,,,,,,,,,,,,321261,,,Fri May 10 10:33:57 UTC 2013,New,Patch Available,,,,,,"0|i1jfif:",321606,,,,,,,,,"04/Apr/13 18:59;shaie;Maybe instead of fixing NumericDVSorter, we can have a ReverseSortSorter (must have a better name!) which wraps any Sorter and reverses the DocMap?","04/Apr/13 19:30;jpountz;That's another option. I have no strong feeling towards any of them.","09/Apr/13 15:13;shaie;I hacked this up real quickly, so I could be missing something. Patch adds a ReverseOrderSorter which wraps a Sorter and on sort() returns a DocMap that reverses whatever the wrapped Sorter DocMap returned.

I still didn't figure out how to plug that sorter with existing tests, so it could be this approach doesn't work. Will look at it later.","09/Apr/13 17:51;shaie;Added ReverseOrderSorter to IndexSortingTest (was after all very easy), which uncovered a bug in my original implementation. It's now working and tests are happy.

I basically think this is ready, would appreciate some review.","09/Apr/13 18:13;shaie;Patch on latest trunk (previous one had issues applying).","09/Apr/13 19:24;jpountz;We can add this ReverseOrderSorter, but as far as NumericDocValuesSorter is concerned, I would rather have the abstraction at the level of the DocComparator rather than the Sorter. This would allow {{Sorter.sort(int,DocComparator)}} to quickly return null without allocating (potentially lots of) memory for the doc maps if the reader is already sorted. Additionally, this would allow for more readable diagnostics (such as ""DocValues(fieldName,desc)"" instead of ""Reverse(DocValues(fieldName,asc))"".
","10/Apr/13 03:21;shaie;Fair enough. It was quite easy to modify NumericDVSorter. Patch adds another ctor which takes a {{boolean ascending}}, and integrates it with the test.

This got me thinking if ascending/descending should be on the Sorter.sort API, but I think it shouldn't because someone can dangerously sort some segments in ascending and others in descending order. Better, I think, if it's a consistent decision for all segments. What do you think?","10/Apr/13 08:58;jpountz;bq. This got me thinking if ascending/descending should be on the Sorter.sort API

I think it shouldn't for the reasons you mentioned.

The patch looks good to me, +1 to commit!","10/Apr/13 10:12;shaie;I am also wondering if I should keep ReverseOrderSorter, because implementing reverse in numeric is so trivial, that ReverseOrderSorter becomes just another sorter to maintain... I think I'll remove it","10/Apr/13 10:14;jpountz;It is OK for me.","10/Apr/13 10:56;shaie;Committed to trunk and 4x.","10/May/13 10:33;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The PayloadTermWeight explanation hides the details of the payload score,LUCENE-4249,12599902,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,smerchek,smerchek,23/Jul/12 20:56,09/May/16 18:31,30/Sep/19 08:38,23/Jul/12 21:57,4.0-ALPHA,,,,,,,,,,4.0-BETA,6.0,,,core/search,,,0,newdev,patch,,"I'm using the PayloadTermQuery and scoring documents using a custom algorithm based on the payloads of the matching terms. The algorithm is implemented in the custom PayloadFunction and I have added an Override for the explain. However, the PayloadTermWeight explanation hides the details of the payload score...

{code}
Explanation payloadExpl = new Explanation(scorer.getPayloadScore(), ""scorePayload(...)"");
{code}

This is different than the way that PayloadNearSpanWeight explains the payload. It actually asks the payload function for the explanation rather than hiding it:

{code}
Explanation payloadExpl = function.explain(doc, scorer.payloadsSeen, scorer.payloadScore);
{code}",,,,,,,,,,,,,,,,"23/Jul/12 21:33;smerchek;LUCENE-4249.patch;https://issues.apache.org/jira/secure/attachment/12537614/LUCENE-4249.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-23 20:59:05.249,,,false,,,,,,,,,,,,,,,243719,,,Mon Jul 23 21:57:15 UTC 2012,New,,,,,,,"0|i04fhb:",23773,,,,,,,,,"23/Jul/12 20:57;smerchek;Once I can get past the hurdles of creating my first patch from the git mirror, I'll upload it.","23/Jul/12 20:59;rcmuir;Thanks Scott. git formatted patches are fine in my opinion: lets get this fixed","23/Jul/12 21:57;rcmuir;Thanks Scott!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FreeTextSuggester can now use Files.createTempDirectory(),LUCENE-5893,12735061,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,varunthacker,varunthacker,19/Aug/14 13:52,09/May/16 18:31,30/Sep/19 08:38,20/Aug/14 09:41,,,,,,,,,,,4.10,6.0,,,,,,0,,,,Came across the TODO in the code and now it's possible to use Files.createTempDirectory since 4x is also on Java 7. ,,,,,,,,,,,,,,,,"20/Aug/14 08:19;varunthacker;LUCENE-5893.patch;https://issues.apache.org/jira/secure/attachment/12663080/LUCENE-5893.patch","19/Aug/14 13:56;varunthacker;LUCENE-5893.patch;https://issues.apache.org/jira/secure/attachment/12662756/LUCENE-5893.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-08-20 07:56:32.523,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 20 09:41:40 UTC 2014,New,,,,,,,"0|i1z1w7:",9223372036854775807,,,,,,,,,"19/Aug/14 13:56;varunthacker;Simple patch. TestFreeTextSuggester passes.

I removed the TODO and switched to Files.createTempDirectory()

[~mikemccand] - Let me know if I missed something.","20/Aug/14 07:56;mikemccand;Thanks [~varunthacker] this looks good, except I think we no longer need to add the random.nextInt(Integer.MAX_VALUE) part?  Ie, this API will find a unique name for us from the prefix we provide?","20/Aug/14 08:19;varunthacker;You're right :) New patch fixes it.","20/Aug/14 09:39;mikemccand;Thanks [~varunthacker] that looks great, I'll commit shortly!","20/Aug/14 09:40;jira-bot;Commit 1619057 from [~mikemccand] in branch 'dev/trunk'
[ https://svn.apache.org/r1619057 ]

LUCENE-5893: use Files.createTempDirectory","20/Aug/14 09:41;jira-bot;Commit 1619058 from [~mikemccand] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1619058 ]

LUCENE-5893: use Files.createTempDirectory",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mistake in the comment in source ...surround.parser.QueryParser.jj ,LUCENE-6143,12636862,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,ehatcher,ariel_lieberman@hotmail.com,ariel_lieberman@hotmail.com,13/Mar/13 21:08,09/May/16 18:31,30/Sep/19 08:38,30/Dec/14 10:19,4.1,4.2,,,,,,,,,5.0,6.0,,,modules/queryparser,,,0,documentation,,,"The comment in the source regarding Surround query parser  *""N is ordered, and W is unordered.""*  is a mistake.  Should be the other way around.

Appears in
in org.apache.lucene.queryparser.surround.parser  QueryParser.jj",,,,,,,,,,,,,,,,"04/Nov/14 21:49;mdrob;SOLR-4572.patch;https://issues.apache.org/jira/secure/attachment/12679328/SOLR-4572.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-11-04 21:49:17.862,,,false,,,,,,,,,,,,,,,317354,,,Mon Feb 23 05:03:02 UTC 2015,New,Patch Available,,,,,,"0|i1irdb:",317695,,,,,,,,,"04/Nov/14 21:49;mdrob;Patch that fixes the JavaDoc.","05/Dec/14 05:37;shinichiro abe;I've fixed [that solr wiki page|https://wiki.apache.org/solr/SurroundQueryParser] in advance.","30/Dec/14 10:16;jira-bot;Commit 1648497 from [~ehatcher] in branch 'dev/trunk'
[ https://svn.apache.org/r1648497 ]

LUCENE-6143: Correct javadoc for surround query parser","30/Dec/14 10:17;jira-bot;Commit 1648498 from [~ehatcher] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1648498 ]

LUCENE-6143: Correct javadoc for surround query parser (merged from trunk r1648497)","30/Dec/14 10:19;ehatcher;Fixed on 5x and trunk.  I manually made the change to the .java file as I wasn't able to successfully get the regenerate to run.","23/Feb/15 05:03;anshum;Bulk close after 5.0 release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove RamUsageTester.IdentityHashSet,LUCENE-6095,12759742,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,05/Dec/14 18:11,09/May/16 18:30,30/Sep/19 08:38,11/Dec/14 11:21,,,,,,,,,,,5.0,6.0,,,,,,0,,,,RamUsageTester has an implementation of an identity hash set. I am not sure why it is there but now that RamUsageTester can only be used in the context of tests (ie. memory usage or speed are less of a concern) I think we can switch to Collections.newSetFromMap(new IdentityHashMap<>())?,,,,,,,,,,,,,,,,"05/Dec/14 18:11;jpountz;LUCENE-6095.patch;https://issues.apache.org/jira/secure/attachment/12685371/LUCENE-6095.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-12-05 18:15:08.557,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 23 05:02:29 UTC 2015,New,,,,,,,"0|i233yn:",9223372036854775807,,,,,,,,,"05/Dec/14 18:11;jpountz;Here is a patch.","05/Dec/14 18:15;dweiss;It's there because newSetFromMap wastes additional (unnecessary) memory.","05/Dec/14 18:15;rcmuir;+1","05/Dec/14 18:18;jpountz;[~dweiss] This is what I thought, thanks for confirming!","05/Dec/14 18:23;dweiss;Don't get me wrong -- you can remove it if you really want to. That custom identity set is just a plug for a missing class in the JDK; turning a map into a set still keeps the map's backend implementation and storage for keys, so it should be slightly slower and memory-hungry... but it may not matter at all in our case (unless we're really collecting super-large object graphs).

So if this simplifies things, feel free to remove it.","23/Feb/15 05:02;anshum;Bulk close after 5.0 release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Randomize file.encoding,LUCENE-4094,12558672,12542552,Sub-task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,30/May/12 20:40,09/May/16 18:30,30/Sep/19 08:38,05/Jul/12 10:49,,,,,,,,,,,4.0-BETA,6.0,,,general/test,,,1,,,,"Stated in the code:
{code}
    // TODO we can't randomize this yet (it drives ant crazy) but this makes tests reproduce
    // in case machines have different default charsets...
    sb.append("" -Dargs=\""-Dfile.encoding="" + System.getProperty(""file.encoding"") + ""\"""");
{code}

But this should work without any problems with junit4 because communication streams are separate and we're decoding output properly (or so I hope). 

Try and see what happens :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-30 20:47:55.586,,,false,,,,,,,,,,,,,,,243871,,,Wed Jul 11 23:12:44 UTC 2012,New,,,,,,,"0|i04gf3:",23925,,,,,,,,,"30/May/12 20:47;rcmuir;This would be really nice. The limitation before, if i remember right, was in ant-junit, because
by changing the default charset of the forked jvm, it would also change the encoding of the output from the test runner:
this caused it to go crazy.

It would be nice if junit4 could somehow separate these two concerns: if this could somehow work, imagine how tests running with
a default charset like UTF-16LE would actually fail when they rely upon the system charset and shouldnt, even if they use all 
ascii in their tests like they usually do.
","30/May/12 20:51;dweiss;junit4 is already default encoding independent (it uses its own communication channel). I haven't tested this intensively though so I'll give it a shot locally first and them if everything works well, switch the testing framework to randomize file.encoding.
","05/Jul/12 08:12;dweiss;Follow-up discussion wrt overriding file.encoding:
http://markmail.org/message/q4eeac7q6fjalbtd","05/Jul/12 09:58;rcmuir;I totally disagree with everything the jdk developers are saying. They tend to just whine when we find bugs in their shit.

we should continue to do this: its important to seek out these default charset bugs (this is because of their stupid design).
","05/Jul/12 10:08;dweiss;I understand their argument (""combination not encountered in practice"") but I disagree with the claim it should justify crappy code. The default charset should be independent of the OS-filesystem interaction. It should just work with UTF-16.

Anyway, when I run our stuff with enforced UTF-16 lots of weird things start to happen. new FileReader(file), benchmarks run forever (will provide a seed) and such. I'll commit in one by one and then we can start testing/ fixing locally.","11/Jul/12 23:12;hossman;hoss20120711-manual-post-40alpha-change",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use explicit capacity ArrayList instead of a LinkedList in MultiFieldQueryNodeProcessor,LUCENE-6827,12902670,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,06/Oct/15 11:24,09/May/16 18:30,30/Sep/19 08:38,06/Oct/15 12:45,,,,,,,,,,,5.4,6.0,,,,,,0,,,,,,,,,,,,,,,,,,,,"06/Oct/15 11:25;dweiss;LUCENE-6827.patch;https://issues.apache.org/jira/secure/attachment/12765163/LUCENE-6827.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-10-06 11:30:15.6,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 12:45:10 UTC 2015,New,,,,,,,"0|i2mmu7:",9223372036854775807,,,,,,,,,"06/Oct/15 11:25;dweiss;Patch. Also piggybacks {{new RuntimeException()}} if clone fails (should never happen means it probably will at some point -- we shouldn't ignore that quietly).","06/Oct/15 11:30;jpountz;+1","06/Oct/15 11:37;uschindler;Indeed, we should review all usages of LinkedList throughout Lucene/Solr. It is not clear why it was used here, but some places used it in pre Java 6 times to allow fast removal and addition of entries at beginning (typical LIFO/FIFO usage).

Since Java 6 the much better data structure for this is java.util.Deque (which LinkedList implements), but using ArrayDeque as implementation is much more heap/performance efficient.","06/Oct/15 11:47;dweiss;In fact I thought about that too -- if somebody uses LinkedList (or Hashtable or a Vector... any of these) then it's probably an ancient artefact and very likely a mistake and/or could be replaced with a faster implementation.

You should add these to forbidden APIs, Uwe :D","06/Oct/15 12:43;jira-bot;Commit 1707040 from [~dawidweiss] in branch 'dev/trunk'
[ https://svn.apache.org/r1707040 ]

LUCENE-6827: Use explicit capacity ArrayList instead of a LinkedList in MultiFieldQueryNodeProcessor","06/Oct/15 12:45;jira-bot;Commit 1707041 from [~dawidweiss] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1707041 ]

LUCENE-6827: Use explicit capacity ArrayList instead of a LinkedList in MultiFieldQueryNodeProcessor",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScandinavianFoldingFilterFactory and ScandinavianNormalizationFilterFactory,LUCENE-5013,12648931,,New Feature,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,janhoy,karl.wettin,karl.wettin,22/May/13 17:04,09/May/16 18:29,30/Sep/19 08:38,03/Jul/13 15:07,4.3,,,,,,,,,,4.4,6.0,,,modules/analysis,,,0,,,,"This filter is an augmentation of output from ASCIIFoldingFilter,
it discriminate against double vowels aa, ae, ao, oe and oo, leaving just the first one.

blåbærsyltetøj == blåbärsyltetöj == blaabaarsyltetoej == blabarsyltetoj
räksmörgås == ræksmørgås == ræksmörgaos == raeksmoergaas == raksmorgas

Caveats:
Since this is a filtering on top of ASCIIFoldingFilter äöåøæ already has been folded down to aoaoae when handled by this filter it will cause effects such as:

bøen -> boen -> bon
åene -> aene -> ane

I find this to be a trivial problem compared to not finding anything at all.

Background:
Swedish åäö is in fact the same letters as Norwegian and Danish åæø and thus interchangeable in when used between these languages. They are however folded differently when people type them on a keyboard lacking these characters and ASCIIFoldingFilter handle ä and æ differently.

When a Swedish person is lacking umlauted characters on the keyboard they consistently type a, a, o instead of å, ä, ö. Foreigners also tend to use a, a, o.

In Norway people tend to type aa, ae and oe instead of å, æ and ø. Some use a, a, o. I've also seen oo, ao, etc. And permutations. Not sure about Denmark but the pattern is probably the same.

This filter solves that problem, but might also cause new.
",,,,,,,,,,,,,,,,"23/May/13 16:03;karl.wettin;LUCENE-5013-2.txt;https://issues.apache.org/jira/secure/attachment/12584519/LUCENE-5013-2.txt","23/May/13 16:34;karl.wettin;LUCENE-5013-3.txt;https://issues.apache.org/jira/secure/attachment/12584524/LUCENE-5013-3.txt","23/May/13 19:07;karl.wettin;LUCENE-5013-4.txt;https://issues.apache.org/jira/secure/attachment/12584557/LUCENE-5013-4.txt","24/May/13 14:10;karl.wettin;LUCENE-5013-5.txt;https://issues.apache.org/jira/secure/attachment/12584696/LUCENE-5013-5.txt","27/May/13 11:44;karl.wettin;LUCENE-5013-6.txt;https://issues.apache.org/jira/secure/attachment/12584926/LUCENE-5013-6.txt","27/May/13 12:11;karl.wettin;LUCENE-5013.patch;https://issues.apache.org/jira/secure/attachment/12584933/LUCENE-5013.patch","22/May/13 17:04;karl.wettin;LUCENE-5013.txt;https://issues.apache.org/jira/secure/attachment/12584339/LUCENE-5013.txt",,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2013-05-22 17:16:00.911,,,false,,,,,,,,,,,,,,,329260,,,Fri Jan 23 13:44:36 UTC 2015,New,,,,,,,"0|i1kt2f:",329598,,,,,,,,,"22/May/13 17:04;karl.wettin;Code blessed with ASL2","22/May/13 17:16;elyograg;[~karl.wettin] I'm clueless when it comes to Scandinavian characters and languages ... but I do have a question.  Does this filter do anything that isn't already accomplished by ICUNormalizer2Filter, also incorporated in ICUFoldingFilter?

http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.ICUFoldingFilterFactory
http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.ICUNormalizer2FilterFactory
","22/May/13 17:35;karl.wettin;I do indeed believe that this does something different, at least as far as I can see.

Example:

People in Norway would spell the Swedish village of Särdal as Særdal, but when lacking those characters on their keyboard they would write Saerdal. In Sweden people would write Sardal. ASCIIFoldingFilter and friends would fold æ as ae and ä as a. The mismatch is primarily when a query contains the folded text, such as Saerdal. Folding all ä:s to ae will cause problem for people that just writes an a rather than ä. The same sort of mismatch will occur for å->aa, å->a, å->ao, ø->oe, ö->o. People tend to use different permutations of these alternatives and this filter normalizes it.

So this is a filter that solves mismatching on ASCII folds for people in Norway and Denmark searching in a Swedish index and vice verse.

See what I mean?","22/May/13 18:13;rcmuir;This is conceptually similar to the one for german (algorithm created by the snowball folks, but factored out of their stemmer):
http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanNormalizationFilter.java?view=markup

I think its nice to provide filters like this with language-specific normalizations. Though maybe the name could be simpler, (ScandinavianNormalizationFilter?)","22/May/13 18:18;sarowe;bq. GermanNormalizationFilter.java

This one operates directly on the input buffer, instead of copying to a (fixed 512 char, potentially too small) output buffer and then swapping.

bq. Though maybe the name could be simpler, (ScandinavianNormalizationFilter?)

+1","22/May/13 18:25;cm;bq. Though maybe the name could be simpler, (ScandinavianNormalizationFilter?)

+1","22/May/13 21:31;janhoy;Nice and needed component.

I have one question though, whether it is too aggressive to fold å->a, ö->o, æ->a etc?

In my experience it is better to skip the generic folding of ø/ö->oe/o, æ/ä->ae/a, å->aa/a which is rather destructive and instead normalize across Norwegian/Swedish/Danish the opposite way, preserving the semantic meaning:

{noformat}
ø,ö,oe->ø
æ,ä,ae->æ
å,aa->å
{noformat}

This will support most common cases and give:

blåbærsyltetøj == blåbärsyltetöj == blaabaersyltetoej (but not blabarsyltetoj)
räksmörgås == ræksmørgås == ræksmörgaas == raeksmoergaas (but not raksmorgas)

I think this would be a good compromise which avoids many false matches between ø/o, å/a, æ/a. One other example is the Norwegian word for ""berry"": *bær*. With the aggressive approach it would be *bar* which clashes with the words for ""bare"" and ""bar"" as well as clash with *bår* folded to *bar*. Other unfortunate Norwegian examples are bør/bor, klær/klår/klar, får/far, før/for, klør/klor, møte/mote, blå/bla... Perhaps the aggressive option could be a configuration option?

Btw. I have never seen the use of eo for ø or ea for æ

bq. Though maybe the name could be simpler, (ScandinavianNormalizationFilter?)
+1","22/May/13 22:39;karl.wettin;bq. I have one question though, whether it is too aggressive

You do indeed have a point I never thought of before. It makes a lot of sense to also go from ø,ö,oe->ø for those that are using a Scandinavian keyboard. This is a feature I too want now.

But the problem isn't just that we use ä and you use æ, it's native and non speakers sitting in front of the wrong sort of keyboard. Swedish people will most definitely in that situation write raksmorgas when searching for räksmörgås and most probably blabarsyltetoj when searching for blåbærssyltetøj, while my guess is that an American would write raksmorgas and blabaersyltetoj.
 

I ran a test too see how bad the Norwegian mismatches are using the ""Norsk scrabbleforbund""-dictionary:

593526 Norwegian words in dictionary.
  4698 Norwegian mismatches using ScandinavianNormalizerFilter.
  3943 Norwegian mismatches using ASCIIFoldingFilter.

That's something like 0,6%-0,8%. I find that totally acceptable, but I also suppose it depends on how you implement your index. If you're indexing nothing but the folded text then it might be a problem, but if it's something secondary on a disjunction with a lower boost, then it's hopefully just a matter of a few extra CPU-cycles and FS-seeks.
","22/May/13 23:07;elyograg;Does it make sense to have this filter do the Scandinavian folding *before* the ascii folding, rather than after?  Would that cause fewer search misses and false positives, or more?  Would it make sense to leave the ASCII step out, and let the user run it separately, either before or after according to the way they want it to work?

One of the things I really like about the ICU filters is that they handle international notions of uppercase and lowercase, so you're not dealing with just ASCII characters.  The example given on the wiki page is ß/SS, which honestly means little to me with my uneducated (American) viewpoint.  If this filter can do something similar for the differences between Scandinavian languages, that would really be useful.
","23/May/13 14:20;karl.wettin;bq. Does it make sense to have this filter do the Scandinavian folding before the ascii folding, rather than after? 

I implemented it the way I did because I want all the features of ASCIIFoldingFilter but slightly improved for my Scandinavian corpora. I suppose it's not completely wrong to say that ASCIIFoldingFilter is in this case used to fold æ->ae and is thus required to be executed prior to the Scandinavian normalization. 

What possibly makes most sense it to not rely on ASCIIFoldingFilter at all. To make it a pure ScandinavianNormalizationFilter without ü, ß and what not, that people would have to run a second pass through some ICU-filter in order to get that.","23/May/13 14:36;karl.wettin;A nice comment appeared on java-users, I'm pasting it in here to gather everything in one place.



22 maj 2013 kl. 20:29 skrev Petite Abeille:


On May 22, 2013, at 7:08 PM, Karl Wettin <karl.wettin@kodapan.se> wrote:

* Use a filter after ASCIIFoldingFilter that discriminate all use of ae, oe, oo, and other combination of double vowels, just keeping the first one.

I ended up with that solution.

https://issues.apache.org/jira/browse/LUCENE-5013

Interesting problem… perhaps you could generalize your solution a bit… for example, in, say, German, one could substitute 'ue' for 'ü', etc… so it looks like what you are after is folding double vowels… irrespectively of how they got there…

So… assuming something along the lines of Sean M. Burke Unidecode [1] for the purpose of ASCII transliteration, what's left is simply to fold double vowels, e.g.:

print( 1, Unidecode( 'blåbærsyltetøj' ):lower():gsub( '([aeiou]?)([aeiou]?)', '%1' ) )
print( 2, Unidecode( 'blåbärsyltetöj' ):lower():gsub( '([aeiou]?)([aeiou]?)', '%1' ) )
print( 3, Unidecode( 'blaabaarsyltetoej' ):lower():gsub( '([aeiou]?)([aeiou]?)', '%1' ) )
print( 4, Unidecode( 'blabarsyltetoj' ):lower():gsub( '([aeiou]?)([aeiou]?)', '%1' ) )
print( 5, Unidecode( 'Räksmörgås' ):lower():gsub( '([aeiou]?)([aeiou]?)', '%1' ) )
print( 6, Unidecode( 'Göteborg' ):lower():gsub( '([aeiou]?)([aeiou]?)', '%1' ) )
print( 7, Unidecode( 'Gøteborg' ):lower():gsub( '([aeiou]?)([aeiou]?)', '%1' ) )
print( 8, Unidecode( 'Über' ):lower():gsub( '([aeiou]?)([aeiou]?)', '%1' ) )
print( 9, Unidecode( 'ueber' ):lower():gsub( '([aeiou]?)([aeiou]?)', '%1' ) )
print( 10, Unidecode( 'uber' ):lower():gsub( '([aeiou]?)([aeiou]?)', '%1' ) )
print( 11, Unidecode( 'uuber' ):lower():gsub( '([aeiou]?)([aeiou]?)', '%1' ) )

1	blabarsyltetoj
2	blabarsyltetoj
3	blabarsyltetoj
4	blabarsyltetoj
5	raksmorgas
6	goteborg
7	goteborg	
8	uber	
9	uber	
10	uber	
11	uber	



[1] http://search.cpan.org/~sburke/Text-Unidecode-0.04/lib/Text/Unidecode.pm
","23/May/13 14:39;karl.wettin;Hmmm interesting thought though. I have to consider if it make sense to make it this generic. I think it might be problematic for some languages though, especially Dutch.
","23/May/13 15:00;markus17;Dutch does not use many accents except for some french loan words, the ASCIIFoldingFilter should suffice. Frisian does use a grave, aigu and circumflex quite frequently.","23/May/13 15:15;karl.wettin;bq. Dutch does not use many accents

My comment was regarding Petite's idea to use a more generic double vowel-removal filter. I fear it might be too destructive.

heersen -> hersen
noors -> nors
een -> en","23/May/13 15:57;karl.wettin;{quote}
I have one question though, whether it is too aggressive to fold å->a, ö->o, æ->a etc?
In my experience it is better to skip the generic folding of ø/ö->oe/o, æ/ä->ae/a, å->aa/a which is rather destructive and instead normalize across Norwegian/Swedish/Danish the opposite way, preserving the semantic meaning:
ø,ö,oe->ø
æ,ä,ae->æ
å,aa->å
{quote}

I think it should be two different filters rather than a setting. 

ScandinavianFoldingFilter (æ, ä,ae->a) and ScandinavianNormalizationFilter (ae,ä,æ->æ)?","23/May/13 16:03;karl.wettin;* Renamed to ScandinavianFoldingFilter
* Does not use ASCIIFoldingFilter (less destructive, bøen -> boen rather than bøen -> bon as previously)
* Modifies the input term char buffer rather than copying and switching
* \escaped utf-8 in code","23/May/13 16:26;sarowe;Karl, I like this approach better - focussed and self-contained.

bq. Does not use ASCIIFoldingFilter

I think the class javadoc needed updating?  E.g. ""This filter is an augmentation of output from ASCIIFoldingFilter""

Also, @author tags aren't allowed anymore - CHANGES.txt is where attribution happens.
","23/May/13 16:28;karl.wettin;Oups, artifacts from copy and pasting between two projects :-) Sorry. I'll send a new patch.","23/May/13 16:34;karl.wettin;Cleaned up docs","23/May/13 16:49;karl.wettin;Just realized that this new patch can cause an ArrayIndexOutOfBoundsException. Will send an updated version tomorrow.","23/May/13 16:49;rcmuir;Can the test be changed to use BaseTokenStreamTestCase?

here's an example: http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanNormalizationFilter.java?view=markup

We should also add a factory (and a test for that).","23/May/13 19:07;karl.wettin;* ScandinavianNormalizationFilter (new, feature as described by Jan)
* ScandinavianFoldingFilter
* Factories
* Factory tests, but their failing with SPI exceptions for me, not sure what to do here.

Jan, would you mind spending a few minutes reading javadocs of the filters to see if you think it seems to make sense?","23/May/13 19:55;janhoy;h4. Comments for class ScandinavianFoldingFilter:
* Typo in sentence ""It's is a semantically more...""
* ""I've also seen oo, ao, etc."" -> Don't use personal ""I"" in JavaDocs
* ""Not sure about Denmark..."" -> Better not to mention Denmark if you're not sure

h4. Comments for class ScandinavianFoldingFilterFactory:
* Comment ""Creates a new ScandinavianFoldingFilterFactory"" does not add any value

h4. Comments for class ScandinavianNormalizationFilter:
* ""...æäÆÄöøÖØ...translating them to åæøÅÆØ"" -> Should perhaps be ""æÆäÄöÖøØ...to æÆæÆøØøØ""

h4. Comments for class ScandinavianNormalizationFilterFactory:
* Unneccesary comment for constructor

Have not tested or really reviewed the code, but unit tests seem sound.

PS: Karl, you can use the same name LUCENE-5013.patch for every upload. JIRA will take care of greying out the older ones.","24/May/13 14:10;karl.wettin;Cleaned up the javadocs. 

This is as far as I can take this patch my self:

I need help with the TestFilterFactories, they throw SPI exceptions stating the factories are not available in the classpath via lookup. I shouldn't have to register them somewhere, right?","24/May/13 14:36;sarowe;bq. I need help with the TestFilterFactories, they throw SPI exceptions stating the factories are not available in the classpath via lookup. I shouldn't have to register them somewhere, right?

The following lines need to be added to {{src/resources/META-INF/services/org.apache.lucene.analysis.util.TokenFilterFactory}} - all tests pass for me when I do this:

{noformat}
org.apache.lucene.analysis.miscellaneous.ScandinavianFoldingFilterFactory
org.apache.lucene.analysis.miscellaneous.ScandinavianNormalizationFilterFactory
{noformat}
","27/May/13 11:44;karl.wettin;It's all good now.

Thanks for the help and input, everybody. Have fun, and I hope someone else but me finds this useful.","27/May/13 12:08;janhoy;Can you upload the patch as LUCENE-5013.patch ? That's the standard naming convention around here :)","27/May/13 12:10;karl.wettin;Patch blessed with ASL2","27/May/13 12:11;karl.wettin;Patch blessed with ASL2.","03/Jul/13 13:09;jira-bot;Commit 1499382 from janhoy@apache.org
[ https://svn.apache.org/r1499382 ]

LUCENE-5013: ScandinavianFoldingFilterFactory and ScandinavianNormalizationFilterFactory","03/Jul/13 13:18;janhoy;Oops, added at wrong root path, will fix","03/Jul/13 13:28;jira-bot;Commit 1499392 from janhoy@apache.org
[ https://svn.apache.org/r1499392 ]

LUCENE-5013: Revert bad commit","03/Jul/13 13:59;jira-bot;Commit 1499409 from janhoy@apache.org
[ https://svn.apache.org/r1499409 ]

LUCENE-5013: ScandinavianFoldingFilterFactory and ScandinavianNormalizationFilterFactory","03/Jul/13 15:08;jira-bot;Commit 1499437 from janhoy@apache.org
[ https://svn.apache.org/r1499437 ]

LUCENE-5013: ScandinavianFoldingFilterFactory and ScandinavianNormalizationFilterFactory (backport)","03/Jul/13 15:42;karl.wettin;Takk Jan! <3","23/Jul/13 18:37;sarowe;Bulk close resolved 4.4 issues","23/Jan/15 13:44;janhoy;Refguide paragraph added (SOLR-4493): https://cwiki.apache.org/confluence/display/solr/Language+Analysis#LanguageAnalysis-Scandinavian",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MockTokenizer throws away the character right after a token even if it is a valid start to a new token,LUCENE-5278,12673483,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,nik9000@gmail.com,nik9000@gmail.com,11/Oct/13 19:25,09/May/16 18:29,30/Sep/19 08:38,12/Oct/13 04:31,,,,,,,,,,,4.6,6.0,,,,,,0,,,,"MockTokenizer throws away the character right after a token even if it is a valid start to a new token.  You won't see this unless you build a tokenizer that can recognize every character like with new RegExp(""."") or RegExp(""..."").

Changing this behaviour seems to break a number of tests.",,,,,,,,,,,,,,,,"12/Oct/13 01:35;rcmuir;LUCENE-5278.patch;https://issues.apache.org/jira/secure/attachment/12608125/LUCENE-5278.patch","12/Oct/13 00:48;rcmuir;LUCENE-5278.patch;https://issues.apache.org/jira/secure/attachment/12608122/LUCENE-5278.patch","11/Oct/13 19:31;nik9000@gmail.com;LUCENE-5278.patch;https://issues.apache.org/jira/secure/attachment/12608061/LUCENE-5278.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-10-11 19:59:18.571,,,false,,,,,,,,,,,,,,,353106,,,Sat Oct 12 04:31:15 UTC 2013,New,,,,,,,"0|i1ovmn:",353393,,,,,,,,,"11/Oct/13 19:31;nik9000@gmail.com;This patch ""fixes"" the behaviour from my perspective but breaks a bunch of other tests.","11/Oct/13 19:59;rcmuir;I think i understand what you want: it makes sense. The only reason its the way it is today is because this thing historically came from CharTokenizer (see the isTokenChar?).

But it would be better if you could e.g. make a pattern like ([A-Z]a-z+) and for it to actually break FooBar into Foo, Bar rather than throwout out ""bar"" all together.

I'll dig into this!","12/Oct/13 00:48;rcmuir;Nice patch Nik!

I think this is ready: i tweaked variable names and rearranged stuff (e.g. i use -1 instead of Integer so we arent boxing and a few other things).

I also added some unit tests.

The main issues why tests were failing with your original patch:
* reset() needed to clear the buffer variables.
* the state machine needed some particular extra check when emitting a token: e.g. if you make a regex of "".."", but you send it ""abcde"", the tokens should be ""ab"", ""cd"", but not ""e"". so when we end on a partial match, we have to check that we are in an accept state.
* term-limit-exceeded is a special case (versus last character being in a reject state)","12/Oct/13 01:35;rcmuir;added a few more tests to TestMockAnalyzer so all these crazy corner cases are found there and not debugging other tests :)","12/Oct/13 01:57;jira-bot;Commit 1531479 from [~rcmuir] in branch 'dev/trunk'
[ https://svn.apache.org/r1531479 ]

LUCENE-5278: remove CharTokenizer brain-damage from MockTokenizer so it works better with custom regular expressions","12/Oct/13 01:57;rcmuir;I committed this to trunk: I did a lot of testing locally but I want to let Jenkins have its way with it for a few hours before backporting to branch_4x.","12/Oct/13 04:30;jira-bot;Commit 1531498 from [~rcmuir] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1531498 ]

LUCENE-5278: remove CharTokenizer brain-damage from MockTokenizer so it works better with custom regular expressions","12/Oct/13 04:31;rcmuir;Thanks again Nik!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
two unused variables in analysis/stempel/src/java/org/egothor/stemmer/Compile.java,LUCENE-6621,12840997,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,cpoerschke,rmp91,rmp91,26/Jun/15 21:29,09/May/16 18:29,30/Sep/19 08:38,31/Jul/15 10:03,6.0,,,,,,,,,,5.3,6.0,,,modules/analysis,,,0,,,,"{code:title=Compile.java|borderStyle=solid}
public static void main(java.lang.String[] args) throws Exception {
...
  for (int i = 1; i < args.length; i++) {
      // System.out.println(""["" + args[i] + ""]"");
      Diff diff = new Diff();
      int stems = 0;
      int words = 0;
...
{code}

In the file {{Compile.java}}, the variables {{stems}} and {{words}} are unused.
Although {{words}} gets incremented further in the file, it does not get referenced or used elsewhere.

{{stems}} is neither incremented nor used elsewhere in the project.

Are these variables redundant?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-07-30 19:25:39.767,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 26 13:05:57 UTC 2015,New,,,,,,,"0|i2gkgv:",9223372036854775807,,,,,,,,,"30/Jul/15 19:25;jira-bot;Commit 1693482 from [~cpoerschke] in branch 'dev/trunk'
[ https://svn.apache.org/r1693482 ]

LUCENE-6621: Removed two unused variables in analysis/stempel/src/java/org/egothor/stemmer/Compile.java","30/Jul/15 19:31;jira-bot;Commit 1693485 from [~cpoerschke] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1693485 ]

LUCENE-6621: Removed two unused variables in analysis/stempel/src/java/org/egothor/stemmer/Compile.java","31/Jul/15 10:03;cpoerschke;Thanks Rishabh!","06/Aug/15 19:50;rmp91;Thanks Christine! ","26/Aug/15 13:05;shalinmangar;Bulk close for 5.3.0 release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Term constructor javadoc refer to BytesRef.deepCopyOf,LUCENE-4483,12611711,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,paul.elschot@xs4all.nl,paul.elschot@xs4all.nl,13/Oct/12 19:47,09/May/16 18:29,30/Sep/19 08:38,07/Nov/12 03:55,4.1,,,,,,,,,,4.1,6.0,,,core/index,,,0,,,,"The Term constructor from BytesRef javadoc indicates that a clone needs to be made of the BytesRef.
But the clone() method of BytesRef is not what is meant, a deep copy needs to be made.",,,,,,,,,,,,,,,,"13/Oct/12 19:51;paul.elschot@xs4all.nl;LUCENE-4483.patch;https://issues.apache.org/jira/secure/attachment/12549037/LUCENE-4483.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-07 03:55:13.511,,,false,,,,,,,,,,,,,,,248472,,,Fri Mar 22 16:19:33 UTC 2013,New,Patch Available,,,,,,"0|i09ven:",55529,,,,,,,,,"07/Nov/12 03:55;rcmuir;Thanks Paul! Good catch!","25/Dec/12 11:45;paul.elschot@xs4all.nl;Fixed","22/Mar/13 16:19;commit-tag-bot;[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1406441

LUCENE-4483: clarify Term constructor documentation
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some cosmetics in test-framework,LUCENE-6338,12779609,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,paul.elschot@xs4all.nl,paul.elschot@xs4all.nl,04/Mar/15 22:17,09/May/16 18:29,30/Sep/19 08:38,05/Mar/15 08:56,,,,,,,,,,,5.1,6.0,,,,,,0,,,,"One assertion is done twice in AssertingScorer, and a QueryUtils message still uses skipTo instead of advance",,,,,,,,,,,,,,,,"04/Mar/15 22:18;paul.elschot@xs4all.nl;LUCENE-6338.patch;https://issues.apache.org/jira/secure/attachment/12702621/LUCENE-6338.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-05 08:56:09.812,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 00:31:05 UTC 2015,New,,,,,,,"0|i26ddz:",9223372036854775807,,,,,,,,,"05/Mar/15 08:56;jpountz;Thanks Paul. I replaced a couple more ""skipTo"" with ""advance"" when committing.","05/Mar/15 08:57;jira-bot;Commit 1664279 from [~jpountz] in branch 'dev/trunk'
[ https://svn.apache.org/r1664279 ]

LUCENE-6338: s/skipTo/advance/ and removed duplicate assertion.","05/Mar/15 08:59;jira-bot;Commit 1664281 from [~jpountz] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1664281 ]

LUCENE-6338: s/skipTo/advance/ and removed duplicate assertion.","05/Mar/15 23:23;paul.elschot@xs4all.nl;Thanks, the test framework is a good help to me.
But debugging still tends to make me a bit myopic :)
","15/Apr/15 00:31;thelabdude;Bulk close after 5.1 release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update READ_BEFORE_REGENERATING.txt,LUCENE-5964,12742790,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,sarowe,tkrah,tkrah,19/Sep/14 18:21,09/May/16 18:29,30/Sep/19 08:38,19/Sep/14 20:05,4.10,,,,,,,,,,5.0,6.0,,,modules/analysis,,,0,,,,"Reading the file READ_BEFORE_REGENERATING.txt from analysis/common/src/java/org/apache/lucene/analysis/standard tells me to use jflex trunk.
{{ant regenerate}} already uses ivy to get current jflex (1.6) which should be used - does the text still apply or is it obsolete?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-09-19 19:32:30.57,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 23 05:01:07 UTC 2015,New,,,,,,,"0|i209l3:",9223372036854775807,,,,,,,,,"19/Sep/14 19:32;sarowe;Thanks for bringing it up [~tkrah], that file is out of date.  That file can just be removed, since the build does the right thing now, pulling exact versions of JFlex it needs from Maven Central.","19/Sep/14 20:00;jira-bot;Commit 1626318 from [~sarowe@syr.edu] in branch 'dev/trunk'
[ https://svn.apache.org/r1626318 ]

LUCENE-5964: drop obsolete file telling users how to set up JFlex, since this is now automated","19/Sep/14 20:02;jira-bot;Commit 1626321 from [~sarowe@syr.edu] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1626321 ]

LUCENE-5964: drop obsolete file telling users how to set up JFlex, since this is now automated (merged trunk r1626318)","19/Sep/14 20:05;sarowe;Removed from trunk and branch_5x.

Thanks Torsten!","23/Feb/15 05:01;anshum;Bulk close after 5.0 release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor PackedInts API cleanups,LUCENE-5733,12718328,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,04/Jun/14 07:54,09/May/16 18:29,30/Sep/19 08:38,05/Jun/14 12:38,,,,,,,,,,,4.9,6.0,,,,,,0,,,,"The PackedInts API has quite some history now and some of its methods are not used anymore, eg. PackedInts.Reader.hasArray. I'd like to remove them.",,,,,,,,,,,,,,,,"04/Jun/14 07:56;jpountz;LUCENE-5733.patch;https://issues.apache.org/jira/secure/attachment/12648298/LUCENE-5733.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-05 12:38:39.174,,,false,,,,,,,,,,,,,,,396530,,,Thu Jun 05 12:38:39 UTC 2014,New,,,,,,,"0|i1w9of:",396651,,,,,,,,,"04/Jun/14 07:56;jpountz;Here is a patch:
 - removes Reader.hasArray and Reader.getArray
 - moves getBitsPerValues from Reader (unused there) to Mutable","05/Jun/14 12:38;jira-bot;Commit 1600637 from [~jpountz] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1600637 ]

LUCENE-5733: Remove PackedInts.Reader.(has|get)Array and move getBitsPerValue to PackedInts.Mutable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bring back the functional equivalent of tests.iters.min,LUCENE-4160,12595418,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,21/Jun/12 11:50,09/May/16 18:28,30/Sep/19 08:38,29/Jun/12 09:10,,,,,,,,,,,4.0-BETA,6.0,,,general/test,,,0,,,,"What is needed is effectively saying: ""repeat this test N times, but stop once you hit a failure"".

Previously it was ""tests.iters.min=X"" which is (still) kind of confusing to me because I don't understand how ""X"" is related to the original question.

I propose to implement a boolean ""tests.fastfail"" which would ignore any tests running on the same JVM after the first failure has been hit.

Those with fond memories of ""tests.iters.min"" speak up, please.
",,,,,,,,,,,,,,,,"28/Jun/12 10:56;dweiss;LUCENE-4160.patch;https://issues.apache.org/jira/secure/attachment/12533805/LUCENE-4160.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-21 12:07:22.379,,,false,,,,,,,,,,,,,,,243805,,,Wed Jul 11 23:15:01 UTC 2012,New,,,,,,,"0|i04g0f:",23859,,,,,,,,,"21/Jun/12 12:07;mikemccand;bq. I propose to implement a boolean ""tests.fastfail""

+1, I think that's a nice simplification over tests.iters.min.","25/Jun/12 16:56;hossman;bq. ""tests.iters.min=X"" which is (still) kind of confusing to me because I don't understand how ""X"" is related to the original question.

i think maybe you simplified the statement of the original question?

With tests.iters.min=X tests.iters=Y you were saying ""attempt to run this test Y times, and even if it fails, run it a minimum of X times."" (at least, that was the theory as i understood it, maybe it never actually worked that way).

bq. I propose to implement a boolean ""tests.fastfail""

...the key here being that tests.iters today already repeats the test the specified number of times, even if it fails -- and a new tests.fastfail"" would default to ""false"" but if it was true then you could have the old behavior of tests.iters.min=1.

seems fine to me ... i think hte main motivation of tests.iters.min isn't really applicable anymore since the default logic for running multiple iterations is basically the reverse of what it use to be. 

an alternative way to think about it would be to add a ""tests.iters.max"", since tests.iters already behaves similar to what tests.iters.min use to do.

Or just say to hell with it, and people who want the X,Y equivilent from before can run...

{noformat}
ant -Dtestcase=... -Dtestmethod=... -Dtests.iters=X \
&& \
ant -Dtestcase=... -Dtestmethod=... -Dtests.failfast -Dtests.iters=${Y-X}
{noformat}
","28/Jun/12 09:29;dweiss;Thanks for the clarification, Hoss. We could also do:

{noformat}
-Dtests.iters=X -Dtests.minfailures=Y
{noformat}

What this would do is repeat everything X times but ignore anything after the first Y failures... So if you'd like to quickly abort after the first failure, you'd do:

{noformat}
-Dtests.iters=X -Dtests.minfailures=1
{noformat}

and if you wanted more (for whatever reason) you could wait for more, but still abort earlier than X. I'll just provide a patch for this and we'll see how it turns out in practice.","28/Jun/12 09:30;dweiss;Now that I think of it minfailures or maxfailures depends on how you look at the problem :)","28/Jun/12 10:56;dweiss;Patch against trunk.
{noformat}
ant -Dtests.maxfailures=M
{noformat}

will cause any test after the first M failures to be assumption-ignored. Example:

{noformat}
   [junit4] IGNOR/A 0.03s | TestBuhu.testFailSometimes {#96 seed=[15604D6381DA415B:BE5164F081683030]}
   [junit4]    > Assumption #1: Ignored, failures limit reached (1 >= 1).
   [junit4] IGNOR/A 0.01s | TestBuhu.testFailSometimes {#97 seed=[15604D6381DA415B:E515E49B8D52E84E]}
   [junit4]    > Assumption #1: Ignored, failures limit reached (1 >= 1).
   [junit4] IGNOR/A 0.02s | TestBuhu.testFailSometimes {#98 seed=[15604D6381DA415B:2128D6F71D04F74D]}
   [junit4]    > Assumption #1: Ignored, failures limit reached (1 >= 1).
   [junit4] IGNOR/A 0.01s | TestBuhu.testFailSometimes {#99 seed=[15604D6381DA415B:EBBC938E1A402A5E]}
   [junit4]    > Assumption #1: Ignored, failures limit reached (1 >= 1).
   [junit4]    > (@AfterClass output)
   [junit4]   2> NOTE: test params are: codec=Lucene40: {}, sim=RandomSimilarityProvider(queryNorm=false,coord=true): {}, locale=hu, timezone=US/Arizona
   [junit4]   2> NOTE: Linux 3.0.0-21-generic amd64/Oracle Corporation 1.7.0 (64-bit)/cpus=2,threads=1,free=50888552,total=78512128
   [junit4]   2> NOTE: All tests run in this JVM: [TestBuhu]
   [junit4]   2> 
   [junit4] Completed in 1.44s, 100 tests, 1 failure, 91 skipped <<< FAILURES!
   [junit4]  
   [junit4] JVM J0:     0.70 ..     3.16 =     2.47s
   [junit4] Execution time total: 3.21 sec.
   [junit4] Tests summary: 1 suite, 100 tests, 1 failure, 91 ignored (91 assumptions)
{noformat}","28/Jun/12 10:59;dweiss;One note -- this property is per-jvm-global in the sense that you don't need -Dtests.iters, you can wait for M failures in general, so for example:
{noformat}
ant test-core -Dtests.maxfailures=1
{noformat}
will ignore any remaining tests after the first failure. This applies per-JVM though, so if you're running with > 1 fork ed JVM then only that JVM's tests will be ignored. ","28/Jun/12 11:02;rcmuir;What is the use case of the min/maxfailures (besides just your idea of a fastfail boolean, which I liked)","28/Jun/12 11:11;dweiss;Maybe somebody would want to wait for 5 failures instead of 1 to get a bunch of stack traces? A boolean can be emulated at ant level, it's basically -Dtests.maxfailures=1... Don't know, really -- I admit I don't need this so I'm shooting at the dark here, it seemed to be useful for folks. To me it can be a boolean as well.","28/Jun/12 11:16;rcmuir;I think i originally caused the complexity by wanting to still have a way to run a test like 1000 times and look at the failure rate. This is occasionally useful: e.g. this test fails 2% of the time and I improved the test to fail 10% of the time or whatever :)

But really I think its more useful in general to have 'fastfail' on by default... especially now that in general tests are reproducing a lot better than before...","28/Jun/12 16:08;hossman;bq. I think i originally caused the complexity by wanting to still have a way to run a test like 1000 times and look at the failure rate.

right, the original driving usecase is already possible with the way tests.iters works now; it just so happened that the way it worked before you could not only say ""run this test at least 1000 times, even if it fails, so i can compute a pass/fail rate and look for patterns"" you could say ""try to run this test at least 1000 times, even if it fails, so i can compute a pass/fail rate and look for patterns -- but if it doesn't fail, just keep on trying up to 5000 times for good measure.  I'm going to lunch anyway.""

if tests.maxfailures is just as easy to implement as tests.fastfail (and already implemented in this patch) then i say go with that.

we can always add ant sugar so that -Dtests.failfast=true sets tests.maxfailures=1","28/Jun/12 17:33;dweiss;Yes, this patch is already working. I'll make it an alias so that -Dtests.failfast=[on/true/yes] will also work and commit it in.","11/Jul/12 23:15;hossman;hoss20120711-manual-post-40alpha-change",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding a class to help debug a TokenFilter,LUCENE-7003,12935303,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Implemented,,Gueust,Gueust,30/Jan/16 00:30,26/Apr/16 21:43,30/Sep/19 08:38,26/Apr/16 21:42,,,,,,,,,,,,,,,modules/test-framework,,,0,,,,"When debugging a TokenFilter, I need to run it on strings to see the details of the token generated.

I haven't found any class doing that, but I did found a TODO comment asking for one, so here it is.

It may already exist, or make more sense to put in in another location. Just tell me if it is the case.",,,,,,,,,,,,,,,,"30/Jan/16 00:30;Gueust;LUCENE-7003.PATCH;https://issues.apache.org/jira/secure/attachment/12785302/LUCENE-7003.PATCH",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,9223372036854775807,,,2016-01-30 00:30:15.0,New,Patch Available,,,,,,"0|i2s6hj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MatchAllDocsQuery should not expose approximations,LUCENE-6761,12858636,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,25/Aug/15 14:23,15/Dec/15 13:55,30/Sep/19 08:38,02/Sep/15 10:23,,,,,,,,,,,5.4,,,,,,,0,,,,"This is a relic from when queries had to deal with deleted docs themselves: MatchAllDocsQuery used to return an iterator that matched everything as an approximation and applied live docs in the confirmation phase. But now that live docs are checked on top, it should just returns an efficient Scorer that matches every document.",,,,,,,,,,,,,,,,"01/Sep/15 10:26;jpountz;LUCENE-6761.patch;https://issues.apache.org/jira/secure/attachment/12753502/LUCENE-6761.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-09-02 10:18:36.37,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 02 10:23:27 UTC 2015,New,,,,,,,"0|i2jcdj:",9223372036854775807,,,,,,,,,"01/Sep/15 10:26;jpountz;Here is a simple patch.","02/Sep/15 10:18;jira-bot;Commit 1700754 from [~jpountz] in branch 'dev/trunk'
[ https://svn.apache.org/r1700754 ]

LUCENE-6761: MatchAllDocsQuery's Scorers do not expose approximations anymore.","02/Sep/15 10:23;jira-bot;Commit 1700759 from [~jpountz] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1700759 ]

LUCENE-6761: MatchAllDocsQuery's Scorers do not expose approximations anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a DocValuesFormat for sparse doc values,LUCENE-4921,12641661,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,jpountz,jpountz,09/Apr/13 16:35,06/Nov/15 15:36,30/Sep/19 08:38,06/Nov/15 15:36,,,,,,,,,,,,,,,core/codecs,,,1,gsoc2014,,,"We could have a special DocValuesFormat in lucene/codecs to better handle sparse doc values.

See http://search-lucene.com/m/HUeYW1RlEtc",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-04-09 16:42:39.669,,,false,,,,,,,,,,,,,,,322077,,,Fri Nov 06 15:36:45 UTC 2015,New,,,,,,,"0|i1jkjb:",322422,,,,,,,,,"09/Apr/13 16:42;rcmuir;a good baseline could be something as simple as passing COMPACT to the default DVConsumer?

or we could provide something that works entirely different... there are a lot of possibilities.","06/Nov/15 15:36;jpountz;This functionnality has been folded into the default codec via LUCENE-6863",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoostingTermQuery's BoostingSpanScorer class should be protected instead of package access,LUCENE-1234,12391503,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,gsingers,vajda,vajda,15/Mar/08 00:07,21/Oct/15 14:28,30/Sep/19 08:38,15/Mar/08 01:47,2.3.1,,,,,,,,,,,,,,core/search,,,0,,,,"Currently, BoostingTermScorer, an inner class of BoostingTermQuery is not accessible from outside the search.payloads
making it difficult to write an extension of BoostingTermQuery. The other inner classes are protected already, as they should be.",,,,,,,,,,,,,,,,"15/Mar/08 00:09;vajda;patches-lucene-2.3.1;https://issues.apache.org/jira/secure/attachment/12377954/patches-lucene-2.3.1",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12513,,,Sat Mar 15 00:20:56 UTC 2008,New,Patch Available,,,,,,"0|i04y8v:",26813,,,,,,,,,"15/Mar/08 00:09;vajda;patch against lucene-2.3.1 sources","15/Mar/08 00:20;vajda;The inaccessible class is called BoostingSpanScorer.
The method I'd to override there is the score() method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typo in version.properties file,LUCENE-6363,12782734,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,anshum,anshum,anshum,18/Mar/15 00:25,15/Apr/15 00:30,30/Sep/19 08:38,18/Mar/15 00:28,,,,,,,,,,,5.1,,,,,,,0,,,,"As reported by [~arafalov], there's a typo in the lucene version.properties file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-03-18 00:25:57.882,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 00:30:32 UTC 2015,New,,,,,,,"0|i26w8f:",9223372036854775807,,,,,,,,,"18/Mar/15 00:25;anshum;https://github.com/apache/lucene-solr/pull/134/files","18/Mar/15 00:25;jira-bot;Commit 1667428 from [~anshumg] in branch 'dev/trunk'
[ https://svn.apache.org/r1667428 ]

LUCENE-6363: Fix typo in version.properties file","18/Mar/15 00:27;jira-bot;Commit 1667429 from [~anshumg] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1667429 ]

LUCENE-6363: Fix typo in version.properties file (merge from trunk)","18/Mar/15 00:28;anshum;Thanks [~arafalov].","18/Mar/15 00:40;githubbot;Github user arafalov commented on the pull request:

    https://github.com/apache/lucene-solr/pull/134#issuecomment-82654575
  
    Merged in LUCENE-6363
","15/Apr/15 00:30;thelabdude;Bulk close after 5.1 release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DocValuesDocIdSet: check deleted docs before doc values,LUCENE-6022,12750094,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,23/Oct/14 15:44,23/Feb/15 05:02,30/Sep/19 08:38,23/Oct/14 16:52,,,,,,,,,,,5.0,,,,,,,0,,,,"When live documents are not null, DocValuesDocIdSet checks if doc values match the document before the live docs. Given that checking if doc values match could involve a heavy computation (eg. geo distance) and that the default codec has live docs in memory but doc values on disk, I think it makes more sense to check live docs first?",,,,,,,,,,,,,,,,"23/Oct/14 15:46;jpountz;LUCENE-6022.patch;https://issues.apache.org/jira/secure/attachment/12676607/LUCENE-6022.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-10-23 16:46:55.375,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 23 05:02:53 UTC 2015,New,,,,,,,"0|i21i27:",9223372036854775807,,,,,,,,,"23/Oct/14 15:46;jpountz;Here is a patch.","23/Oct/14 16:46;rjernst;+1, patch looks good. And I especially like the simplification to {{nextDoc()}} to just use {{advance(doc + 1)}}...we should do this in more places.","23/Oct/14 16:49;jira-bot;Commit 1633879 from [~jpountz] in branch 'dev/trunk'
[ https://svn.apache.org/r1633879 ]

LUCENE-6022: DocValuesDocIdSet checks live docs before doc values.","23/Oct/14 16:51;jpountz;bq. And I especially like the simplification to nextDoc() to just use advance(doc + 1)...we should do this in more places.

Maybe we should make it the default impl of nextDoc() in DocIdSetIterator (it's an abstract class, not an interface).","23/Oct/14 16:52;jira-bot;Commit 1633883 from [~jpountz] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1633883 ]

LUCENE-6022: DocValuesDocIdSet checks live docs before doc values.","23/Oct/14 16:52;jpountz;Thanks for the review Ryan!","23/Oct/14 21:08;dsmiley;+1 yeah the patch is good.

This class got me curious what it's for.  Despite its name and what it's javadocs claim, it doesn't appear to have anything to do with DocValues, even if some DocValues code might in turn use it.  It looks remarkably like FilteredDocIdSet but instead of wrapping another DocIdSet, it wraps a Bits.  I've seem somewhat similar classes actually.  I propose that it be renamed to not have DocValues in its name.  But to what?  BitsFilteredDocIdSet is already taken.  It needn't have Bits in the name; that could be just a feature (optional Bits filter).
_gotta go now..._","24/Oct/14 15:20;jpountz;I agree this thing could be merged with FilteredDocIdSet. The only additional thing it has is the optimization when deleted docs are a bit set but I'm wondering if it really helps in practice given that we try to merge more agressively segments that have lots of deleted documents.","23/Feb/15 05:02;anshum;Bulk close after 5.0 release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jflex files are out of sync with the generated files,LUCENE-6243,12774888,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Invalid,,dweiss,dweiss,13/Feb/15 13:17,13/Feb/15 13:19,30/Sep/19 08:38,13/Feb/15 13:19,,,,,,,,,,,,,,,,,,0,,,,"This kind of sucks I think -- there have been a few changes to generated Java files that make the jflex sources out of sync. This means that if somebody regenerates those files he or she has to apply the same changes manually again. For example:

https://svn.apache.org/viewvc?view=revision&revision=r1625586

I understand some of these would require changes in jflex but just wanted to raise the issue.

Also, ant jflex currently doesn't work for me on trunk:
{code}
cd lucene/analysis/common
ant jflex

-jflex-StandardAnalyzer:
    [jflex] Generated: StandardTokenizerImpl.java
    [jflex] Generated: ClassicTokenizerImpl.java

-jflex-UAX29URLEmailTokenizer:
    [jflex] Generated: UAX29URLEmailTokenizerImpl.java

-jflex-wiki-tokenizer:
    [jflex] Generated: WikipediaTokenizerImpl.java

generate-jflex-html-char-entities:
     [exec]   File ""htmlentity.py"", line 22
     [exec]     print get_apache_license()
     [exec]                            ^
     [exec] SyntaxError: invalid syntax

BUILD FAILED
C:\Work\lucene-solr-svn\trunk\lucene\analysis\common\build.xml:49: exec returned: 1
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 13 13:19:34 UTC 2015,New,,,,,,,"0|i25ljb:",9223372036854775807,,,,,,,,,"13/Feb/15 13:19;dweiss;Oh jeez... I now see the patch in full. Ok, nevermind.
{code}
  <macrodef name=""run-jflex-and-disable-buffer-expansion"">
    <attribute name=""dir""/>
    <attribute name=""name""/>
    <sequential>
      <jflex file=""@{dir}/@{name}.jflex"" outdir=""@{dir}"" nobak=""on"" inputstreamctor=""false""/>
      <!-- LUCENE-5897: Disallow scanner buffer expansion -->
      <replaceregexp file=""@{dir}/@{name}.java""
                     match=""[ \t]*/\* is the buffer big enough\? \*/\s+if \(zzCurrentPos >= zzBuffer\.length.*?\}[ \t]*\r?\n""
                     replace="""" flags=""s"" />
      <replaceregexp file=""@{dir}/@{name}.java""
                     match=""private static final int ZZ_BUFFERSIZE =""
                     replace=""private int ZZ_BUFFERSIZE =""/>
      <replaceregexp file=""@{dir}/@{name}.java""
                     match=""int requested = zzBuffer.length - zzEndRead;""
                     replace=""int requested = zzBuffer.length - zzEndRead - zzFinalHighSurrogate;""/>
      <replaceregexp file=""@{dir}/@{name}.java""
                     match=""(zzFinalHighSurrogate = 1;)(\r?\n)""
                     replace=""\1\2          if (totalRead == 1) { return true; }\2""/>
    </sequential>
  </macrodef>
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MANIFEST.MF cleanup (main jar and luci customizations),LUCENE-908,12370964,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,05/Jun/07 20:45,08/Sep/14 21:14,30/Sep/19 08:38,22/Aug/07 23:18,,,,,,,,,,,2.3,,,,general/build,,,0,,,,"there are several problems with the MANIFEST.MF file used in the core jar, and some inconsistencies in th luci jar:

Lucli's build.xml has an own ""jar"" target and does not use the jar target from common-build.xml. The result
is that the MANIFEST.MF file is not consistent and the META-INF dir does not contain LICENSE.TXT and NOTICE.TXT.

Is there a reason why lucli behaves different in this regard? If not I think we should fix this.",,,,,,,,,,,,,,,,"06/Jun/07 00:39;hossman;LUCENE-908.patch;https://issues.apache.org/jira/secure/attachment/12359014/LUCENE-908.patch","19/Aug/07 17:11;michaelbusch;lucene-908-new.patch;https://issues.apache.org/jira/secure/attachment/12364108/lucene-908-new.patch","06/Jun/07 19:38;michaelbusch;lucene-908.patch;https://issues.apache.org/jira/secure/attachment/12359104/lucene-908.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2007-06-05 21:41:49.713,,,false,,,,,,,,,,,,,,,12834,,,Wed Aug 22 23:18:10 UTC 2007,New,,,,,,,"0|i0509b:",27139,,,,,,,,,"05/Jun/07 21:41;steven_parkes;I'm pretty sure it's to get the stuff that's in the MANIFEST.MF in there: the Main-Class header, in particular. With this, you can say java -jar jar w/o having to specify the main class.

Probably simplest just to also add the necessary files/lines manually? It's not DRY with the code common-build.xml, but it's simple for an uncommon case.","05/Jun/07 21:56;hossman;the existing jar logic in common-build.xml could be refacotred into a macro with a a nested tag option so that contribs could add additional items, that would probably be the cleanest way to support MANIFEST.MF add ons.

on a related subject, when i was setting up the solr MANIFEST.MF i discovered lots of things are ""wrong"" about the way Lucene's MANIFEST file is built (aparently i never raised them in lucene-java, or if i did we never did anythng about them), here are the comments from Solr's build.xml that we may also want to fix...

http://svn.apache.org/viewvc/lucene/solr/trunk/build.xml
        <!--
        http://java.sun.com/j2se/1.5.0/docs/guide/jar/jar.html#JAR%20Manifest
        http://java.sun.com/j2se/1.5.0/docs/guide/versioning/spec/versioning2.html
        http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Package.html
        http://java.sun.com/j2se/1.5.0/docs/api/java/util/jar/package-summary.html
        http://java.sun.com/developer/Books/javaprogramming/JAR/basics/manifest.html
        -->
        <!-- Don't set 'Manifest-Version' it identifies the version of the
             manifest file format, and should allways be 1.0 (the default)

             Don't set 'Created-by' attribute, it's purpose is 
             to identify the version of java used to build the jar,
             which ant will do by default.

             Ant will happily override these with bogus strings if you
             tell it to, so don't.

             NOTE: we don't use section info because all of our manifest data
             applies to the entire jar/war ... no package specific info.
        -->
        <!-- spec version must match ""digit+{.digit+}*"" -->
        <!-- impl version can be any string -->






","05/Jun/07 22:04;michaelbusch;Hi Hoss,

I think this makes sense. It would be great if you could provide a patch here?","06/Jun/07 00:39;hossman;quick pass at adopting some of the stuff i learned doing the Solr MANIFEST.MF ... i haven't tested it extensively (Michael i'm hoping you can take the ball and run with it, i've got about a million other things going on at the moment)

note: it was a while ago when i looked into all of this MANIFEST stuff and i'm not sure i fully understood it then, let alone now.

patch moves jaring into a new macro (jarify) ... contribs can override the ""jar-core"" target to call jarify and override some options as well as add new <attributes> to appear in the manifest file.

manifest now includes a lot more information then it did before.

things this doesn't address...
  * manifest file in demo war file
  * manifest file in any of gdata's jars/war (it doesn't use the contrib-build.xml either)
  * spec version must match ""digit+{.digit+}*"" ... this is true for our official releases, but broken in our nightlies.
  * need to svn remove the existing luci MANIFEST file
  * should luci's ""Class-Path"" refer to the full name of the lucene core jar?
","06/Jun/07 05:46;michaelbusch;> Michael i'm hoping you can take the ball and run with it,

Thanks for the pass, Hoss, I'm already running...","06/Jun/07 18:58;michaelbusch;> * manifest file in any of gdata's jars/war (it doesn't use the contrib-build.xml either) 
> * should luci's ""Class-Path"" refer to the full name of the lucene core jar? 

I would like to ask the contrib owners to take care of these issues.

> * spec version must match ""digit+{.digit+}*"" ... this is true for our official releases, 
>   but broken in our nightlies. 

I will leave this for now as this patch doesn't change the spec version.

> * need to svn remove the existing luci MANIFEST file 
> * manifest file in demo war file 

Will take care...
","06/Jun/07 19:38;michaelbusch;In addition to Hoss' great patch this one:

- changes the MANIFEST file in the demo jar and war in a consistent way
- adds a patternset via refid to the metainf type of jarify. This can
  be used by contrib modules to add additional files to the META-INF dir.
  I use it to add SNOWBALL-LICENSE.txt to META-INF of the snowball jar.
  
I'm planning to commit this soon.","06/Jun/07 21:11;hossman;A couple of random thoughts...

1) macro's can take multiple optional named <element> tags to embed in their bodies ... so instead of declaring a refid for what to include in the metinf, callers of the macro could put <metainf> call directly in the call to <jarify>

2) one way to reduce some redundancy in the build files (between jar-core, jar-demo, and war-demo) might be to use the <manifest> task instead of the <manifest> sub element of the <jar> task ... there are a few subtle differences but the main key is that the <manifest> taks let's you build up a file which you can then refer to by name from the <jar> task ... we could have a single <buildmanifest> macro with all of the common attributes in it and then it could be called from the various jar/war targets just before building the actual jar using attributes and <element> tags to customize things that need to be different. 


...neither of these are crucial, they're just things you might want to consider to keep the build files smaller (and arguably simpler)","07/Jun/07 00:27;michaelbusch;As always these are very good recommendations Hoss! I think I will commit my patch for 2.2, because it works fine. But I will leave this issue open (just clear the Fix version) to keep in mind that we want to make these improvements.
","07/Jun/07 00:50;michaelbusch;Patch committed. Leaving this issue open for the simplifications suggested by Hoss.","19/Aug/07 17:11;michaelbusch;This patch:

- removes the patternset ""metainf.includes""; adds the optional element <metainf-includes> to the body of jarify
- adds the macro ""build-manifest"" to common-build.xml to remove redundant code from the demo targets","22/Aug/07 23:18;michaelbusch;Committed. Revision: 568766",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Explore preset dictionaries for CompressingStoredFieldsFormat,LUCENE-5046,12651910,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Duplicate,jpountz,jpountz,jpountz,09/Jun/13 09:26,29/Aug/14 17:15,30/Sep/19 08:38,29/Aug/14 17:15,,,,,,,,,,,,,,,,,,1,,,,"I discussed this possible improvement with Stefan Pohl and Andrzej Białecki at Berlin Buzzwords: By having preset dictionaries (which could be user-provided and/or computed on a per-block basis), decompression could be faster since we would never have to decompress several documents from a block in order to access a single document.

One drawback is that it would require putting some boundaries in the compressed stream, so it would maybe decrease a little the compression ratio. But then if decompression is faster, we could also afford larger blocks, so I think this is worth exploring.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,332234,,,Fri Aug 29 17:15:23 UTC 2014,New,,,,,,,"0|i1lbbr:",332563,,,,,,,,,"29/Aug/14 17:15;jpountz;I close this issue in favor of LUCENE-5914 that uses shared dictionaries in order to make decompression faster.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose SloppyMath earth diameter table,LUCENE-5457,12695988,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,19/Feb/14 15:50,16/Mar/14 13:02,30/Sep/19 08:38,19/Feb/14 17:39,,,,,,,,,,,4.7,,,,,,,0,,,,LUCENE-5271 introduced a table in order to get approximate values of the diameter of the earth given a latitude. This could be useful for other computations so I think it would be nice to have a method that exposes this table.,,,,,,,,,,,,,,,,"19/Feb/14 15:51;jpountz;LUCENE-5457.patch;https://issues.apache.org/jira/secure/attachment/12629791/LUCENE-5457.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-19 16:22:49.451,,,false,,,,,,,,,,,,,,,374466,,,Wed Feb 19 17:39:37 UTC 2014,New,,,,,,,"0|i1sj3z:",374766,,,,,,,,,"19/Feb/14 15:51;jpountz;Here is a patch.","19/Feb/14 16:22;rcmuir;Is there a reason to go from radius to diameter (i don't even know whats happening here, just curious). 

Maybe [~giladbarkai] or  [~rjernst] can review better.
","19/Feb/14 16:24;rcmuir;or maybe the cause of my confusion is the local was named badly before?","19/Feb/14 16:29;jpountz;Right I forgot to mention that. I changed the variable name from radius to diameter because it looked to me like it actually was a diameter.","19/Feb/14 17:19;rjernst;+1, patch looks good.

{quote}
maybe the cause of my confusion is the local was named badly before?
{quote}
I had asked for the 2*radius in the computation to be moved into the table in LUCENE-5271, and the table was renamed, but the variable was missed and I didn't notice it.","19/Feb/14 17:25;jpountz;Thanks Ryan. I plan to commit to the 4.7 branch as well if there is no objection (since we need to respin).","19/Feb/14 17:28;jira-bot;Commit 1569835 from [~jpountz] in branch 'dev/trunk'
[ https://svn.apache.org/r1569835 ]

LUCENE-5457: Expose SloppyMath earth diameter table","19/Feb/14 17:30;jira-bot;Commit 1569839 from [~jpountz] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1569839 ]

LUCENE-5457: Expose SloppyMath earth diameter table","19/Feb/14 17:39;jira-bot;Commit 1569847 from [~jpountz] in branch 'dev/branches/lucene_solr_4_7'
[ https://svn.apache.org/r1569847 ]

LUCENE-5457: Expose SloppyMath earth diameter table","19/Feb/14 17:39;gilad;+1 
Sorry for the confusion, indeed it should be diameter as the multiplication (*2) was moved to the pre-computed table, hence saving the operation in runtime as per Ryan's comment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Point2D defines equals by comparing double types with ==,LUCENE-1895,12434972,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,cmale,markrmiller@gmail.com,markrmiller@gmail.com,07/Sep/09 03:36,02/Dec/13 04:00,30/Sep/19 08:38,02/Dec/13 04:00,,,,,,,,,,,,,,,modules/spatial,,,1,,,,"Ideally, this should allow for a margin of error right?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-04-05 15:20:59.088,,,false,,,,,,,,,,,,,,,11872,,,Mon Dec 02 04:00:56 UTC 2013,New,,,,,,,"0|i04u53:",26148,,,,,,,,,"05/Apr/10 15:20;cmale;Mark,

Its been over 6 months I know, but do you remember why you suggested this? Was there a particular case you had in mind? 

I'd like to see Point2D (or Point as I'd like it renamed) to be an all purpose class for the spatial work, so it seems comparing the values exactly would be a good idea.","05/Apr/10 15:23;markrmiller@gmail.com;I put this up not knowing really anything about the specific use case(s) of the Point2D class - I have never used Spatial - so close if it makes sense to do so.

My generic worry is that you can come to the *same* double value in two different ways, but == will not find them to be equal.","05/Apr/10 15:28;cmale;{quote}
My generic worry is that you can come to the same double value in two different ways, but == will not find them to be equal.
{quote}

How so?","05/Apr/10 15:44;cmale;As suggested, its best to use an epsilon when comparing doubles, much like junit.

I will work on a patch.","02/Dec/13 04:00;dsmiley;Closing because the old spatial contrib module referenced here no longer exists as of Solr 4.

FWIW I don't agree with the premise of this ""bug"".  Points are not the same if their coordinates are not 100% the same (==).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BaseDocIdSetTestCase,LUCENE-5100,12657149,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,10/Jul/13 21:03,05/Oct/13 10:18,30/Sep/19 08:38,12/Jul/13 07:18,,,,,,,,,,,4.5,,,,,,,0,,,,"As Robert said on LUCENE-5081, we would benefit from having common testing infrastructure for our DocIdSet implementations.",,,,,,,,,,,,,,,,"11/Jul/13 11:32;jpountz;LUCENE-5100.patch;https://issues.apache.org/jira/secure/attachment/12591833/LUCENE-5100.patch","10/Jul/13 21:30;jpountz;LUCENE-5100.patch;https://issues.apache.org/jira/secure/attachment/12591730/LUCENE-5100.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-07-10 22:10:06.906,,,false,,,,,,,,,,,,,,,337372,,,Sat Oct 05 10:18:56 UTC 2013,New,,,,,,,"0|i1m6xb:",337695,,,,,,,,,"10/Jul/13 21:30;jpountz;Patch. This is mostly a factorization of the tests of EliasFanoDocIdSet and WAH8DocIdSet. It is hard to factor the tests of FixedBitSet and OpenBitSet since they don't share a common interface for prevSetBit and nextSetBit for instance. Nothing that we can't improve but I am wondering why we need OpenBitSet? Are there any cases where we need the bit set to grow on demand? I have checked a few places where it was used (it is used all over the code base) and every time it seemed to me that it could have been advantageously replaced with a FixedBitSet?","10/Jul/13 22:10;rcmuir;I know of a few places where I used this in lucene-core where FixedBitSet cannot currently be used:
Anything having to do with SortedSetDocValues, where ordinals can exceed > 2B, because OpenBitSet can do that. FixedBitSet is limited to int.

However use for example, in SloppyPhraseScorer seems unnecessary.","11/Jul/13 11:32;jpountz;Thanks for the explanation, Robert. I tried to factorize some code between TestFixedBitSet and TestOpenBitSet by adding an abstraction level on top of both FixedBitSet and OpenBitSet but its complexity made the tests even harder to read, so I think I won't touch the prevSetBit/nextSetBit/flip/... tests and just add the tests from {{BaseDcIdSetTestCase}}.

Updated patch. The modification in EliasFanoEncoder is here to always be able to pass {{maxDoc - 1}} as an upper bound even when the set is empty (an assertion would trip otherwise). I think it is ready?","11/Jul/13 12:04;uschindler;Patch looks good. I would also keep the non-DocIdSet testing separate: prevSetBit/nextSetBit/flip/... are not part of the main DocIdSet API, so they should not be in the base test case. But you can still use methods from the base class to check what happend after you did a flip(), so implementing a test for flip() should be easy.","11/Jul/13 21:28;paul.elschot@xs4all.nl;The patch provides independent tests for EliasFanoDocIdSet and does not change the existing TestEliasFanoSequence for the Elias-Fano special cases, great.","12/Jul/13 07:13;jira-bot;Commit 1502448 from [~jpountz]
[ https://svn.apache.org/r1502448 ]

LUCENE-5100: BaseDocIdSetTestCase.","12/Jul/13 07:15;jira-bot;Commit 1502450 from [~jpountz]
[ https://svn.apache.org/r1502450 ]

LUCENE-5100: BaseDocIdSetTestCase (merged from r1502448).","05/Oct/13 10:18;jpountz;4.5 release -> bulk close",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Thus terms are represented ...'  should be 'Thus fields are represented ...',LUCENE-3288,12513117,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Invalid,,pfoster,pfoster,07/Jul/11 10:04,01/Oct/13 18:39,30/Sep/19 08:38,12/Nov/11 22:29,3.1,,,,,,,,,,,,,,general/website,,,0,documentation,,,"In the last paragraph of http://lucene.apache.org/java/3_1_0/fileformats.html#Definitions, second sentance, it says:

       Thus terms are represented as a pair of 
       strings, the first naming the field, and 
       the second naming text within the field. 

Shouldn't it start ""Thus fields are ...""  ?",n/a,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-07-12 12:55:42.859,,,false,,,,,,,,,,,,,,,10718,,,Tue Jul 12 12:55:42 UTC 2011,New,,,,,,,"0|i04ld3:",24726,,,,,,,,,"12/Jul/11 12:55;mikemccand;I think it's correct as-is?  Ie, a term is two strings (field, text).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WAH8DocIdSet: dense sets compression,LUCENE-5150,12660446,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,29/Jul/13 18:23,09/Aug/13 18:30,30/Sep/19 08:38,09/Aug/13 18:14,,,,,,,,,,,,,,,,,,0,,,,"In LUCENE-5101, Paul Elschot mentioned that it would be interesting to be able to encode the inverse set to also compress very dense sets.",,,,,,,,,,,,,,,,"29/Jul/13 18:39;jpountz;LUCENE-5150.patch;https://issues.apache.org/jira/secure/attachment/12594757/LUCENE-5150.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-08 23:51:28.829,,,false,,,,,,,,,,,,,,,340638,,,Fri Aug 09 18:30:39 UTC 2013,New,,,,,,,"0|i1mr0v:",340956,,,,,,,,,"29/Jul/13 18:39;jpountz;Here is a patch. It reserves an additional bit in the header to say whether the encoding should be ""inversed"" (meaning clean words are actually 0xFF instead of 0x00).

It should reduce the amount of memory required to build and store dense sets. In spite of this change, compression ratios remain the same for sparse sets.

For random dense sets, I observed compression ratios of 87% when the load factor is 90% and 20% when the load factor is 99% (vs. 100% before).","08/Aug/13 17:16;jpountz;I'll commit soon if there is no objection. These dense sets can be common in cases where e.g. users are allowed to see everything but something.","08/Aug/13 23:51;rcmuir;Thanks Adrien, i am too curious if its possible for you to re-run http://people.apache.org/~jpountz/doc_id_sets.html

Because now with smaller sets in the dense case, maybe there is no need for wacky heuristics in CachingWrapperFilter and we could just always cache (i am sure some cases would be slower, but if in general its faster...). This would really simplify LUCENE-5101.","09/Aug/13 18:00;jira-bot;Commit 1512422 from [~jpountz] in branch 'dev/trunk'
[ https://svn.apache.org/r1512422 ]

LUCENE-5150: Better compression of dense sets with WAH8DocIdSet.","09/Aug/13 18:01;jira-bot;Commit 1512423 from [~jpountz] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1512423 ]

LUCENE-5150: Better compression of dense sets with WAH8DocIdSet.","09/Aug/13 18:30;jpountz;Robert, I commented on LUCENE-5101 with an updated version of the benchmark.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor SorterTemplate,LUCENE-4946,12643843,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,21/Apr/13 20:46,31/Jul/13 07:51,30/Sep/19 08:38,03/May/13 15:54,,,,,,,,,,,4.4,,,,,,,0,,,,"When working on TimSort (LUCENE-4839), I was a little frustrated of not being able to add galloping support because it would have required to add new primitive operations in addition to compare and swap.

I started working on a prototype that uses inheritance to allow some sorting algorithms to rely on additional primitive operations. You can have a look at https://github.com/jpountz/sorts/tree/master/src/java/net/jpountz/sorts (but beware it is a prototype and still misses proper documentation and good tests).

I think it would offer several advantages:
 - no more need to implement setPivot and comparePivot when using in-place merge sort or insertion sort,
 - the ability to use faster stable sorting algorithms at the cost of some memory overhead (our in-place merge sort is very slow),
 - the ability to implement properly algorithms that are useful on specific datasets but require different primitive operations (such as TimSort for partially-sorted data).

If you are interested in comparing these implementations with Arrays.sort, there is a Benchmark class in src/examples.

What do you think?",,,,,,,,,,,,LUCENE-5140,,,,"03/May/13 12:19;jpountz;LUCENE-4946.patch;https://issues.apache.org/jira/secure/attachment/12581687/LUCENE-4946.patch","02/May/13 21:58;jpountz;LUCENE-4946.patch;https://issues.apache.org/jira/secure/attachment/12581615/LUCENE-4946.patch","02/May/13 21:45;jpountz;LUCENE-4946.patch;https://issues.apache.org/jira/secure/attachment/12581611/LUCENE-4946.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-04-21 21:15:19.985,,,false,,,,,,,,,,,,,,,324210,,,Wed Jul 31 07:51:44 UTC 2013,New,,,,,,,"0|i1jxp3:",324555,,,,,,,,,"21/Apr/13 21:15;dweiss;Looks cool to me!","02/May/13 21:45;jpountz;This patch contains one base class Sorter and 3 implementations:
 * IntroSorter (improved quicksort like we had before but I think the name is better since it makes it clear that the worst case complexity is O(n ln(n)) instead of O(n^2) as with traditional quicksort
 * InPlaceMergeSort, the merge sort we had before.
 * TimSort, an improved version of the previous implementation that can gallop to make sorting even faster on partially-sorted data.

One major difference is that the end offsets are now exclusive. I tend to find it less confusing since you would now call {{sort(0, array.length)}} instead of {{sort(0, array.length - 1)}}.

Please let me know if you would like to review the patch!","02/May/13 21:58;jpountz;Add missing @lucene.internal.","02/May/13 22:55;uschindler;Hi Adrien,
thansk for the refactoring. The history of the SorterTemplate class going back to CGLIB is long and this is a really good idea. Its also useful for other projects, so its maybe a good idea to make a Apache Commons projects out of it :-)

I scanned the patch, looks good. The from...to semantics are better now for the user. I think the original implementation used inclusive end because most implementations on the web were based on this. For me it always looked wrong, but I did not want to change it.

I found some code duplication: To me it looks like ArrayUtil has a private re-implementation of ArrayIntroSorter which is a top-level class in oal.util. Could ArrayUtil not simply use that public impl instead? I know there are 2 implementations with Comparators and without comparators, just an idea! Maybe add a static final singleton NaturalComparator<T extends Comparable<? super T>> that calls compareTo, so we dont need 2 implementations.

I also like that you used timsort at places were the lists are already sorted in the common case (like Automatons).","02/May/13 22:57;uschindler;We should remove the following from NOTICE.txt:

{quote}
The class org.apache.lucene.util.SorterTemplate was inspired by CGLIB's class
with the same name. The implementation part is mainly done using pre-existing
Lucene sorting code. In-place stable mergesort was borrowed from CGLIB,
which is Apache-licensed.
{quote}

The new code has no similarity anymore to the original code - its a complete reimplementation. Only the ""pattern"" stayed alive (you have abstract class, where you have to implement the compare and swap ops).","03/May/13 06:49;dweiss;bq. I think the original implementation used inclusive end because most implementations on the web were based on this. For me it always looked wrong, but I did not want to change it.

I admit I am on the 'inclusive' side of things. To me sort(0..5) means sort elements between indexes 0 and 5, simple. There is also a side-effect of making it exclusive -- you can't sort the full array because an exclusive index on any end would overflow into negative values. I guess it's really a matter of taste in most cases. Perhaps the best way to change it would be to give (startIndex, elementsCount) which still reads (0, array.length) in most cases and does not have the problems mentioned above...","03/May/13 08:55;jpountz;bq. Its also useful for other projects, so its maybe a good idea to make a Apache Commons projects out of it.

Why not. Or maybe use an already existing commons project such as commons collections? I'll dig that...

bq. I found some code duplication

I'll fix that. The reason is that I modified ArrayUtil and CollectionUtil which have their own private Sorter implementations and then I added tests which required me to have concrete implementations in src/test. I'll merge them.

bq. We should remove the following from NOTICE.txt

I'll fix that too.

bq. Perhaps the best way to change it would be to give (startIndex, elementsCount) which still reads (0, array.length) in most cases and does not have the problems mentioned above...

I have no strong opinion about that. I think the reason I like the (from,to) option better is that List.subList and Arrays.copyOfRange have the same arguments. For example someone who wants to sort a sub-list with the JDK would do {{Collections.sort(list.subList(from,to))}}. So I think it'd be nice to make directly translatable to {{new InPlaceMergeSorter() \{ compare/swap \}.sort(from, to)}}.
","03/May/13 12:19;jpountz;New Patch:

 * no more code duplication between ArrayUtil and the test classes

 * ArrayUtil exposes a NATURAL_COMPARATOR to sort arrays based on the natural order (for objects that implement Comparable)

 * Removed references to CGlib in the NOTICE.","03/May/13 12:49;uschindler;+1, looks good.

About the from/to issue: The whole JDK collections API used from and exclusive to, so I agree with Adrien, we should do it in the same way. The overflow issue is no real issue as the meximum array size is limited, too :-) new byte[Integer.MAX_VALUE] does not work on most JDKs.","03/May/13 12:56;dweiss;I still think inclusive ranges are more logical :). For JDK subList and others the argument probably was that specifying inclusive zero-elements range becomes problematic with inclusive values.... so there's always something. I'm not objecting to choosing the ""exclusive"" option either, I'm just saying both options have their pros and cons.","03/May/13 13:32;jpountz;bq. make a Apache Commons projects out of it

I just left an email on their dev@ mailing-list to get their opinion about it: http://markmail.org/message/if5cgarhavzuy45j.","03/May/13 13:37;commit-tag-bot;[trunk commit] jpountz
http://svn.apache.org/viewvc?view=revision&revision=1478785

LUCENE-4946: Refactor SorterTemplate (now Sorter).","03/May/13 14:11;commit-tag-bot;[trunk commit] jpountz
http://svn.apache.org/viewvc?view=revision&revision=1478801

LUCENE-4946: Re-add the random-access checks that have been lost during refactoring.","03/May/13 14:15;commit-tag-bot;[branch_4x commit] jpountz
http://svn.apache.org/viewvc?view=revision&revision=1478802

LUCENE-4946: Refactor SorterTemplate (now Sorter) (merged from r1478785 and r1478801).","23/Jul/13 18:37;sarowe;Bulk close resolved 4.4 issues","31/Jul/13 07:51;jira-bot;Commit 1508757 from [~jpountz] in branch 'dev/trunk'
[ https://svn.apache.org/r1508757 ]

LUCENE-5140: Fixed performance regression of span queries caused by LUCENE-4946.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose PagedGrowableWriter memory usage,LUCENE-5053,12652332,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,12/Jun/13 07:11,23/Jul/13 18:37,30/Sep/19 08:38,12/Jun/13 09:20,,,,,,,,,,,4.4,,,,,,,0,,,,"The idea is to add PagedGrowableWriter.ramBytesUsed, similarly to PackedInts.Mutable and AppendingLongBuffer.",,,,,,,,,,,,,,,,"12/Jun/13 07:13;jpountz;LUCENE-5053.patch;https://issues.apache.org/jira/secure/attachment/12587390/LUCENE-5053.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-12 08:34:27.529,,,false,,,,,,,,,,,,,,,332656,,,Tue Jul 23 18:37:06 UTC 2013,New,,,,,,,"0|i1ldx3:",332985,,,,,,,,,"12/Jun/13 07:13;jpountz;Patch.","12/Jun/13 08:34;commit-tag-bot;[trunk commit] jpountz
http://svn.apache.org/viewvc?view=revision&revision=1492114

LUCENE-5053: Expose PagedGrowableWriter memory usage.","12/Jun/13 09:20;commit-tag-bot;[branch_4x commit] jpountz
http://svn.apache.org/viewvc?view=revision&revision=1492124

LUCENE-5053: Expose PagedGrowableWriter memory usage.","23/Jul/13 18:37;sarowe;Bulk close resolved 4.4 issues",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow GrowableWriter to store negative values,LUCENE-5063,12653385,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,18/Jun/13 06:42,23/Jul/13 18:36,30/Sep/19 08:38,21/Jun/13 13:18,,,,,,,,,,,4.4,,,,,,,0,,,,"For some use-cases, it would be convenient to be able to store negative values in a GrowableWriter, for example to use it in FieldCache: The first term is the minimum value and one could use a GrowableWriter to store deltas between this minimum value and the current value. (The need for negative values comes from the fact that maxValue - minValue might be larger than Long.MAX_VALUE.)",,,,,,,,,,,,,,,,"18/Jun/13 18:08;jpountz;LUCENE-5063.patch;https://issues.apache.org/jira/secure/attachment/12588423/LUCENE-5063.patch","18/Jun/13 07:33;jpountz;LUCENE-5063.patch;https://issues.apache.org/jira/secure/attachment/12588304/LUCENE-5063.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-06-18 10:36:08.443,,,false,,,,,,,,,,,,,,,333663,,,Tue Jul 23 18:36:59 UTC 2013,New,,,,,,,"0|i1lk4n:",333991,,,,,,,,,"18/Jun/13 07:33;jpountz;Here is a patch which makes GrowableWriter able to store negative values and makes FieldCache.DEFAULT.get(Ints|Longs) use it. In order to not make field cache loading too slow, the GrowableWriters are created with an acceptable overhead ratio of 50% so that they can grow the number of bits per value quickly in order not to perform too much resizing.","18/Jun/13 10:36;rcmuir;On one hand we pay the price of an add:
{code}
     @Override
     public long get(int docID) {
-      return values[docID];
+      return minValue + values.get(docID);
     }
{code}

But we get no benefit...
{code}
+ * <p>Beware that this class will accept to set negative values but in order
+ * to do this, it will grow the number of bits per value to 64.
{code}

This doesn't seem right...","18/Jun/13 11:02;rcmuir;i see, so we only need negatives in growablewriter for the case where we'd use 64 bpv for longs anyway.
Can we add a comment?

Also, we start at 4bpv here, but we don't bitpack for byte/short too? it could be a little unintuitive that using long takes less ram than byte :)

Or, maybe FC should only have a 'long' API to better match DV?

{quote}
In order to not make field cache loading too slow, the GrowableWriters are created with an acceptable overhead ratio of 50% so that they can grow the number of bits per value quickly in order not to perform too much resizing.
{quote}

This is consistent with SortedDocValuesImpl, except SortedDocValuesImpl has a 'startBPV' of 1, whereas its 4 here. Maybe we should use 1 here too?","18/Jun/13 12:03;jpountz;bq. i see, so we only need negatives in growablewriter for the case where we'd use 64 bpv for longs anyway.

Exactly. Negative values in a GrowableWriter are more 64-bits unsigned values than actual negative values.

bq. Or, maybe FC should only have a 'long' API to better match DV?

Are you talking about removing all get(Bytes|Shorts|Ints|Floats|Doubles) and only have getLongs which would return a NumericDocValues instance? Indeed I think it would make things simpler and more consistent (eg. comparators and FieldCacheRangeFilter) but this looks like a big change!

bq. This is consistent with SortedDocValuesImpl, except SortedDocValuesImpl has a 'startBPV' of 1, whereas its 4 here. Maybe we should use 1 here too?

Agreed.","18/Jun/13 12:07;rcmuir;{quote}
Indeed I think it would make things simpler and more consistent (eg. comparators and FieldCacheRangeFilter) but this looks like a big change!
{quote}

It doesnt need to hold up this issue. we can make a followup issue for that. Maybe we should do something about the Bytes/Shorts though here...","18/Jun/13 15:03;jpountz;bq. Maybe we should do something about the Bytes/Shorts though here...

Given that we don't even have numeric support (they are just encoded/decoded as strings) for these types, maybe we should just remove or deprecate them?","18/Jun/13 16:23;mikemccand;{quote}
bq. Maybe we should do something about the Bytes/Shorts though here...

Given that we don't even have numeric support (they are just encoded/decoded as strings) for these types, maybe we should just remove or deprecate them?
{quote}

+1","18/Jun/13 16:49;mikemccand;+1, patch looks good!","18/Jun/13 18:08;jpountz;Same patch with added deprecation warnings:
 - FieldCache.get(Byte|Short)s
 - FieldCache.DEFAULT_*_PARSER (because they assume numeric data is encoded as strings)
 - SortField.Type.(Byte|Short)
 - (Byte|Short)FieldSource
 - Solr's ByteField and ShortField","20/Jun/13 19:39;commit-tag-bot;[branch_4x commit] jpountz
http://svn.apache.org/viewvc?view=revision&revision=1495146

LUCENE-5063: ...continuation.","20/Jun/13 20:07;commit-tag-bot;[trunk commit] jpountz
http://svn.apache.org/viewvc?view=revision&revision=1495156

LUCENE-5063: Compress integer and long field caches and remove FieldCache.get(Byte|Short)s, default parsers and related class/methods (merged from r1494753 and r1495146).","23/Jul/13 18:36;sarowe;Bulk close resolved 4.4 issues",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Rename SpatialPrefixTree's ""Node"" back to ""Cell""",LUCENE-4742,12630210,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dsmiley,dsmiley,dsmiley,31/Jan/13 20:35,10/May/13 18:22,30/Sep/19 08:38,10/May/13 18:22,,,,,,,,,,,4.3,,,,modules/spatial,,,0,,,,"SpatialPrefixTree makes ""Node""s which are basically a rectangular spatial region that is more colloquially referred to as a ""Cell"".  It was named ""Cell"" in the first place and for whatever reason, Ryan and/or Chris renamed it as part of extracting it to a top level class from an inner class.  Most comments and variable names still use the ""cell"" terminology.  I'm working on an algorithm that keeps track of a tree of ""nodes"" and it has gotten confusing which kind of node I'm referring to, as each Node has one cell.

In maybe a week or so if there isn't discussion to the contrary, I'm going to commit a rename it back to ""Cell"".  And... while we're on this naming subject, perhaps ""SpatialPrefixTree"" could be named ""SpatialGrid"" ?  FWIW the variables referring to it are always ""grid"".",,,,,,,,,,,,,,,,"27/Mar/13 14:58;dsmiley;LUCENE-4742_Rename_spatial_Node_back_to_Cell.patch;https://issues.apache.org/jira/secure/attachment/12575713/LUCENE-4742_Rename_spatial_Node_back_to_Cell.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-27 15:19:29.691,,,false,,,,,,,,,,,,,,,310706,,,Fri May 10 18:22:29 UTC 2013,New,,,,,,,"0|i1hmdj:",311051,,,,,,,,,"27/Mar/13 14:58;dsmiley;The attached patch renames all ""node"" terminology back to ""cell"" in the API and code, comments, etc.  I think it deserves a mention in backwards-compatibility in CHANGES.txt.

I'll wait until your +1 Ryan.","27/Mar/13 15:19;ryantxu;this is fine -- the term ""node"" came from QuadTree structures.  If it conflicts with something else then use ""Cell""","27/Mar/13 15:28;dsmiley;Wether it's QuadTree or whatever other tree, I don't think matters.  The reason why I think ""Cell"" is much better is that it evokes a conceptual understanding that people grok more easily in the context of spatial, I think, and it's thus colloquially also referred to as a ""cell"".  ""node"" is just too generic.  So using ""cell"" will hopefully be easier for people to understand as they use the API.  Hypothetically one might create a SpatialPrefixTree with a different number of dimensions than 2, and then ""cell"" is less convincing of an ideal name, but nonetheless I think that's okay.","28/Mar/13 14:01;dsmiley;Node->Cell is committed on both branches, including a short note in the API changes section of CHANGES.txt.

I'm not sure wether to take this further and have SpatialPrefixTree be SpatialGrid.  It makes sense for basically the same reason as Node->Cell, but this class is more used and thus will more likely be breaking, and it also triggers other possible renames like the prefix and prefix.tree packages, and ...PrefixTree... classes.  Though it'd be nice to substitute the short 'n sweet ""Grid"" for the longer and compound word ""PrefixTree"".  ","01/Apr/13 16:43;elyograg;I've got the latest branch_4x and trunk pulled up in eclipse, and the Cell class (as well as the old Node class) seems to be missing.  Did you forget to svn add it?
","01/Apr/13 16:52;dsmiley;Um; it's in source control: http://svn.apache.org/repos/asf/lucene/dev/branches/branch_4x/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java
Jenkins would have had a fit if it wasn't.  I'm not sure what is amiss in your local environment.","01/Apr/13 18:45;elyograg;bq.  I'm not sure what is amiss in your local environment.

Sorry for the false alarm.  I'm not sure what went wrong either.  I did completely redo the checkout and 'ant eclipse' followed by a clean/refresh, but it didn't recover properly until I completely removed the project from eclipse, deleted the checkout, and did it all again.

Unfortunately I had deleted everything before I thought to check the filesystem for the .java file rather than the eclipse project view, so I have no way of knowing whether it was the checkout or eclipse that was screwed up.
","10/May/13 18:22;dsmiley;Closing against 4.3 where it Node->Cell done.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More Locale problems in Lucene,LUCENE-1846,12433796,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,uschindler,uschindler,23/Aug/09 09:45,10/May/13 10:44,30/Sep/19 08:38,25/Jan/11 23:10,,,,,,,,,,,2.9,,,,core/other,,,0,,,,"This is a followup to LUCENE-1836: I found some more Locale problems in Lucene with Date Formats. Even for simple date formats only consisting of numbers (like ISO dates), you should always give the US locale. Because the dates in DateTools should sort according to String.compare(), it is important, that the decimal digits are western ones. In some strange locales, this may be different. Whenever you want to format dates for internal formats you exspect to behave somehow, you should at least set the locale to US, which uses ASCII. Dates entered by users and displayed to users, should be formatted according to the default or a custom specified locale.
I also looked for DecimalFormat (especially used for padding numbers), but found no problems.",,,,,,,,,,,,LUCENE-1836,LUCENE-1852,,,"23/Aug/09 09:47;uschindler;LUCENE-1846.patch;https://issues.apache.org/jira/secure/attachment/12417392/LUCENE-1846.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-08-23 14:48:56.602,,,false,,,,,,,,,,,,,,,11917,,,Tue Jan 25 23:08:06 UTC 2011,New,,,,,,,"0|i04ugf:",26199,,,,,,,,,"23/Aug/09 09:47;uschindler;Patch.

The changes in DateTools may affect users with very strange default locales that indexed with prior Lucene versions, but this is unlikely a problem, as the whole sorting may be broken already.

Should I add a note to CHANGES.txt?","23/Aug/09 14:48;rcmuir;Uwe, thanks for bringing this issue up! 

we still have more work to do. Out of curiosity, i looked to see if the old queryparser in core passed under korean locale.
it does not...

{noformat}
setenv ANT_ARGS ""-Dargs=-Duser.language=ko -Duser.country=KR""
ant -Dtestcase=TestQueryParser test
{noformat}
","24/Aug/09 08:31;uschindler;Committed revision: 807117","25/Jan/11 22:45;uschindler;In DateTools is another bug:
The Calendar used is not always the Gregorian one (e.g. if default Locale is Thai). We should also pass Locale.US to the Calendar.getInstance() call.","25/Jan/11 23:08;uschindler;Committed Calendar fix in:
trunk revision 1063501
3.x revision: 1063502
3.0 revision: 1063506
2.9 revision: 1063509",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove some unused code in Surround query parser,LUCENE-2502,12467115,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,paul.elschot@xs4all.nl,paul.elschot@xs4all.nl,16/Jun/10 18:51,10/May/13 10:44,30/Sep/19 08:38,17/Jun/10 09:07,4.0-ALPHA,,,,,,,,,,4.0-ALPHA,,,,modules/other,,,0,,,,,,,,,,,,,,,,,,,,"16/Jun/10 18:53;paul.elschot@xs4all.nl;LUCENE-2502.patch;https://issues.apache.org/jira/secure/attachment/12447259/LUCENE-2502.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-06-17 09:07:36.999,,,false,,,,,,,,,,,,,,,11321,,,Thu Jun 17 09:07:36 UTC 2010,New,Patch Available,,,,,,"0|i04q6v:",25508,,,,,,,,,"16/Jun/10 18:53;paul.elschot@xs4all.nl;Remove getTermsEnum method from SpanNearClauseFactory.
The patch was generated from an svn diff in the contrib directory.","17/Jun/10 09:07;mikemccand;Thanks Paul!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Check if all the packaging/ development tasks work with latest Ant 1.8.x and switch to ant 1.8.x as the ""officially supported"" build platform.",LUCENE-4016,12552164,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,23/Apr/12 17:09,10/May/13 10:44,30/Sep/19 08:38,27/Apr/12 08:02,,,,,,,,,,,4.0-ALPHA,,,,general/build,,,1,,,,"Diff the outputs between ant 1.8.2 and ant 1.7.1.
{noformat}
Target         Windows   Ubuntu    Mac   Jenkins
------------------------------------------------
/
  ivy-bootstrap  OK        OK      OK       ?
  jar-checksums  OK        OK      OK       ?
  validate       OK        OK      OK       ?
  test           OK        OK      OK      OK
lucene/
  prepare-relea* OK        OK      OK       ?
solr/
  prepare-relea* OK        OK      OK       ?
{noformat}

Check consistency with release instructions: http://wiki.apache.org/lucene-java/ReleaseTodo and http://wiki.apache.org/solr/HowToRelease

Differences log:
- ant 1.8.x creates empty package-info.class where ant 1.7.x would fail to do so. This is documented at http://ant.apache.org/manual/Tasks/javac.html and is the expected behavior.
- manifest timestamps are slightly different (Created-By - jvm version is formatted differently, I think more human-friendly in 1.8).
{noformat}
1.7: Created-By: 22.1-b02 (Oracle Corporation)
1.8: Created-By: 1.7.0_03-b05 (Oracle Corporation)
{noformat}",,,,,,,,,,,,,,,,"25/Apr/12 21:25;dweiss;LUCENE-4016.patch;https://issues.apache.org/jira/secure/attachment/12524352/LUCENE-4016.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-24 19:59:18.377,,,false,,,,,,,,,,,,,,,236913,,,Thu Apr 26 23:14:38 UTC 2012,New,,,,,,,"0|i04gwv:",24005,,,,,,,,,"24/Apr/12 19:59;rcmuir;{quote}
ant 1.8.x creates empty package-info.class where ant 1.7.x would fail to do so. This is documented at http://ant.apache.org/manual/Tasks/javac.html and is the expected behavior.
{quote}

What is the effect of this on javadocs? I intentionally added checks to flush out 'secret' javadocs for packages that had no classes, because its a sign they should really be in overview.html or restructured somehow (http://svn.apache.org/viewvc?rev=1328844&view=rev). Will this break that?

I know, its funky how the check works, by *allowing* it (includenosourcepackages=""true""), we cause a javadocs warning
to occur as a side effect (versus silently discarding the documentation), failing the build :)
","24/Apr/12 20:46;dweiss;I only see ""package-info.class"" generated for source files like spatial\src\java\org\apache\lucene\spatial\package-info.java. This is generated with Ant 1.8 but is not with Ant 1.7.

There are no other differences other than timestamps (builds executed at different time).","24/Apr/12 20:52;rcmuir;So that means if i have a package, with only a package.html and no classes... that its package.html is now visible?

I'm just trying to think about how my checker will cope :)","24/Apr/12 20:58;dweiss;We misunderstood each other. These package-info.class files are generated for package-info.java (which are a way to put annotations on a package). There are three such files in Lucene (in spatial).","24/Apr/12 21:41;dweiss;I'll check Ubuntu tomorrow. If anybody wants to give Mac a try, go ahead. I don't think there will be any differences, really -- if it passed without any glitches on my Windows machine I don't expect any problems on other platforms (typically it's windows that causes problems).
","25/Apr/12 18:13;dweiss;I'm planning on switching Ubuntu's symlinks to ant 1.8.2 and let it run for a while. So far nothing's been different (I still don't have a confirmation from a mac, but I bet it's identical there). Report any problems, please.","25/Apr/12 21:25;dweiss;Updating the required ant version to 1.8.2+.","25/Apr/12 21:27;dweiss;Jenkins is running ant 1.8.2. Everything seems to work identically to ant 1.7. I've attached a patch that enforces ant 1.8.2 or later so if you have an older ant, the build will fail for you with a message saying you need to upgrade.

Explanation -- ant 1.8.2 is available by default on macs and 1.8.3 is relatively new so I decided to make 1.8.2 a required version. Everything works fine with 1.8.3 as well so you can use the latest one too.

If there are no objections, I'll commit it shortly.","26/Apr/12 22:04;mikemccand;As far as I can tell, I can't reproduce the output truncation I was seeing before.

Before, it reproduced easily (1.7.1 worked, 1.8.3 truncated)... I never got to the bottom of it.

So I think the recent changes to the test infra must've worked around the problem?

So I'm fine w/ requiring 1.8.x.","26/Apr/12 22:10;dweiss;I'm sure it's switching to junit4 - it's a different runner and different type of buffering. Not everything is smooth (there are issues with jvm debugging options, jvm warnings, etc.) but I'm working on these.","26/Apr/12 23:14;rcmuir;patch looks good. 

Thanks for doing all this testing! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for type whitelist in TypeTokenFilter,LUCENE-3744,12540698,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,teofili,smolav,smolav,01/Feb/12 10:23,10/May/13 10:44,30/Sep/19 08:38,03/Feb/12 10:29,,,,,,,,,,,3.6,4.0-ALPHA,,,modules/analysis,,,0,,,,"A usual use case for TypeTokenFilter is allowing only a set of token types. That is, listing allowed types, instead of filtered ones. I'm attaching a patch to add a useWhitelist option for that.",,,,,,,,,,,,,,,,"03/Feb/12 08:43;teofili;LUCENE-3744_2.patch;https://issues.apache.org/jira/secure/attachment/12513113/LUCENE-3744_2.patch","01/Feb/12 10:24;smolav;TypeTokenFilter-whitelist.patch;https://issues.apache.org/jira/secure/attachment/12512740/TypeTokenFilter-whitelist.patch","02/Feb/12 11:18;smolav;TypeTokenFilter_whitelst_lucene_and_solr.patch;https://issues.apache.org/jira/secure/attachment/12512936/TypeTokenFilter_whitelst_lucene_and_solr.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-02-01 13:33:56.309,,,false,,,,,,,,,,,,,,,226093,,,Fri Feb 03 09:14:49 UTC 2012,New,Patch Available,,,,,,"0|i04ikf:",24273,,,,,,,,,"01/Feb/12 10:24;smolav;Add useWhitelist option to TypeTokenFilter.","01/Feb/12 13:33;teofili;Hello Santiago,
would you mind also providing unit tests for the whitelist usage?
","02/Feb/12 11:18;smolav;Sure. Here's the patch with tests and Solr's factory changes.","03/Feb/12 08:43;teofili;Thanks Santiago, I updated the patch to split the Lucene changes from the Solr changes (will open a new Jira for the Solr factories changes).
","03/Feb/12 09:14;teofili;applied on trunk r1240034
appliend on branch-3.x r1240035",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove @author tags in Lucene/Solr sources,LUCENE-3959,12549788,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,sarowe,sarowe,sarowe,05/Apr/12 20:26,10/May/13 10:44,30/Sep/19 08:38,06/Apr/12 22:37,4.0-ALPHA,,,,,,,,,,4.0-ALPHA,,,,general/javadocs,,,0,,,,"Lucene/Solr sources should not include {{@author}} tags.  See the [solr-dev@l.a.o|http://lucene.markmail.org/thread/bbcbnfdssi3ir5sg] and [java-dev@l.a.o|http://lucene.markmail.org/thread/g6meuxgs34goy373] threads in which this was discussed. 

The Jenkins builds should fail if they are found, in the same way that {{nocommit}}'s are currently handled
",,,,,,,,,,,,LUCENE-974,LUCENE-1378,,,"05/Apr/12 22:48;sarowe;LUCENE-3959.patch;https://issues.apache.org/jira/secure/attachment/12521597/LUCENE-3959.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-05 23:06:24.513,,,false,,,,,,,,,,,,,,,234779,,,Sat Apr 07 01:10:47 UTC 2012,New,Patch Available,,,,,,"0|i04h9j:",24062,,,,,,,,,"05/Apr/12 22:48;sarowe;Patch removing all {{@author}} tags from Lucene/Solr sources.","05/Apr/12 23:06;cmale;+1

Matches the list of files I can find.","06/Apr/12 22:20;sarowe;Previous ""remove all @author tags"" issues in Lucene.","06/Apr/12 22:27;sarowe;Hossman's Solr commit removing @author tags in July 2007: http://svn.apache.org/viewvc?view=revision&revision=555343","06/Apr/12 22:37;sarowe;Committed to trunk.

Modified {{dev/nightly/hudson-settings.sh}} to check for @author tags right after checking for no(n)commits in .java files.","06/Apr/12 23:38;rcmuir;This broke the 3.x hudson task. We should either disable this check for 3.x, or disable the 3.x hudson <-- best!","06/Apr/12 23:42;rcmuir;I don't know how to disable these 3.x tasks in jenkins completely or i would.

my hudson: http://sierranevada.servebeer.com/ is still running for 3.x and i will disable when 3.6
is released. 

so I think its ok to disable the 3.x tasks and dedicate them towards trunk?","07/Apr/12 01:10;sarowe;bq. I don't know how to disable these 3.x tasks in jenkins completely or i would.

I think you got it - thanks!

bq. so I think its ok to disable the 3.x tasks and dedicate them towards trunk?

+1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadoc corrections for IndexWriter,LUCENE-3958,12549749,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,iuliux,iuliux,05/Apr/12 16:41,10/May/13 10:44,30/Sep/19 08:38,07/Apr/12 16:47,3.5,4.0-ALPHA,,,,,,,,,4.0-ALPHA,,,,core/index,general/javadocs,,0,newbie,,,"Linguistic corrections to the javadoc of {{forceMerge()}} in {{org.apache.lucene.index.IndexWriter}}.
Main problem in ""In general, once *the this* completes, the total size...""",,120,120,,0%,120,120,,,,,,,,,"05/Apr/12 16:47;iuliux;LUCENE-3958.patch;https://issues.apache.org/jira/secure/attachment/12521532/LUCENE-3958.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-05 16:51:26.531,,,false,,,,,,,,,,,,,,,234740,,,Sat Apr 07 16:47:56 UTC 2012,Patch Available,,,,,,,"0|i04h9r:",24063,,,,,,,,,"05/Apr/12 16:47;iuliux;Added a patch that solves ""the this"" problem and a few other small things (on the subject of the bug, of course).","05/Apr/12 16:51;rcmuir;Patch looks good. Thanks for improving this!","07/Apr/12 16:47;rcmuir;Thanks Iulius!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Two-stage state expansion for the FST: distance-from-root and child-count criteria.,LUCENE-2933,12499199,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,21/Feb/11 08:49,10/May/13 10:44,30/Sep/19 08:38,23/Feb/11 10:17,,,,,,,,,,,4.0-ALPHA,,,,core/index,,,0,,,,"In the current implementation FST states are expanded into a binary search on labels (from a vector of transitions) when the child count of a state exceeds a given predefined threshold (NUM_ARCS_FIXED_ARRAY). This threshold affects automaton size and traversal speed (as it turns out when benchmarked). For some degenerate (?) data sets, close-to-the-root nodes could have a small number of children (below the threshold) and yet be traversed on every single seek.

A fix of this is to introduce two control thresholds: 
  EXPAND state if (distance-from-root <= MIN_DISTANCE || children-count >= NUM_ARCS_FIXED_ARRAY)

My plan is to create a data set that will prove this first and then to implement the workaround above.",,,,,,,,,,,,,,,,"21/Feb/11 17:49;dweiss;LUCENE-2933.patch;https://issues.apache.org/jira/secure/attachment/12471572/LUCENE-2933.patch","21/Feb/11 17:56;dweiss;out copy.png;https://issues.apache.org/jira/secure/attachment/12471573/out+copy.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-02-21 08:57:06.604,,,false,,,,,,,,,,,,,,,10939,,,Wed Feb 23 10:17:38 UTC 2011,,,,,,,,"0|i04nj3:",25077,,,,,,,,,"21/Feb/11 08:51;dweiss;I can't assign issues to myself, is that all right?","21/Feb/11 08:57;simonw;bq. I can't assign issues to myself, is that all right?
some jira admin needs to enable this. Grant has JIRA admin rights afaik.

simon","21/Feb/11 09:02;uschindler;Dawid: I added you as Committer to JIRA!","21/Feb/11 09:24;dweiss;Thanks Uwe!","21/Feb/11 13:50;dweiss;Implementation of node depth expansion into an array. This patch also piggybacks package-scope getBytes() so that other classes can access the internal representation (specifically, Util.toDot)","21/Feb/11 17:49;dweiss;Patch and tests.","21/Feb/11 17:51;dweiss;Guys, can you peek at this and review it? It's of minimal priority, but I checked and this change:

a) doesn't seem to degenerate FST compilation performance
b) hurt traversal performance on real data (checked against the allterms-20110115),
c) improves worst case behavior on degenerate input sequences.

If approved, I will commit it in.","21/Feb/11 17:56;dweiss;A visual confirmation that this really works (other than JUnit tests).","22/Feb/11 11:43;mikemccand;Patch looks great Dawid!  I like the blue = expanded node dot coloring.

+1 to commit!","23/Feb/11 10:17;dweiss;In trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct copy-paste victim Comment,LUCENE-3815,12543549,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,simonw,iuliux,iuliux,21/Feb/12 19:54,10/May/13 10:44,30/Sep/19 08:38,21/Feb/12 20:12,4.0-ALPHA,,,,,,,,,,4.0-ALPHA,,,,core/codecs,,,0,documentation,newbie,,"Correct the doc-comment of FieldsProducer (being a copy-paste victim of FieldsConsumer).
""consumes"" replaced with ""produces"".

One word change to avoid confusion: safe to commit.
",,60,60,,0%,60,60,,,,,,,,,"21/Feb/12 19:56;iuliux;LUCENE-3815.patch;https://issues.apache.org/jira/secure/attachment/12515479/LUCENE-3815.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-21 20:12:45.043,,,false,,,,,,,,,,,,,,,228788,,,Tue Feb 21 20:12:45 UTC 2012,New,Patch Available,,,,,,"0|i04i4n:",24202,,,,,,,,,"21/Feb/12 20:12;simonw;committed, thanks for this!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add @Deprecated annotations,LUCENE-2185,12444267,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,rcmuir,rcmuir,28/Dec/09 19:42,10/May/13 10:44,30/Sep/19 08:38,15/Jan/10 22:15,,,,,,,,,,,4.0-ALPHA,,,,,,,0,,,,"as discussed on LUCENE-2084, I think we should be consistent about use of @Deprecated annotations if we are to use it.

This patch adds the missing annotations... unfortunately i cannot commit this for some time, because my internet connection does not support heavy committing (it is difficult to even upload a large patch).

So if someone wants to take it, have fun, otherwise in a week or so I will commit it if nobody objects.
",,,,,,,,,,,,,,,,"28/Dec/09 19:52;cmale;LUCENE-2185.patch;https://issues.apache.org/jira/secure/attachment/12429026/LUCENE-2185.patch","15/Jan/10 14:07;rcmuir;LUCENE-2185_flex.patch;https://issues.apache.org/jira/secure/attachment/12430403/LUCENE-2185_flex.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-28 19:52:49.975,,,false,,,,,,,,,,,,,,,11601,,,Fri Jan 15 22:15:39 UTC 2010,New,Patch Available,,,,,,"0|i04scn:",25858,,,,,,,,,"28/Dec/09 19:52;cmale;Attaching patch for Robert.","30/Dec/09 16:17;sarowe;The justification for using @Deprecated, AFAICT, is that conforming compilers are required to issue warnings for each so-annotated class/method, where compilers are *not* required to issue warnings for javadoc @deprecated tags, and although Sun compilers do this, other vendors' compilers might not.

Another (similarly theoretical) argument in favor of using @Deprecated annotations is that, unlike @deprecated javadoc tags, this annotation is available via runtime reflection.

A random information point: MYFACES-2135 removed all @Deprecated annotations from MyFaces code because an apparent bug in the Sun TCK flags methods bearing this annotation as changing method signatures.
","30/Dec/09 16:23;rcmuir;Steven, yeah i know theoretically why this is here, but it is completely stupid.

@Deprecated is a gigantic mistake, requiring me to write it twice, once lowercase in the javadocs, once uppercase as an annotation. This is the only way to properly deprecate something with a message as to why.

and so this additional work buys me nothing, except some theoretical workaround to sun's piss-poor broken language design.

this being said, i still think we should be consistent: either remove all these annotations or add them all, and not just use them 'sometime'.","03/Jan/10 10:34;rcmuir;Committed revision 895342 to trunk.

I added flex branch here, after the next time trunk is merged to it we should do the same there.","15/Jan/10 14:07;rcmuir;patch for flex, also RegexTermsEnum is undeprecated as it was accidentally deprecated in favor of itself.","15/Jan/10 22:15;rcmuir;Committed revision 899831 to flex",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FieldValueFitler should expose the field it uses,LUCENE-3625,12534188,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,simonw,simonw,simonw,07/Dec/11 14:18,10/May/13 10:44,30/Sep/19 08:38,08/Dec/11 13:59,3.6,4.0-ALPHA,,,,,,,,,3.6,4.0-ALPHA,,,,,,0,,,,FieldValueFitler should expose the field it uses. It currently hides this entirely.,,,,,,,,,,,,,,,,"07/Dec/11 14:19;simonw;LUCENE-3625.patch;https://issues.apache.org/jira/secure/attachment/12506445/LUCENE-3625.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,219910,,,2011-12-07 14:18:46.0,New,Patch Available,,,,,,"0|i04jan:",24391,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrap SegmentInfos in public class ,LUCENE-1742,12430308,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,jasonrutherglen,jasonrutherglen,14/Jul/09 00:44,10/May/13 10:44,30/Sep/19 08:38,26/Jun/11 12:23,2.4.1,,,,,,,,,,2.9,,,,core/index,,,0,,,,Wrap SegmentInfos in a public class so that subclasses of MergePolicy do not need to be in the org.apache.lucene.index package.  ,,172800,172800,,0%,172800,172800,,,,,,,,,"18/Jul/09 16:08;mikemccand;LUCENE-1742.patch;https://issues.apache.org/jira/secure/attachment/12413930/LUCENE-1742.patch","17/Jul/09 22:05;jasonrutherglen;LUCENE-1742.patch;https://issues.apache.org/jira/secure/attachment/12413879/LUCENE-1742.patch","16/Jul/09 20:55;jasonrutherglen;LUCENE-1742.patch;https://issues.apache.org/jira/secure/attachment/12413736/LUCENE-1742.patch","14/Jul/09 18:42;jasonrutherglen;LUCENE-1742.patch;https://issues.apache.org/jira/secure/attachment/12413461/LUCENE-1742.patch","14/Jul/09 01:16;jasonrutherglen;LUCENE-1742.patch;https://issues.apache.org/jira/secure/attachment/12413369/LUCENE-1742.patch",,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2009-07-14 09:49:27.513,,,false,,,,,,,,,,,,,,,12017,,,Sun Jul 19 13:31:36 UTC 2009,New,,,,,,,"0|i04v3b:",26302,,,,,,,,,"14/Jul/09 00:59;jasonrutherglen;In order for this class to be compatible with out current
default LogMergePolicy, we'll need to expose readers from the IW
reader pool. This is because presumably classes may need to
access readers such as in
LogMergePolicy.findMergesToExpungeDeletes.","14/Jul/09 01:16;jasonrutherglen;After looking at it, I wasn't sure why we couldn't simply make
the read only methods in SegmentInfo and SegmentInfos (and the
classes) public. 

Maybe this can make it into 2.9?","14/Jul/09 09:49;mikemccand;I agree it'd be good to do this, and this baby step (making the read-only methods public) seems like a good start.

Would we want better names here (Segments/Segment), as Earwin suggested a while back?

Can you also mark these classes as Expert and add the ""subject to change w/o warning between releases"" caveat?","14/Jul/09 18:42;jasonrutherglen;* Added some more javadocs (as suggested). More could be added
to the SegmentInfo and SegmentInfos methods.

* Made SegmentReader public. I think we need to do this as it's
becoming more necessary after LUCENE-1483 where the user may
access individual readers? 

{quote}Would we want better names here (Segments/Segment), as
Earwin suggested a while back?{quote}

Wouldn't this require a bunch of renaming/refactoring? Earwin
what was your suggestion? (couldn't find it)","16/Jul/09 17:34;mikemccand;I don't think we should make IndexWriter's ReaderPool public just yet?  Maybe instead we can add API to query for whether a segment has pending unflushed deletes?  (And fix core merge policies to use that API when deciding how to expungeDeletes).","16/Jul/09 20:55;jasonrutherglen;* Reader pool isn't public anymore

* Left methods of reader as public (could roll back?)

* I'd rather that readerpool be public, however since it's new I
guess we don't want people relying on it?

* All tests pass

* It would be great to get this into 2.9","17/Jul/09 19:47;mikemccand;Latest patch looks good Jason!  I'll commit soon.

bq. since it's new I guess we don't want people relying on it?

Right, and, we haven't thought about / tested for users randomly checking out readers from a writer at different times, it'd make me nervous to expose that now.","17/Jul/09 21:07;jasonrutherglen;LogMergePolicy needs to access the pooled reader's num deletes.  I'll add it, remove IW.hasDeletes, and offer IW.numDeletedDocs(SegmentInfo)","17/Jul/09 22:05;jasonrutherglen;* Added IW.numDeletedDocs method which LogMergePolicy uses to obtain the delCount of an info

* Updated to trunk","18/Jul/09 16:08;mikemccand;Attached patch with tiny changes: made a few more read-only methods public, fixed javadoc warning, one formatting fix, added CHANGES.

I think it's ready to commit.  I'll commit soon...","19/Jul/09 13:31;mikemccand;Thanks Jason!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SegmentReader.numDeletedDocs() sometimes gives an incorrect numDeletedDocs,LUCENE-4080,12558365,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,28/May/12 21:33,10/May/13 10:44,30/Sep/19 08:38,04/Jul/12 10:23,4.0-ALPHA,4.1,,,,,,,,,4.0-ALPHA,,,,core/index,,,0,,,,"At merge time, SegmentReader sometimes gives an incorrect value for numDeletedDocs.

From LUCENE-2357:
bq. As far as I know, [SegmenterReader.numDeletedDocs() is] only unreliable in this context (SegmentReader passed to SegmentMerger for merging); this is because we allow newly marked deleted docs to happen concurrently up until the moment we need to pass the SR instance to the merger (search for ""// Must sync to ensure BufferedDeletesStream"" in IndexWriter.java) ... but it would be nice to fix that, so I think open a new issue (it won't block this one)? We should be able to make a new SR instance, sharing the same core as the current one but using the correct delCount...

bq. It would be cleaner (but I think hairier) to create a new SR for merging that holds the correct delCount, but let's do that under the separate issue.

bq.  it would be best if the SegmentReader's numDeletedDocs were always correct, but, fixing that in IndexWriter is somewhat tricky. Ie, the fix could be hairy but the end result (""SegmentReader.numDeletedDocs can always be trusted"") would be cleaner...",,,,,,,,,,,,,,,,"29/Jun/12 12:20;jpountz;LUCENE-4080.patch;https://issues.apache.org/jira/secure/attachment/12533974/LUCENE-4080.patch","27/Jun/12 21:19;jpountz;LUCENE-4080.patch;https://issues.apache.org/jira/secure/attachment/12533707/LUCENE-4080.patch","27/Jun/12 16:29;jpountz;LUCENE-4080.patch;https://issues.apache.org/jira/secure/attachment/12533670/LUCENE-4080.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-06-27 16:55:44.278,,,false,,,,,,,,,,,,,,,243885,,,Wed Jul 04 10:23:52 UTC 2012,New,,,,,,,"0|i04gi7:",23939,,,,,,,,,"27/Jun/12 16:29;jpountz;Patch. The {{SegmentReader}} returned by {{getMergeReader}} now has a correct {{numDeletedDocuments()}} and {{getLiveDocs()}}. Could someone familiar with Lucene merging internals review this patch?

","27/Jun/12 16:55;rcmuir;I think its cleaner not to have the 'if numDocs >= 0' in SegmentReader ctor#2
Instead i think ctor #1 should just forward docCount - delCount like ctor#3","27/Jun/12 17:15;rcmuir;Also is it ok in mergeMiddle that we call rld.getMergeReader inside the sync?

Previously, we never did actual i/o here...","27/Jun/12 21:19;jpountz;New patch.

Only the {{liveDocs/numDeletedDocs}} copy needs to be protected by the {{IndexWriter}} lock. However, the whole method needs to be protected by the ReadersAndLiveDocs lock but we can't nest the former into the latter since other pieces of code do the opposite (potential deadlock). So I replaced the {{ReadersAndLiveDocs}} lock with a {{ReentrantLock}} so that it can overlap with the {{IndexWriter}} lock. Does it look better?","28/Jun/12 11:05;mikemccand;I only looked briefly at the patch ... but: could we do the liveDocs/numDeletedDocs copy up above, in IW (syncd), and pass them to RLD.getMergeReader?  Then we don't need to cutover to ReentrantLock?","28/Jun/12 11:14;rcmuir;I don't understand the concurrency here but thats what I read from the issue description: my concern was just that its not obvious in the first patch if we are actually just opening an SR with an existing core inside this sync or not, and I dont even know if its a problem :)","28/Jun/12 11:33;jpountz;@Robert I think this is an issue since one must hold the IndexWriter lock to perform deletes on any segment (cf. assert Thread.holdsLock(writer); in ReadersAndLiveDocs.delete).

@Michael Oh right, I think I understand what you mean. I'll try to produce a better patch.","29/Jun/12 12:20;jpountz;New patch (simpler than the previous ones). This time, I only removed {{MergeState.IndexReaderAndLiveDocs}} and modified {{IndexWriter.mergeMiddle}} so that it fixes the merge reader in case its del count doesn't match the {{ReadersAndLiveDocs}} del count. There shouldn't be more contention as with the current trunk version.","03/Jul/12 23:30;mikemccand;New patch looks great!  I like this solution: much simpler.  Maybe just add a comment explaining why we must sometimes make a new reader?  (Ie, that deletes could have snuck in after we pulled the merge reader but before the sync block where we get the live docs)?

It's nice to simply pass around the reader and not the pair of liveDocs + reader...

We can remove liveDocs and delCount args to SegmentMerger.add now?

+1, thanks Adrien!","04/Jul/12 10:23;jpountz;Thanks for your feedback, Mike. I just committed (r1357195 on trunk and r1357212 on branch 4.x).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a target to recalculate SHA1 checksums for JAR,LUCENE-3984,12550938,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,14/Apr/12 10:43,10/May/13 10:44,30/Sep/19 08:38,14/Apr/12 19:23,,,,,,,,,,,4.0-ALPHA,,,,,,,0,,,,"Something like this. Either top-level or common-build.xml?
{noformat}
  <target name=""refresh-checksums"">
    <checksum algorithm=""SHA1"">
      <fileset dir=""${basedir}"">
        <include name=""**/*.jar""/>
      </fileset>
    </checksum>
  </target>
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-04-14 18:45:19.647,,,false,,,,,,,,,,,,,,,235802,,,Sat Apr 14 18:45:19 UTC 2012,New,,,,,,,"0|i04h3z:",24037,,,,,,,,,"14/Apr/12 18:32;dweiss;Can I commit this in as a top-level target? It shouldn't matter for svn/git files that don't change (their timestamps will but contents will not) and it helps folks on Windows who can't use Hoss's magic bash pipe (doesn't this sound wrong somehow?).","14/Apr/12 18:45;rcmuir;Please do, i dont remember what my magic bash pipe was (as i backported hossman's changes to 3.x), but it was something aweful.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve readability of StandardTermsDictWriter,LUCENE-2509,12467560,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,paul.elschot@xs4all.nl,paul.elschot@xs4all.nl,22/Jun/10 11:04,10/May/13 10:44,30/Sep/19 08:38,22/Jun/10 14:35,,,,,,,,,,,4.0-ALPHA,,,,core/index,,,0,,,,"One variable is named indexWriter, but it is a termsIndexWriter. Also some layout.",,,,,,,,,,,,,,,,"22/Jun/10 11:08;paul.elschot@xs4all.nl;LUCENE-2509.patch;https://issues.apache.org/jira/secure/attachment/12447692/LUCENE-2509.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-06-22 14:34:30.037,,,false,,,,,,,,,,,,,,,11315,,,Tue Jun 22 14:34:30 UTC 2010,New,Patch Available,,,,,,"0|i04q5b:",25501,,,,,,,,,"22/Jun/10 11:08;paul.elschot@xs4all.nl;Not really sure about the layout of the parameter lists...","22/Jun/10 14:34;mikemccand;Looks good; thanks Paul.  I'll commit shortly...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ant clean does not remove maven artifacts,LUCENE-3858,12545765,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,gbowyer@fastmail.co.uk,gbowyer@fastmail.co.uk,09/Mar/12 03:13,10/May/13 10:44,30/Sep/19 08:38,09/Mar/12 14:42,3.6,4.0-ALPHA,,,,,,,,,3.6,4.0-ALPHA,,,,,,0,build,,,"Hi all, whilst I am aware that maven support is best effort, there is a small issue with the ant clean target in that it will not delete maven artefacts from the dist folder

Patch attached to fix this.",,,,,,,,,,,,,,,,"09/Mar/12 03:14;gbowyer@fastmail.co.uk;LUCENE-3858.patch;https://issues.apache.org/jira/secure/attachment/12517672/LUCENE-3858.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-09 14:42:57.759,,,false,,,,,,,,,,,,,,,230948,,,Fri Mar 09 14:42:57 UTC 2012,New,Patch Available,,,,,,"0|i04hvj:",24161,,,,,,,,,"09/Mar/12 14:42;rcmuir;Thanks Greg!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExtendableQueryParser should allow extensions to access the toplevel parser settings/ properties,LUCENE-2162,12443361,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,simonw,simonw,simonw,15/Dec/09 10:50,10/May/13 10:43,30/Sep/19 08:38,28/Dec/09 14:27,,,,,,,,,,,4.0-ALPHA,,,,modules/other,,,0,,,,Based on the latest discussions in LUCENE-2039 this issue will expose the toplevel parser via the ExtensionQuery so that ExtensionParsers can access properties like getAllowLeadingWildcards() from the top level parser.,,,,,,,,,,,,,,,,"15/Dec/09 11:10;simonw;LUCENE-2162.patch;https://issues.apache.org/jira/secure/attachment/12428029/LUCENE-2162.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,11623,,,Mon Dec 28 14:27:55 UTC 2009,New,Patch Available,,,,,,"0|i04shr:",25881,,,,,,,,,"15/Dec/09 11:10;simonw;attached patch","24/Dec/09 11:52;simonw;I plan to commit this until Dec. 28. If nobody objects.","28/Dec/09 14:27;simonw;committed in revision 894180",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exception consistency in o.a.l.store,LUCENE-3680,12537643,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,rcmuir,rcmuir,07/Jan/12 17:34,10/May/13 10:43,30/Sep/19 08:38,09/Mar/12 18:36,,,,,,,,,,,3.6,4.0-ALPHA,,,,,,0,,,,"just some minor improvements:
* always use EOFException when its eof
* always include the inputstream too so we know filename etc
* use FileNotFoundException consistently in CFS when a sub-file is not found
",,,,,,,,,,,,,,,,"07/Jan/12 17:34;rcmuir;LUCENE-3680.patch;https://issues.apache.org/jira/secure/attachment/12509796/LUCENE-3680.patch","07/Jan/12 21:26;rcmuir;LUCENE-3680_more.patch;https://issues.apache.org/jira/secure/attachment/12509801/LUCENE-3680_more.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-01-07 17:46:49.692,,,false,,,,,,,,,,,,,,,223149,,,Tue Mar 06 02:40:14 UTC 2012,New,,,,,,,"0|i04iyf:",24336,,,,,,,,,"07/Jan/12 17:46;uschindler;+1 Thanks for taking care!","07/Jan/12 17:55;mikemccand;+1","07/Jan/12 21:07;rcmuir;I committed the patch (backporting), but i found a few more problems relating to
inconsistent use of exceptions when accessing a closed resource.

here are some from 3.x's compoundfiledirectory:
{noformat}
  public synchronized void close() throws IOException {
    if (stream == null)
      throw new IOException(""Already closed"");
  ...
  public synchronized IndexInput openInput(String id, int readBufferSize) throws IOException {
    if (stream == null)
      throw new IOException(""Stream closed"");
{noformat}

I think the close() one is wrong since it impls closeable, but in the other case we should use AlreadyClosedException (like other directories).
Ill look around for more of these and create another patch.","07/Jan/12 21:22;uschindler;I agree, Closeable.close() is allowed to be closed multiple times, while additional calls must have no effect. So the first throws is wrong, should be simple return.","07/Jan/12 21:26;rcmuir;patch with more consistency fixes.

I put TODO's in the lockfactory exceptions where a directory is a regular file (not sure if we should be using NoSuchDirectoryException here?)","07/Jan/12 21:54;uschindler;Cool +1

We should better check all Closeables...","06/Mar/12 02:40;rcmuir;I think this patch is safe, I'll bring it up to speed and commit it tomorrow.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Idea modules settings - verify and fix,LUCENE-3737,12540536,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,sarowe,doronc,doronc,31/Jan/12 11:41,10/May/13 10:43,30/Sep/19 08:38,31/Jan/12 15:47,4.0-ALPHA,,,,,,,,,,4.0-ALPHA,,,,,,,0,,,,"Idea's settings for modules/queries and modules/queryparser refer to lucene/contrib instead of modules.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-01-31 11:52:29.755,,,false,,,,,,,,,,,,,,,225949,,,Tue Jan 31 16:49:25 UTC 2012,New,,,,,,,"0|i04ilr:",24279,,,,,,,,,"31/Jan/12 11:52;cmale;I can't immediately see where it makes reference to contrib?","31/Jan/12 12:06;doronc;In dev-tools/idea/.idea/ant.xml there are these two:

{code}
<buildFile url=""file://$PROJECT_DIR$/lucene/contrib/queries/build.xml"" />
<buildFile url=""file://$PROJECT_DIR$/lucene/contrib/queryparser/build.xml"" />
{code}

I assume this has the potential to break an Idea setup, but haven't tried it yet, just wanted to not forget about it, therefore this issue. Is this a none-issue?
","31/Jan/12 12:10;cmale;Oh I'm with you now.

Yup, it's an issue.","31/Jan/12 15:41;sarowe;Also, {{xml-query-parser}} contrib needs to be removed from {{.idea/ant.xml}}, and {{sandbox}} contrib needs to be added.","31/Jan/12 15:47;sarowe;Committed fix to trunk.  branch_3x's version doesn't need anything fixed, AFAICT.","31/Jan/12 16:49;doronc;Yes, only saw this on trunk, thanks for taking care of this!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The Field ctors that take byte[] shouldn't take Store, since it must be YES",LUCENE-2177,12443906,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,21/Dec/09 10:17,10/May/13 10:43,30/Sep/19 08:38,22/Dec/09 14:26,,,,,,,,,,,4.0-ALPHA,,,,core/other,,,0,,,,"API silliness.  Makes you think you can set Store.NO for binary fields.  This used to be meaningful when we also accepted COMPRESS, but now it's an orphan.",,,,,,,,,,,,,,,,"21/Dec/09 22:59;mikemccand;LUCENE-2177.patch;https://issues.apache.org/jira/secure/attachment/12428665/LUCENE-2177.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,11609,,,2009-12-21 10:17:32.0,New,,,,,,,"0|i04sef:",25866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analysis for Irish,LUCENE-3883,12547034,,New Feature,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,jimregan,jimregan,19/Mar/12 16:12,10/May/13 10:43,30/Sep/19 08:38,24/Mar/12 16:22,,,,,,,,,,,3.6,4.0-ALPHA,,,modules/analysis,,,0,analysis,newbie,,"Adds analysis for Irish.

The stemmer is generated from a snowball stemmer. I've sent it to Martin Porter, who says it will be added during the week.",,,,,,,,,,,,,,,,"22/Mar/12 14:27;rcmuir;LUCENE-3883.patch;https://issues.apache.org/jira/secure/attachment/12519441/LUCENE-3883.patch","20/Mar/12 04:28;rcmuir;LUCENE-3883.patch;https://issues.apache.org/jira/secure/attachment/12519019/LUCENE-3883.patch","19/Mar/12 17:20;jimregan;LUCENE-3883.patch;https://issues.apache.org/jira/secure/attachment/12518913/LUCENE-3883.patch","19/Mar/12 16:38;jimregan;irish.sbl;https://issues.apache.org/jira/secure/attachment/12518912/irish.sbl",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-03-19 16:22:08.905,,,false,,,,,,,,,,,,,,,232192,,,Wed May 09 21:32:02 UTC 2012,New,Patch Available,,,,,,"0|i04hpz:",24136,,,,,,,,,"19/Mar/12 16:13;jimregan;Patch adding Irish analysis","19/Mar/12 16:17;jimregan;Patch, redone from top level of svn.","19/Mar/12 16:22;rcmuir;Thanks Jim! This looks really nicely done... 

Out of curiousity could you share your snowball rules (the .sbl) with us?
","19/Mar/12 16:33;uschindler;Hi,

very funny lowercase filter! :-) One thing: It does not actually ArrayIndexOutOfBoundsEx in the filter because of the way how CharTermAttributeImpl is implemented internally, but theoretically there is a length check missing. The nUpper/tUpper stuff can get out of bounds if the length of term in 0 or 1 (which are valid length). But thats only a minor complaint about the code. Otherwise looks great. Just appearing from no irish support at all! really needed! :-)

Uwe","19/Mar/12 16:36;rcmuir;By the way I created LUCENE-3884 to move the ElisionFilter out of the french package
into a more general .util package. That doesnt need to hold up this issue: it just
reminded me we should move it because its not really french-specific.
","19/Mar/12 16:38;jimregan;Irish snowball script","19/Mar/12 16:41;jimregan;Yeah, it's quite an odd thing (Scots Gaelic has a similar phenomenon, but they consistently keep the hyphen), but it does help with the stemmer in those cases to know that the t or n at the start of the word is due only to mutation.","19/Mar/12 16:56;jimregan;I'm not sure if I actually needed to use the ElisionFilter, because the stemmer handles those - because of the initial mutation in Irish, trimming the start of the word is more important than trimming the end. I was copying the Catalan analyser, and using ElisionFilter seemed like The Thing To Do.","19/Mar/12 17:20;jimregan;New version of patch, also checking that chLen (array length) > 1","20/Mar/12 03:26;rcmuir;Thanks for updating the patch Jim!

one concern doing some very very rudimentary testing:

we have special lowercasing for situations like nAthair -> n-athair,

which the snowball rules then strip:

{noformat}
define initial_morph as (
  [substring] among (
    'h-' 'n-' 't-' //nAthair -> n-athair, but alone are problematic
    (delete)
{noformat}

The problem is if the input initially comes as n-athair, Unicode break rules
will split this up on the hyphen into two tokens {n, athair}. You can visualize this at http://unicode.org/cldr/utility/breaks.jsp

This means we can add many spurious 'n' tokens in the index...

So we have two potential solutions to this:
# we can simply add 'n', 'h', 't', etc to the stopwords list. This is the simplest solution. Would this be too aggressive?
# we can add a CharFilter for IrishAnalyzer to prevent this splitting from happening. This is more complex.
","20/Mar/12 03:38;rcmuir;Hmm another downside of #1 is that with a simple stopfilter approach, position increments won't line up
if we have a phrase query of ""n-athair"" with indexed nAthair.

So I start to lean towards #2 since it would be a better solution... but I'm going to think about it
and see if I come up with any other ideas.

Separately, what about h- when succeeded by a vowel? Is there actually usually a hyphen here?
(Wikipedia says no, playing around with GaelSpell seems to agree, but I don't know anything about this language!)
Would this case be too aggressive to handle?","20/Mar/12 04:13;rcmuir;To make matters worse: this exact example of splitting on hyphen for this Irish case is 
actually mentioned on http://en.wikipedia.org/wiki/Hyphen#In_computing

From there it seems like the right thing to do is heuristically convert to 
U+2011 (non-breaking hyphen) but this only affects Unicode line-break rules,
not word break rules :(

So it seems like the least hackish workaround would be for a charfilter to 
convert n-athair -> nAthair (to prevent the tokenizer from splitting it up),
since the IrishLowerCaseFilter will convert it back and stem it anyway.

I'll see if i can hack something up.","20/Mar/12 04:28;rcmuir;updated patch, with a simple solution to the hyphen-phrasequery-problem:

I added a special stopset just for these:
{code}
  /**
   * When StandardTokenizer splits t‑athair into {t, athair}, we don't
   * want to cause a position increment, otherwise there will be problems
   * with phrase queries versus tAthair (which would not have a gap).
   */
  private static final CharArraySet HYPHENATIONS = CharArraySet.unmodifiableSet(
      new CharArraySet(Version.LUCENE_CURRENT,
          Arrays.asList(
              ""h"", ""n"", ""t""
          ), true));
{code}

This is used with enablePositionIncrements=false to ensure no gap is added... I also added a simple test for this.","20/Mar/12 15:01;jimregan;Wow! Thanks Robert!

There isn't usually a hyphen with 'h' before a vowel, but I've started to see it recently -- there are no native Irish words beginning with 'h', so it used to be relatively unambiguous that a 'h' was a mutation, but with an increase of scientific literature in Irish, there are more Greek and Latin loan words being added which do begin with 'h', so it's no longer clear.","20/Mar/12 15:07;rcmuir;Thanks Jim. Personally I think this patch is ready to be committed. 

I'm just going to wait a bit in case you get any feedback from Martin or other snowball developers,
but I won't wait too long :) ","20/Mar/12 17:31;jimregan;Great :)

Regarding the initial 'h', I asked Kevin Scannell (among other feathers in his cap, he created the dictionary used in GaelSpell, and ran an Irish-language search engine), who said: 
""I looked carefully at how often initial h is a prefix vs not a while ago.  I can send you those data - non-prefixes might be more common than you'd think in running text bc of proper names, English mixed in, etc.  So upshot is it's a bad idea to strip all initial h's with no hyphen following. 
  As far as h- (with hyphen) goes, it's non-standard but common enough that I'd leave it in the stemmer.   Not like there would be false positives in that case if the hyphen is there.'","20/Mar/12 17:33;rcmuir;This makes sense to me, I agree with the conservative approach here!
","20/Mar/12 17:53;dsmiley;How ironic this issue is created nearly on St. Patrick's Day.","20/Mar/12 18:01;jimregan;It was on my mind, a little :) I made the stemmer on the 15th, on the 17th I made ICU transliteration rules for Irish->IPA, but that's not quite relevant here.","22/Mar/12 14:27;rcmuir;Same patch but with the solr pieces too (factory/test for the lowercasefilter, text_ga fieldtype, resources synced, etc).
","24/Mar/12 16:22;rcmuir;Thank you very much Jim! I just committed this.
","24/Mar/12 16:38;jimregan;Yay! Thanks for all your help!","09/May/12 20:42;jimregan;Just to follow up, the Irish stemmer is now available from the Snowball site: http://snowball.tartarus.org/otherapps/oregan/intro.html","09/May/12 20:47;rcmuir;Thanks Jim! I already removed our local copy of the irish.sbl as its now available on
the snowball site.

I have to investigate the Czech implementation, I think we should make it available
as well, since it also supports stemming of derivational endings: Dawid opened
LUCENE-4042 for that.

Thanks for contributing these to snowball.","09/May/12 21:32;jimregan;I wouldn't recommend the aggressive mode, and I regret that I left it uncommented. If you really think an alternative would be welcome, it would be quite easy to get the best of both (in fact, I spent roughly half the time on that trying to beat Snowball into overstemming to match the original).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MergeThread throws unchecked exceptions from toString(),LUCENE-3835,12544590,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,29/Feb/12 11:16,10/May/13 10:43,30/Sep/19 08:38,29/Feb/12 11:55,,,,,,,,,,,3.6,4.0-ALPHA,,,,,,0,,,,"This causes nearly all thread-dumping routines to fail and in the effect obscure the original problem. I think this
should return a string (always), possibly indicating the underlying writer has been closed or something.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-02-29 11:29:06.392,,,false,,,,,,,,,,,,,,,229776,,,Wed Feb 29 11:29:06 UTC 2012,New,,,,,,,"0|i04i07:",24182,,,,,,,,,"29/Feb/12 11:29;mikemccand;+1

toString throwing exceptions is frustrating :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Alternative depth-based DOT layout ordering in FST's Utils,LUCENE-2934,12499224,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,21/Feb/11 12:15,10/May/13 10:43,30/Sep/19 08:38,21/Feb/11 14:26,,,,,,,,,,,4.0-ALPHA,,,,core/other,,,0,,,,"Utils.toDot() dumps GraphViz's DOT file, but it can be quite difficult to read. This patch provides an alternative layout that is probably a little bit easier on the eyes (well, as far as larger FSTs can be ;)",,,,,,,,,,,,,,,,"21/Feb/11 14:22;dweiss;LUCENE-2934.patch;https://issues.apache.org/jira/secure/attachment/12471561/LUCENE-2934.patch","21/Feb/11 12:24;dweiss;graph-after.png;https://issues.apache.org/jira/secure/attachment/12471552/graph-after.png",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-02-21 12:29:31.399,,,false,,,,,,,,,,,,,,,10938,,,Mon Feb 21 14:26:36 UTC 2011,,,,,,,,"0|i04niv:",25076,,,,,,,,,"21/Feb/11 12:24;dweiss;Visual difference between the layered and free ordered dot file.","21/Feb/11 12:26;dweiss;Patch for this. If there are no objections, I'll commit it in.","21/Feb/11 12:29;uschindler;Backwards compatibility is in my opinion not needed, the FST classes are new in 4.0.","21/Feb/11 12:34;uschindler;Additionally, PrintStream is a legacy class from Java 1.0. If DOT is specified to be US-ASCII, it should use a PrintWriter that is instantiated using US-ASCII as charset (PrintStream uses platform default, and that may be Big5 or other ugly things). In my opinion, the method should take a PrintWriter and a possible File/OutputStream method should be hardcoded to US-ASCII.","21/Feb/11 12:35;dweiss;Point taken. I would say both ways are useful, but for different aspects. I wish we had default parameter values, but since we don't wouldn't it be sensible to keep both methods (toDot(FST, PrintStream) with sane-defaults)?","21/Feb/11 12:50;dweiss;I admit I didn't want to take the liberty of changing too much since this is Mike's sacred Utils place ;) I agree that these methods should accept a character stream (I'd even say a Writer because we don't need the wrapping PrintWriter that much), so if Mike doesn't mind, I'll change it to be so.","21/Feb/11 13:25;mikemccand;bq. I admit I didn't want to take the liberty of changing too much since this is Mike's sacred Utils place

Nobody owns any code in Apache ;)  There are no names attached.  We all should always feel free to fix anything!  Especially, newly created code should get lots of attention from others; the more the better.

This better dot formatting looks awesome!!  Keeping both options open seems good?  (Though, I lack intuition on when the non-layered (old way) option would be better... maybe we default to the new way?).","21/Feb/11 13:52;dweiss;I know the principles of collective code ownership, but still -- that function was/is primarly so that you can do the debugging of FST code, so I didn't want to interfere. As for the DOT: layered representation may cause arcs to overlap and be virtually impossible to read, so both reps. are useful, it just depends on the automaton. I'll correct it to Writer and commit in.","21/Feb/11 14:26;dweiss;Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Rewriteable Support to SortField.toString,LUCENE-3227,12511189,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,cmale,cmale,cmale,22/Jun/11 01:51,10/May/13 10:43,30/Sep/19 08:38,22/Jun/11 02:12,,,,,,,,,,,4.0-ALPHA,,,,core/search,,,0,,,,I missed adding support for the new Rewriteable SortField type to toString().,,,,,,,,,,,,,,,,"22/Jun/11 02:11;cmale;LUCENE-3227.patch;https://issues.apache.org/jira/secure/attachment/12483396/LUCENE-3227.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,10771,,,Wed Jun 22 02:12:03 UTC 2011,New,,,,,,,"0|i04lqv:",24788,,,,,,,,,"22/Jun/11 02:11;cmale;Patch adding support.  I will now commit.","22/Jun/11 02:12;cmale;Committed revision 1138281.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Overrides of @Before or @After hooks should cause a test error.,LUCENE-4054,12555061,12542552,Sub-task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,13/May/12 08:16,10/May/13 10:43,30/Sep/19 08:38,07/Jun/12 12:45,,,,,,,,,,,4.0-ALPHA,,,,general/build,general/test,,0,,,,"The only exceptions are setUp/ tearDown methods in LuceneTestCase, which are historically designed for overriding (and for which chaining is verified). Any other override of am instance hook method should just be banned as it is very likely a mistake.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-18 15:29:53.768,,,false,,,,,,,,,,,,,,,239319,,,Thu Jun 07 12:45:12 UTC 2012,New,,,,,,,"0|i04gnz:",23965,,,,,,,,,"16/May/12 21:18;dweiss;The tests for this should not fire when run in stand-alone mode because this confuses intellij idea. I think a safe bet would be to check if we're a nested class -- if so, don't run any validations.","18/May/12 07:53;dweiss;Committed a fix for this.","18/May/12 15:29;sarowe;Thanks Dawid - IntelliJ now shows an icon indicating the affected test is ignored, and no exception is printed.","18/May/12 15:46;dweiss;Yup, this time I checked ;) Thanks for testing.","07/Jun/12 12:45;dweiss;Fixed a while ago.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecated API called in o.a.l.store Directories,LUCENE-2180,12444202,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,simonw,simonw,26/Dec/09 02:17,10/May/13 10:43,30/Sep/19 08:38,26/Dec/09 10:44,,,,,,,,,,,4.0-ALPHA,,,,core/store,,,0,,,,just ran into NIOFSDirectory and others still call getFile instead of getDirectory,,,,,,,,,,,,,,,,"26/Dec/09 02:18;simonw;LUCENE-2180.patch;https://issues.apache.org/jira/secure/attachment/12428968/LUCENE-2180.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-26 03:47:36.74,,,false,,,,,,,,,,,,,,,11606,,,Sat Dec 26 03:47:36 UTC 2009,New,Patch Available,,,,,,"0|i04sdr:",25863,,,,,,,,,"26/Dec/09 02:18;simonw;here is a patch","26/Dec/09 03:47;mikemccand;Thanks Simon!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"don't download/extract 20,000 files when doing the build",LUCENE-2269,12456611,,Test,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,rcmuir,rcmuir,18/Feb/10 03:27,10/May/13 10:43,30/Sep/19 08:38,21/Feb/10 11:41,,,,,,,,,,,4.0-ALPHA,,,,general/build,,,0,,,,"When you build lucene, it downloads and extracts some data for contrib/benchmark, especially the 20,000+ files for the reuters corpus.
this is only needed for one test, and these 20,000 files drive IDEs and such crazy.
instead of doing this by default, we should only download/extract data if you specifically ask (like wikipedia, collation do, etc)

for the qualityrun test, instead use a linedoc formatted 587-line text file, similar to reuters.first20.lines.txt already used by benchmark.
",,,,,,,,,,,,,,,,"21/Feb/10 11:22;rcmuir;LUCENE-2269.patch;https://issues.apache.org/jira/secure/attachment/12436485/LUCENE-2269.patch","18/Feb/10 03:28;rcmuir;LUCENE-2269.patch;https://issues.apache.org/jira/secure/attachment/12436168/LUCENE-2269.patch","18/Feb/10 03:30;rcmuir;reuters.578.lines.zip;https://issues.apache.org/jira/secure/attachment/12436169/reuters.578.lines.zip",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-02-21 10:48:21.118,,,false,,,,,,,,,,,,,,,11520,,,Sun Feb 21 11:41:56 UTC 2010,New,Patch Available,,,,,,"0|i04rtz:",25774,,,,,,,,,"18/Feb/10 03:28;rcmuir;patch","18/Feb/10 03:30;rcmuir;data file that goes in contrib/benchmark/src/test/o/a/l/benchmark/quality
the test is setup to use it in zipped form.","21/Feb/10 07:34;rcmuir;if no one objects, will commit this tomorrow, to speed up the build and increase the lifespan of our hard drives.","21/Feb/10 10:48;mikemccand;Patch looks great!  This speeds up the test from 16.8 sec -> 1.5 sec for me.

Only thing is, I think you don't have to unzip yourself -- benchmark can decompress .bz2 itself on the fly.","21/Feb/10 11:22;rcmuir;great idea Mike, I removed all unzipping code and changed the file to the smaller bz2, which is handled automagically by benchmark.

i also added a note about this test for the future:

{noformat}
NOTE: if the default scoring or StandardAnalyzer is changed, then
this test will no work correctly, as it does not dynamically
generate its test trec topics/qrels!
{noformat}

this is nothing new, but in my opinion an improvement in the future would be to dynamically generate these files, it would also test the QualityQueriesFinder functionality, but we would need to add the 'fake documents', etc for the test to work, too.

will commit shortly","21/Feb/10 11:41;rcmuir;Committed revision 912333.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use linear probing with an additional good bit avalanching function in FST's NodeHash.,LUCENE-2967,12501458,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,dweiss,dweiss,dweiss,15/Mar/11 13:34,10/May/13 10:43,30/Sep/19 08:38,23/Mar/11 13:09,,,,,,,,,,,4.0-ALPHA,,,,,,,0,,,,"I recently had an interesting discussion with Sebastiano Vigna (fastutil), who suggested that linear probing, given a hash mixing function with good avalanche properties, is a way better method of constructing lookups in associative arrays compared to quadratic probing. Indeed, with linear probing you can implement removals from a hash map without removed slot markers and linear probing has nice properties with respect to modern CPUs (caches). I've reimplemented HPPC's hash maps to use linear probing and we observed a nice speedup (the same applies for fastutils of course).

This patch changes NodeHash's implementation to use linear probing. The code is a bit simpler (I think :). I also moved the load factor to a constant -- 0.5 seems like a generous load factor, especially if we allow large FSTs to be built. I don't see any significant speedup in constructing large automata, but there is no slowdown either (I checked on one machine only for now, but will verify on other machines too).",,,,,,,,,,,,,,,,"15/Mar/11 13:35;dweiss;LUCENE-2967.patch;https://issues.apache.org/jira/secure/attachment/12473674/LUCENE-2967.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-15 17:24:04.961,,,false,,,,,,,,,,,,,,,10911,,,Wed Mar 23 13:09:27 UTC 2011,,,,,,,,"0|i04nbj:",25043,,,,,,,,,"15/Mar/11 13:35;dweiss;Linear probing in NodeHash.","15/Mar/11 17:24;mikemccand;Hmm, unfortunately, I'm seeing the patch make FST building slower, at
least in my env/test set.  I built FST for the 38M wikipedia terms.

I ran 6 times each, alternating trunk & patch.

I also turned off saving the FST, and ran -noverify, so I'm only
measuring time to build it.  I run java -Xmx2g -Xms2g -Xbatch, and
measure wall clock time.

Times on trunk (seconds):

{noformat}
  43.795
  43.493
  44.343
  44.045
  43.645
  43.846
{noformat}

Times w/ patch:

{noformat}
  46.595
  47.751
  47.901
  47.901
  47.901
  47.700
{noformat}

We could also try less generous load factors...
","15/Mar/11 20:41;dweiss;Yes, now I see this difference on the 38M too:

trunk:
{noformat}
56.462
55.725
55.544
55.522
{noformat}
w/patch:
{noformat}
59.9
59.6
{noformat}

I'll see if I can find out the problem here; I assume the collision ratio should be nearly identical... but who knows. This is of no priority, but interesting stuff. I'll close if I can't get it better than the trunk version.","23/Mar/11 13:09;dweiss;I spent some time on this. It's quite fascinating: the number of collisions for the default probing is smaller than:

a) linear probing with murmurhash mix of the original hash
b) linear probing without murmurhash mix (start from raw hash only).

Curiously, the number of collisions for (b) is smaller than for (a) -- this could be explained if we assume bits are spread evently throughout the entire 32-bit range after murmurhash, so after masking to table size there should be more collisions on lower bits compared to a raw hash (this would have more collisions on upper bits and fewer on lower bits because it is multiplicative... or at least I think so).

Anyway, I tried many different versions and I don't see any significant difference in favor of linear probing here. Measured the GC overhead during my tests too, but it is not the primary factor contributing to the total cost of constructing the FST (about 3-5% of the total time, running in parallel, typically).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant eclipse should setup default project formatting.,LUCENE-3791,12542736,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,15/Feb/12 19:26,10/May/13 10:43,30/Sep/19 08:38,15/Feb/12 20:51,,,,,,,,,,,3.6,4.0-ALPHA,,,general/build,,,0,,,,"I admit it's selfish. I have a git workflow and I often do ""git clean -xfd"" which restores a pristine state of the current branch (faster than ant clean :). Unfortunately this also results in removal of Eclipse files. ""ant eclipse"" doesn't reset formatting properly so I need to restore it manually.

This patch does two things:
- it sets project formatting automatically on ""ant eclipse"",
- it removes explicit Lucene-formatting.xml to avoid duplication (and potential inconsistencies) between project-specific formatter rules contained in org.eclipse.jdt.core.prefs and the formatter's XML. The XML can be exported manually if needed.",,,,,,,,,,,,,,,,"15/Feb/12 19:54;dweiss;LUCENE-3791.patch;https://issues.apache.org/jira/secure/attachment/12514695/LUCENE-3791.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-15 19:30:20.153,,,false,,,,,,,,,,,,,,,228022,,,Wed Feb 15 20:26:58 UTC 2012,New,,,,,,,"0|i04i9z:",24226,,,,,,,,,"15/Feb/12 19:30;rcmuir;I don't think its selfish, +1 for configuring the project formatting in 'ant eclipse'

the reason it doesnt do this now: i thought that formatters were 'global' in eclipse, and didnt know we could register one and then set it to the project automatically... :)","15/Feb/12 19:34;uschindler;Hey cool, how does that work? I only know the same like Robert said: the format template has to be imported globally and can assigned per project.","15/Feb/12 19:53;dweiss;I really can't explain how it works other than I know it does on my eclipse. I forgot to attach a patch -- please check if this works for you (various Eclipse flavors and systems) and if we're sure it works everywhere I'll commit in. Patch follows.","15/Feb/12 19:54;dweiss;git patch, but should work with patch -p0 <...","15/Feb/12 20:06;rcmuir;This patch actually works, at first it seems like its not working, 
because if you click on project and look at the formatting, it looks 
like it isn't working (shows a project-specific 'default eclipse' 
configuration with tabs, etc).

But what that configuration screen shows is just a lie, this 
patch is somehow tricking eclipse and it actually does work 
when you go to write/format code.
","15/Feb/12 20:15;dweiss;Yes, you won't have a ""named"" formatter -- this is something I couldn't achieve. It's more like project-specific settings based on the default formatter. But they are versioned and everything seems to work fine.","15/Feb/12 20:26;rcmuir;But it does work, +1
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RamUsageEstimator.NUM_BYTES_ARRAY_HEADER and other constants are incorrect,LUCENE-3867,12546359,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,shaie,shaie,14/Mar/12 06:22,10/May/13 10:43,30/Sep/19 08:38,23/Mar/12 19:17,,,,,,,,,,,3.6,4.0-ALPHA,,,core/index,,,0,,,,"RamUsageEstimator.NUM_BYTES_ARRAY_HEADER is computed like that: NUM_BYTES_OBJECT_HEADER + NUM_BYTES_INT + NUM_BYTES_OBJECT_REF. The NUM_BYTES_OBJECT_REF part should not be included, at least not according to this page: http://www.javamex.com/tutorials/memory/array_memory_usage.shtml

{quote}
A single-dimension array is a single object. As expected, the array has the usual object header. However, this object head is 12 bytes to accommodate a four-byte array length. Then comes the actual array data which, as you might expect, consists of the number of elements multiplied by the number of bytes required for one element, depending on its type. The memory usage for one element is 4 bytes for an object reference ...
{quote}

While on it, I wrote a sizeOf(String) impl, and I wonder how do people feel about including such helper methods in RUE, as static, stateless, methods? It's not perfect, there's some room for improvement I'm sure, here it is:

{code}
	/**
	 * Computes the approximate size of a String object. Note that if this object
	 * is also referenced by another object, you should add
	 * {@link RamUsageEstimator#NUM_BYTES_OBJECT_REF} to the result of this
	 * method.
	 */
	public static int sizeOf(String str) {
		return 2 * str.length() + 6 // chars + additional safeness for arrays alignment
				+ 3 * RamUsageEstimator.NUM_BYTES_INT // String maintains 3 integers
				+ RamUsageEstimator.NUM_BYTES_ARRAY_HEADER // char[] array
				+ RamUsageEstimator.NUM_BYTES_OBJECT_HEADER; // String object
	}
{code}

If people are not against it, I'd like to also add sizeOf(int[] / byte[] / long[] / double[] ... and String[]).",,,,,,,,,,,,,,,,"18/Mar/12 15:56;uschindler;LUCENE-3867-3.x.patch;https://issues.apache.org/jira/secure/attachment/12518837/LUCENE-3867-3.x.patch","14/Mar/12 13:04;uschindler;LUCENE-3867-compressedOops.patch;https://issues.apache.org/jira/secure/attachment/12518315/LUCENE-3867-compressedOops.patch","23/Mar/12 12:24;dweiss;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12519612/LUCENE-3867.patch","23/Mar/12 10:40;dweiss;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12519603/LUCENE-3867.patch","18/Mar/12 11:58;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518835/LUCENE-3867.patch","18/Mar/12 10:53;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518833/LUCENE-3867.patch","17/Mar/12 22:50;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518812/LUCENE-3867.patch","17/Mar/12 21:53;dweiss;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518810/LUCENE-3867.patch","17/Mar/12 15:33;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518795/LUCENE-3867.patch","17/Mar/12 14:53;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518793/LUCENE-3867.patch","17/Mar/12 10:56;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518785/LUCENE-3867.patch","17/Mar/12 08:56;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518782/LUCENE-3867.patch","16/Mar/12 22:14;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518743/LUCENE-3867.patch","16/Mar/12 16:53;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518702/LUCENE-3867.patch","16/Mar/12 12:54;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518661/LUCENE-3867.patch","16/Mar/12 12:07;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518657/LUCENE-3867.patch","15/Mar/12 14:07;shaie;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518463/LUCENE-3867.patch","15/Mar/12 12:56;shaie;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518455/LUCENE-3867.patch","15/Mar/12 09:40;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518439/LUCENE-3867.patch","14/Mar/12 16:53;uschindler;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518337/LUCENE-3867.patch","14/Mar/12 13:27;shaie;LUCENE-3867.patch;https://issues.apache.org/jira/secure/attachment/12518317/LUCENE-3867.patch",21.0,,,,,,,,,,,,,,,,,,,2012-03-14 07:42:03.477,,,false,,,,,,,,,,,,,,,231517,,,Mon Mar 26 08:45:41 UTC 2012,New,Patch Available,,,,,,"0|i04htj:",24152,,,,,,,,,"14/Mar/12 07:42;dweiss;One can provide exact object allocation size (including alignments) by running with an agent (acquired from Instrumentation). This is shown here, for example:

http://www.javaspecialists.eu/archive/Issue142.html

I don't think it makes sense to be ""perfect"" here because there is a tradeoff between being accurate and being fast. One thing to possibly improve would be to handle reference size (4 vs. 8 bytes; in particular with compact references while running under 64 bit jvms).","14/Mar/12 07:43;dweiss;Oh, one thing that I had in the back of my mind was to run a side-by-side comparison of Lucene's memory estimator and ""exact"" memory occupation via agent and see what the real difference is (on various vms and with compact vs. non-compact refs).

This would be a 2 hour effort I guess, fun, but I don't have the time for it.","14/Mar/12 08:02;uschindler;I was talking with Shai already about the OBJECT_REF size of 8, in RamUsageEstimator it is:

{code:java}
public final static int NUM_BYTES_OBJECT_REF = Constants.JRE_IS_64BIT ? 8 : 4;
{code}

...which does not take the CompressedOops into account. Can we detect those oops, so we can change the above ternary to return 4 on newer JVMs with compressed oops enabled?","14/Mar/12 08:06;dweiss;If you're running with an agent then it will tell you many bytes a reference is, so this would fix the issue. I don't think you can test this from within Java VM itself, but this is an interesting question. What you could do is spawn a child VM process with identical arguments (and an agent) and check it there, but this is quite awful... 

I'll ask on hotspot mailing list, maybe they know how to do this.","14/Mar/12 09:26;shaie;bq. I don't think it makes sense to be ""perfect"" here because there is a tradeoff between being accurate and being fast.

I agree. We should be fast, and ""as accurate as we can get while preserving speed"".

I will fix the constant's value as it's wrong. The helper methods are just that - helper. Someone can use other techniques to compute the size of objects.

Will post a patch shortly.","14/Mar/12 10:42;mikemccand;Nice catch on the overcounting of array's RAM usage!

And +1 for additional sizeOf(...) methods.","14/Mar/12 10:51;uschindler;Hi Mike,

Dawid and I were already contacting Hotspot list. There is an easy way to get the compressedOoooooops setting from inside the JVM using MXBeans from the ManagementFactory. I think we will provide a patch later! I think by that we could also optimize the check for 64 bit, because that one should also be reported by the MXBean without looking into strange sysprops (see the TODO in the code for JRE_IS_64BIT).

Uwe","14/Mar/12 10:56;dweiss;Sysprops should be a fallback though because (to be verified) they're supported by other vendors whereas the mx bean may not be.

It needs to be verified by running under j9, jrockit, etc.","14/Mar/12 12:52;mikemccand;Consulting MXBean sounds great?

bq. Sysprops should be a fallback though 

+1
","14/Mar/12 13:04;uschindler;Here the patch for detecting compressesOops in Sun JVMs. For other JVMs it will simply use false, so the object refs will be guessed to have 64 bits, which is fine as upper memory limit.

The code does only use public Java APIs and falls back if anything fails to false.","14/Mar/12 13:27;shaie;Patch adds RUE.sizeOf(String) and various sizeOf(arr[]) methods. Also fixes the ARRAY_HEADER.

Uwe, I merged with your patch, with one difference -- the System.out prints in the test are printed only if VERBOSE.","14/Mar/12 13:34;uschindler;Shai: Thanks! I am in a train at the moment, so internet is slow/not working. I will later find out what MXBeans we can use to detect 64bit without looking at strange sysprops (which may have been modified by user code, so not really secure to use...).

I left the non-verbose printlns in it, so people reviewing the patch can quickly see by running that test what happens on their JVM. It would be interesting to see what your jRockit does... :-)","14/Mar/12 13:45;shaie;I tried IBM and Oracle 1.6 JVMs, and both printed the same:

{code}
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: This JVM is 64bit: true
    [junit] NOTE: This JVM uses CompressedOops: false
    [junit] ------------- ---------------- ---------------
{code}

So no CompressedOops for me :).

bq. I will later find out what MXBeans we can use to detect 64bit without looking at strange sysprops

Ok. If you'll make it, we can add these changes to that patch, otherwise we can also do them in a separate issue.","14/Mar/12 13:46;uschindler;Hm, for me (1.6.0_31, 7u3) it prints true. What JVMs are you using and what settings?","14/Mar/12 13:54;uschindler;Here my results:

{noformat}
*****************************************************
JAVA_HOME = C:\Program Files\Java\jdk1.7.0_03
java version ""1.7.0_03""
Java(TM) SE Runtime Environment (build 1.7.0_03-b05)
Java HotSpot(TM) 64-Bit Server VM (build 22.1-b02, mixed mode)
*****************************************************

C:\Users\Uwe Schindler\Projects\lucene\trunk-lusolr1\lucene\core>ant test -Dtestcase=TestRam*
[junit] Testsuite: org.apache.lucene.util.TestRamUsageEstimator
[junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0,561 sec
[junit]
[junit] ------------- Standard Output ---------------
[junit] NOTE: This JVM is 64bit: true
[junit] NOTE: This JVM uses CompressedOops: true
[junit] ------------- ---------------- ---------------

C:\Users\Uwe Schindler\Projects\lucene\trunk-lusolr1\lucene\core>ant test -Dtestcase=TestRam* -Dargs=-XX:-UseCompressedOops
[junit] Testsuite: org.apache.lucene.util.TestRamUsageEstimator
[junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0,5 sec
[junit]
[junit] ------------- Standard Output ---------------
[junit] NOTE: This JVM is 64bit: true
[junit] NOTE: This JVM uses CompressedOops: false
[junit] ------------- ---------------- ---------------

*****************************************************
JAVA_HOME = C:\Program Files\Java\jdk1.6.0_31
java version ""1.6.0_31""
Java(TM) SE Runtime Environment (build 1.6.0_31-b05)
Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)
*****************************************************

C:\Users\Uwe Schindler\Projects\lucene\trunk-lusolr1\lucene\core>ant test -Dtestcase=TestRam*
[junit] Testsuite: org.apache.lucene.util.TestRamUsageEstimator
[junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0,453 sec
[junit]
[junit] ------------- Standard Output ---------------
[junit] NOTE: This JVM is 64bit: true
[junit] NOTE: This JVM uses CompressedOops: true
[junit] ------------- ---------------- ---------------

C:\Users\Uwe Schindler\Projects\lucene\trunk-lusolr1\lucene\core>ant test -Dtestcase=TestRam* -Dargs=-XX:-UseCompressedOops
[junit] Testsuite: org.apache.lucene.util.TestRamUsageEstimator
[junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0,421 sec
[junit]
[junit] ------------- Standard Output ---------------
[junit] NOTE: This JVM is 64bit: true
[junit] NOTE: This JVM uses CompressedOops: false
[junit] ------------- ---------------- ---------------

C:\Users\Uwe Schindler\Projects\lucene\trunk-lusolr1\lucene\core>ant test -Dtestcase=TestRam* -Dargs=-XX:+UseCompressedOops
[junit] Testsuite: org.apache.lucene.util.TestRamUsageEstimator
[junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0,422 sec
[junit]
[junit] ------------- Standard Output ---------------
[junit] NOTE: This JVM is 64bit: true
[junit] NOTE: This JVM uses CompressedOops: true
[junit] ------------- ---------------- ---------------
{noformat}","14/Mar/12 14:05;shaie;Oracle:

{code}
java version ""1.6.0_21""
Java(TM) SE Runtime Environment (build 1.6.0_21-b07)
Java HotSpot(TM) 64-Bit Server VM (build 17.0-b17, mixed mode)
{code}

IBM:

{code}
java version ""1.6.0""
Java(TM) SE Runtime Environment (build pwa6460sr9fp3-20111122_05(SR9 FP3))
IBM J9 VM (build 2.4, JRE 1.6.0 IBM J9 2.4 Windows 7 amd64-64 jvmwa6460sr9-20111111_94827 (JIT enabled, AOT enabled)
J9VM - 20111111_094827
JIT  - r9_20101028_17488ifx45
GC   - 20101027_AA)
JCL  - 20110727_07
{code}","14/Mar/12 14:09;shaie;I ran ""ant test-core -Dtestcase=TestRam* -Dtests.verbose=true -Dargs=-XX:+UseCompressedOops"" and with the Oracle JVM I get ""Compressed Oops: true"" but with IBM JVM I still get 'false'.","14/Mar/12 14:15;uschindler;OK, that is expected. 1.6.0_21 does not enable compressedOops by default, so false is correct. If you manually enable, it gets true.

jRockit is jRockit and not Sun/Oracle, so the result is somehow expected. It seems to nor have that MXBrean. But the code does not produce strange exceptions, so at least in the Sun VM we can detect compressed Oops and guess the reference size better. 8 is still not bad as it gives an upper limit.","14/Mar/12 14:18;uschindler;By the way, here is the code from the hotspot mailing list member (my code is based on it), it also shows the outputs for different JVMs:

https://gist.github.com/1333043

(I just removed the com.sun.* imports and replaced by reflection)","14/Mar/12 14:25;shaie;bq. 8 is still not bad as it gives an upper limit.

I agree. Better to over-estimate here, than under-estimate.

Would appreciate if someone can take a look at the sizeOf() impls before I commit.","14/Mar/12 15:01;uschindler;On Hotspot Mailing list some people also seem to have an idea about jRockit and IBM J9:

{quote}
From: Krystal Mok
Sent: Wednesday, March 14, 2012 3:46 PM
To: Uwe Schindler
Cc: Dawid Weiss; hotspot compiler
Subject: Re: How to detect if the VM is running with compact refs from within the VM (no agent)?

Hi,

Just in case you'd care, the same MXBean could be used to detect compressed references on JRockit, too. It's probably available starting from JRockit R28.

Instead of ""UseCompressedOops"", use ""CompressedRefs"" as the VM option name on JRockit.

Don't know how to extract this information for J9 without another whole bunch of hackeries...well, you could try this, on a ""best-effort"" basis for platform detection:
IBM J9's VM version string contains the compressed reference information. Example:

$ export JAVA_OPTS='-Xcompressedrefs'
$ groovysh
Groovy Shell (1.7.7, JVM: 1.7.0)
Type 'help' or '\h' for help.
----------------------------------------------------------------------------------------------------------------------------
groovy:000> System.getProperty 'java.vm.info'
===> JRE 1.7.0 Linux amd64-64 Compressed References 20110810_88604 (JIT enabled, AOT enabled)
J9VM - R26_Java726_GA_20110810_1208_B88592
JIT  - r11_20110810_20466
GC   - R26_Java726_GA_20110810_1208_B88592_CMPRSS
J9CL - 20110810_88604
groovy:000> quit

So grepping for ""Compressed References"" in the ""java.vm.info"" system property gives you the clue.

- Kris
{quote}","14/Mar/12 15:37;mikemccand;Patch looks good!

Maybe just explain in sizeOf(String) javadoc that this method assumes the String is ""standalone"" (ie, does not reference a larger char[] than itself)?

Because... if you call String.substring, the returned string references a slice the char[] of the original one... and so technically the RAM it's tying up could be (much) larger than expected.  (At least, this used to be the case... not sure if it's changed...).","14/Mar/12 15:50;shaie;Good point. I clarified the jdocs with this:

{code}
  /**
   * Returns the approximate size of a String object. This computation relies on
   * {@link String#length()} to compute the number of bytes held by the char[].
   * However, if the String object passed to this method is the result of e.g.
   * {@link String#substring}, the computation may be entirely inaccurate
   * (depending on the difference between length() and the actual char[]
   * length).
   */
{code}

If there are no objections, I'd like to commit this.","14/Mar/12 15:52;dweiss;I would opt for sizeOf to return the actual size of the object, including underlying string buffers... We can take into account interning buffers but other than that I wouldn't skew the result because it can be misleading.
","14/Mar/12 15:56;dweiss;I don't like this special handling of Strings, to be honest. Why do we need/do it?","14/Mar/12 16:02;shaie;bq. I don't like this special handling of Strings, to be honest. Why do we need/do it?

Because I wrote it, and it seemed useful to me, so why not? We know how Strings look like, at least in their worse case. If there will be a better implementation, we can fix it in RUE, rather than having many impls try to do it on their own?","14/Mar/12 16:18;mikemccand;bq. I don't like this special handling of Strings, to be honest. 

I'm confused: what special handling of Strings are we talking about...?

You mean that sizeOf(String) doesn't return the correct answer if the string came from a previous .substring (.split too) call...?

If so, how can we actually fix that?  Is there some way to ask a string for the true length of its char[]?","14/Mar/12 16:32;dweiss;{code}
+  /** Returns the size in bytes of the String[] object. */
+  public static int sizeOf(String[] arr) {
+    int size = alignObjectSize(NUM_BYTES_ARRAY_HEADER + NUM_BYTES_OBJECT_REF * arr.length);
+    for (String s : arr) {
+      size += sizeOf(s);
+    }
+    return size;
+  }
+
+  /** Returns the approximate size of a String object. */
+  public static int sizeOf(String str) {
+    // String's char[] size
+    int arraySize = alignObjectSize(NUM_BYTES_ARRAY_HEADER + NUM_BYTES_CHAR * str.length());
+
+    // String's row object size    
+    int objectSize = alignObjectSize(NUM_BYTES_OBJECT_REF /* array reference */
+        + 3 * NUM_BYTES_INT /* String holds 3 integers */
+        + NUM_BYTES_OBJECT_HEADER /* String object header */);
+    
+    return objectSize + arraySize;
+  }
{code}

What I mean is that without looking at the code I would expect sizeOf(String[] N) to return the actual memory taken by an array of strings. If they point to a single char[], this should simple count the object overhead, not count every character N times as it would do now. This isn't sizeOf(), this is sum(string lengths * 2) + epsilon to me.

I'd keep RamUsageEstimator exactly what the name says -- an estimation of the actual memory taken by a given object. A string can point to a char[] and if so this should be traversed as an object and counted once.
","14/Mar/12 16:34;dweiss;bq. If so, how can we actually fix that? Is there some way to ask a string for the true length of its char[]?

Same as with other objects -- traverse its fields and count them (once, building an identity set for all objects reachable from the root)?","14/Mar/12 16:46;shaie;bq. What I mean is that without looking at the code I would expect sizeOf(String[] N) to return the actual memory taken by an array of strings.

So you mean you'd want sizeOf(String[]) be just that?

{code}
return alignObjectSize(NUM_BYTES_ARRAY_HEADER + NUM_BYTES_OBJECT_REF * arr.length);
{code}

I don't mind. I just thought that since we know how to compute sizeOf(String), we can use that. It's an extreme case, I think, that someone will want to compute the size of String[] which share same char[] instance ... but I don't mind if it bothers you that much, to simplify it and document that it computes the raw size of the String[].

But I don't think that we should change sizeOf(String) to not count the char[] size. It's part of the object, and really it's String, not like we're trying to compute the size of a general object.

bq. Same as with other objects – traverse its fields and count them

RUE already has .estimateRamUsage(Object) which does that through reflection. I think that sizeOf(String) can remain fast as it is now, with the comment that it my over-estimate if the String is actually a sub-string of one original larger string. In the worse case, we'll just be over-estimating.","14/Mar/12 16:53;uschindler;Hi Shai,

can ypou try this patch with J9 or maybe JRockit (Robert)? If yozu use one of those JVMs you may have to explicitely enable  compressed Oops/refs!","14/Mar/12 16:56;dweiss;bq. RUE already has .estimateRamUsage(Object) which does that through reflection. I think that sizeOf(String) can remain fast as it is now, with the comment that it my over-estimate if the String is actually a sub-string of one original larger string. In the worse case, we'll just be over-estimating.

Yeah, that's exactly what I didn't like. All the primitive/ primitive array methods are fine, but why make things inconsistent with sizeOf(String)? I'd rather have the reflection-based method estimate the size of a String/String[]. Like we mentioned it's always a matter of speed/accuracy but here I'd opt for accuracy because the output can be off by a lot if you make substrings along the way (not to mention it assumes details about String internal implementation which may or may not be true, depending on the vendor).

Do you have a need for this method, Shai? If you don't then why not wait (with this part) until such a need arises?
","14/Mar/12 17:01;shaie;bq. Do you have a need for this method, Shai?

I actually started this issue because of this method :). I wrote the method for my own code, then spotted the bug in the ARRAY_HEADER, and on the go thought that it will be good if RUE would offer it for me / other people can benefit from it. Because from my experience, after I put code in Lucene, very smart people improve and optimize it, and I benefit from it in new releases.

So while I could keep sizeOf(String) in my own code, I know that Uwe/Robert/Mike/You will make it more efficient when Java 7/8/9 will be out, while I'll totally forget about it ! :).","14/Mar/12 17:10;dweiss;Yeah... well... I'm flattered :) I'm still -1 for adding this particular method because I don't like being surprised at how a method works and this is surprising behavior to me, especially in this class (even if it's documented in the javadoc, but who reads it anyway, right?).

If others don't share my opinion then can we at least rename this method to sizeOfBlah(..) where Blah is something that would indicate it's not actually taking into account char buffer sharing or sub-slicing (suggestions for Blah welcome)?","14/Mar/12 17:23;markrmiller@gmail.com;estimateSizeOf(..)
guessSizeOf(..)
wildGuessSizeOf(..)
incorrectSizeOf(..)
sizeOfWeiss(..)
weissSize(..)
sizeOfButWithoutTakingIntoAccountCharBufferSharingOrSubSlicingSeeJavaDoc(..)","14/Mar/12 17:35;mikemccand;{quote}
bq. If so, how can we actually fix that? Is there some way to ask a string for the true length of its char[]?

Same as with other objects – traverse its fields and count them (once, building an identity set for all objects reachable from the root)?
{quote}

Aha, cool!  I hadn't realized RUE can crawl into the private char[] inside string and count up the RAM usage correctly.  That's nice.

Maybe lowerBoundSizeOf(...)?

Or maybe we don't add the new string methods (sizeOf(String), sizeOf(String[])) and somewhere document that you should do new RUE().size(String/String[]) instead...?  Hmm or maybe we do add the methods, but implement them under-the-hood w/ that?","14/Mar/12 18:02;dweiss;bq. sizeOfWeiss(..)

We're talking some serious dimensions here, beware of buffer overflows!

bq. Or maybe we don't add the new string methods (sizeOf(String), sizeOf(String[])) and somewhere document that you should do new RUE().size(String/String[]) instead..

This is something I would go for -- it's consistent with what I would consider this class's logic. I would even change it to sizeOf(Object) -- this would be a static shortcut to just measure an object's size, no strings attached?

Kabutz's code also distinguishes interned strings/ cached boxed integers and enums. This could be a switch much like it is now with interned Strings. Then this would really be either an upper (why lower, Mike?) bound or something that would try to be close to the exact memory consumption.

A fun way to determine if we're right would be to run a benchmark with -Xmx20mb and test how close we can get to the main memory pool's maximum value before OOM is thrown. :)
","14/Mar/12 18:13;mikemccand;bq. (why lower, Mike?)

Oh I just meant the sizeOf(String) impl in the current patch is a lower bound (since it ""guesses"" the private char[] length by calling String.length(), which is a lower bound on the actual char[] length).
","14/Mar/12 18:18;dweiss;John Rose just replied to my question -- there are fields in Unsafe that allow array scaling (1.7). Check these out:

{noformat}
        ARRAY_BOOLEAN_INDEX_SCALE = theUnsafe.arrayIndexScale([Z);
        ARRAY_BYTE_INDEX_SCALE = theUnsafe.arrayIndexScale([B);
        ARRAY_SHORT_INDEX_SCALE = theUnsafe.arrayIndexScale([S);
        ARRAY_CHAR_INDEX_SCALE = theUnsafe.arrayIndexScale([C);
        ARRAY_INT_INDEX_SCALE = theUnsafe.arrayIndexScale([I);
        ARRAY_LONG_INDEX_SCALE = theUnsafe.arrayIndexScale([J);
        ARRAY_FLOAT_INDEX_SCALE = theUnsafe.arrayIndexScale([F);
        ARRAY_DOUBLE_INDEX_SCALE = theUnsafe.arrayIndexScale([D);
        ARRAY_OBJECT_INDEX_SCALE = theUnsafe.arrayIndexScale([Ljava/lang/Object;);
        ADDRESS_SIZE = theUnsafe.addressSize();
{noformat}

So... there is a (theoretical?) possibility that, say, byte[] is machine word-aligned :) I bet any RAM estimator written so far will be screwed if this happens :)","14/Mar/12 20:19;uschindler;So the whole Oops MBean magic is obsolete... ADDRESS_SIZE = theUnsafe.addressSize(); woooah, so simple - works on more platforms for guessing!

I will check this out with the usual reflection magic :-)","15/Mar/12 09:40;uschindler;Hi,
here new patch using Unsafe to get the bitness (with the well-known fallback) and for compressedOops detection. Looks much cleaner.
I also like it more, that the addressSize is now detected natively and not from sysprops.

The constants mentioned by Dawid are only availabe in Java 7, so i reflected the underlying methods from theUnsafe. I also changed the boolean JRE_USES_COMPRESSED_OOPS to an integer JRE_REFERENCE_SIZE that is used by RamUsageEstimator. We might do the same for all other native types... (this is just a start).

Shai: Can you test with your JVMs and also enable/disable compressed oops/refs?","15/Mar/12 12:56;shaie;Thanks Uwe !

I ran the test, and now with both J9 (IBM) and Oracle, I get this print (without enabling any flag):

{code}
    [junit] NOTE: running test testReferenceSize
    [junit] NOTE: This JVM is 64bit: true
    [junit] NOTE: Reference size in this JVM: 8
{code}

* I modified the test name to testReferenceSize (was testCompressedOops).

I wrote this small test to print the differences between sizeOf(String) and estimateRamUsage(String):

{code}
  public void testSizeOfString() throws Exception {
    String s = ""abcdefgkjdfkdsjdskljfdskfjdsf"";
    String sub = s.substring(0, 4);
    System.out.println(""original="" + RamUsageEstimator.sizeOf(s));
    System.out.println(""sub="" + RamUsageEstimator.sizeOf(sub));
    System.out.println(""checkInterned=true(orig): "" + new RamUsageEstimator().estimateRamUsage(s));
    System.out.println(""checkInterned=false(orig): "" + new RamUsageEstimator(false).estimateRamUsage(s));
    System.out.println(""checkInterned=false(sub): "" + new RamUsageEstimator(false).estimateRamUsage(sub));
  }
{code}

It prints:
{code}
original=104
sub=56
checkInterned=true(orig): 0
checkInterned=false(orig): 98
checkInterned=false(sub): 98
{code}

So clearly estimateRamUsage factors in the sub-string's larger char[]. The difference in sizes of 'orig' stem from AverageGuessMemoryModel which computes the reference size to be 4 (hardcoded), and array size to be 16 (hardcoded). I modified AverageGuess to use constants from RUE (they are best guesses themselves). Still the test prints a difference, but now I think it's because sizeOf(String) aligns the size to mod 8, while estimateRamUsage isn't. I fixed that in size(Object), and now the prints are the same.

* I also fixed sizeOfArray -- if the array.length == 0, it returned 0, but it should return its header, and aligned to mod 8 as well.

* I modified sizeOf(String[]) to sizeOf(Object[]) and compute its raw size only. I started to add sizeOf(String), fastSizeOf(String) and deepSizeOf(String[]), but reverted to avoid the hassle -- the documentation confuses even me :).

* Changed all sizeOf() to return long, and align() to take and return long.

I think this is ready to commit, though I'd appreciate a second look on the MemoryModel and size(Obj) changes.

Also, how about renaming MemoryModel methods to: arrayHeaderSize(), classHeaderSize(), objReferenceSize() to make them more clear and accurate? For instance, getArraySize does not return the size of an array, but its object header ...","15/Mar/12 13:56;dweiss;-1 to mixing shallow and deep sizeofs -- sizeOf(Object[] arr) is shallow and just feels wrong to me. All the other methods yield the deep total, why make an exception? If anything, make it explicit and then do it for any type of object -- 

{code}
shallowSizeOf(Object t);
sizeOf(Object t);
{code}

I'm not complaining just because my sense of taste is feeling bad. I am actually using this class in my own projects and I would hate to look into the JavaDoc every time to make sure what a given method does (especially with multiple overloads). In other words, I would hate to see this:

{code}
Object [] o1 = new Object [] {1, 2, 3};
Object o2 = o1;
if (sizeOf(o1) != sizeOf(o2)) throw new WtfException();
{code}



","15/Mar/12 14:02;uschindler;{quote}

I ran the test, and now with both J9 (IBM) and Oracle, I get this print (without enabling any flag):

{code}
    [junit] NOTE: running test testReferenceSize
    [junit] NOTE: This JVM is 64bit: true
    [junit] NOTE: Reference size in this JVM: 8
{code}
{quote}

I hope with compressedOops explicitely enabled (or however they call them), you get a reference size of 4 in J9 and pre-1.6.0_23 Oracle?","15/Mar/12 14:07;shaie;Ok removed sizeOf(Object[]). One can compute it by using RUE.estimateRamSize to do a deep calculation.

Geez Dawid, you took away all the reasons I originally opened the issue for ;).

But at least AvgGuessMemoryModel and RUE.size() are more accurate now. And we have some useful utility methods.","15/Mar/12 14:12;shaie;I ran ""ant test-core -Dtestcase=TestRam* -Dtests.verbose=true -Dargs=-XX:+UseCompressedOops"" and ""ant test-core -Dtestcase=TestRam* -Dtests.verbose=true -Dargs=-XX:-UseCompressedOops"" and get 8 and 4 (with CompressedOops).","15/Mar/12 14:36;markrmiller@gmail.com;Oh, bummer - looks like we lost the whole history of this class...such a bummer. I really wanted to take a look at how this class had evolved since I last looked at it. I've missed the conversations around the history loss - is that gone, gone, gone, or is there still some way to find it?","15/Mar/12 14:40;markrmiller@gmail.com;Scratch that - I was trying to look back from the apache git clone using git - assumed it's history matched svn - but I get a clean full history using svn.","15/Mar/12 14:42;uschindler;Die, GIT, die! :-) (as usual)","15/Mar/12 14:53;uschindler;bq. I ran ""ant test-core -Dtestcase=TestRam* -Dtests.verbose=true -Dargs=-XX:+UseCompressedOops"" and ""ant test-core -Dtestcase=TestRam* -Dtests.verbose=true -Dargs=-XX:-UseCompressedOops"" and get 8 and 4 (with CompressedOops).

OK, thanks. So it seems to work at least with Oracle/Sun and IBM J9. I have no other updates to this detection code.","15/Mar/12 15:00;dweiss;bq. Geez Dawid, you took away all the reasons I originally opened the issue for 

This is by no means wasted time. I think the improvements are clear?

bq. Die, GIT, die!

I disagree here -- git is a great tool, even if the learning curve may be steep at first. git-svn is a whole different story (it's a great hack but just a hack).","15/Mar/12 15:05;uschindler;bq. I disagree here

Calm down, was just my well-known standard answer :-)","15/Mar/12 15:08;dweiss;Oh, I am calm, I just know people do hate git (and I used to as well, until I started using it frequently). Robert has a strong opinion about git, for example. 

Besides, there's nothing wrong in having a strong opinion -- it's great people can choose what they like and still collaborate via patches (and this seems to be the common ground between all vcs's).","15/Mar/12 15:09;shaie;bq. This is by no means wasted time. I think the improvements are clear?

Yes, yes. It was a joke.

Ok so can I proceed with the commit, or does someone intend to review the patch later?","15/Mar/12 15:09;uschindler;With unsafe we also get all those information like size of array header we have hardcoded. Should we not try to get these in the same way like I did for bitness and reference size - using Unsafe.theUnsafe.arrayBaseOffset()? And fallback to our hardcoded defaults?","15/Mar/12 15:16;dweiss;bq. using Unsafe.theUnsafe.arrayBaseOffset()? And fallback to our hardcoded defaults?

+1. 

I will also try on OpenJDK with various jits but I'll do it in the evening.

bq. Yes, yes. It was a joke.

Joke or no joke the truth is I did complain a lot. :)
","15/Mar/12 20:14;dweiss;I just peeked at OpenJDK sources and addressSize() is defined as this:
{code}
// See comment at file start about UNSAFE_LEAF
//UNSAFE_LEAF(jint, Unsafe_AddressSize())
UNSAFE_ENTRY(jint, Unsafe_AddressSize(JNIEnv *env, jobject unsafe))
  UnsafeWrapper(""Unsafe_AddressSize"");
  return sizeof(void*);
UNSAFE_END
{code}

In this light this switch:
{code}
switch (addressSize) {
  case 4:
    is64Bit = Boolean.FALSE;
    break;
  case 8:
    is64Bit = Boolean.TRUE;
    break;
}
{code}
Becomes interesting. Do you know of any architecture with pointers different than 4 or 8 bytes? :)","15/Mar/12 20:49;dweiss;A few more exotic jits from OpenJDK (all seem to be using explicit 8 byte ref size on 64-bit:
{noformat}
> ant test-core -Dtestcase=TestRam* -Dtests.verbose=true ""-Dargs=-jamvm""
    [junit] JVM: OpenJDK Runtime Environment, JamVM, Robert Lougher, 1.6.0-devel, Java Virtual Machine Specification, Sun Microsystems Inc., 1.6.0_23, Sun Microsystems Inc., null,
    [junit] NOTE: This JVM is 64bit: true
    [junit] NOTE: Reference size in this JVM: 8

> ant test-core -Dtestcase=TestRam* -Dtests.verbose=true ""-Dargs=-jamvm -XX:+UseCompressedOops""
    [junit] JVM: OpenJDK Runtime Environment, JamVM, Robert Lougher, 1.6.0-devel, Java Virtual Machine Specification, Sun Microsystems Inc., 1.6.0_23, Sun Microsystems Inc., null,
    [junit] NOTE: This JVM is 64bit: true
    [junit] NOTE: Reference size in this JVM: 8

> ant test-core -Dtestcase=TestRam* -Dtests.verbose=true ""-Dargs=-cacao""
    [junit] JVM: OpenJDK Runtime Environment, CACAO, CACAOVM - Verein zur Foerderung der freien virtuellen Maschine CACAO, 1.1.0pre2, Java Virtual Machine Specification, Sun Microsystems Inc., 1.6.0_23, Sun Microsystems Inc., null,
    [junit] NOTE: This JVM is 64bit: true
    [junit] NOTE: Reference size in this JVM: 8

> ant test-core -Dtestcase=TestRam* -Dtests.verbose=true ""-Dargs=-server""
    [junit] JVM: OpenJDK Runtime Environment, OpenJDK 64-Bit Server VM, Sun Microsystems Inc., 20.0-b11, Java Virtual Machine Specification, Sun Microsystems Inc., 1.6.0_23, Sun Microsystems Inc., null,
    [junit] NOTE: This JVM is 64bit: true
    [junit] NOTE: Reference size in this JVM: 4

> ant test-core -Dtestcase=TestRam* -Dtests.verbose=true ""-Dargs=-server -XX:-UseCompressedOops""
    [junit] JVM: OpenJDK Runtime Environment, OpenJDK 64-Bit Server VM, Sun Microsystems Inc., 20.0-b11, Java Virtual Machine Specification, Sun Microsystems Inc., 1.6.0_23, Sun Microsystems Inc., null,
    [junit] NOTE: This JVM is 64bit: true
    [junit] NOTE: Reference size in this JVM: 8
{noformat}","15/Mar/12 20:57;dweiss;Mac:
{noformat}
> ant test-core -Dtestcase=TestRam* -Dtests.verbose=true
    [junit] JVM: Java(TM) SE Runtime Environment, Java HotSpot(TM) 64-Bit Server VM, Apple Inc., 20.4-b02-402, Java Virtual Machine Specification, Sun Microsystems Inc., 1.6.0_29, Apple Inc., null, 
    [junit] NOTE: This JVM is 64bit: true
    [junit] NOTE: Reference size in this JVM: 4

> ant test-core -Dtestcase=TestRam* -Dtests.verbose=true ""-Dargs=-server -XX:-UseCompressedOops""
    [junit] JVM: Java(TM) SE Runtime Environment, Java HotSpot(TM) 64-Bit Server VM, Apple Inc., 20.4-b02-402, Java Virtual Machine Specification, Sun Microsystems Inc., 1.6.0_29, Apple Inc., null, 
    [junit] NOTE: This JVM is 64bit: true
    [junit] NOTE: Reference size in this JVM: 8
{noformat}","15/Mar/12 21:05;markrmiller@gmail.com;Nooo!!! My eyes!!!! I'm pretty sure my liver has just been virally licensed!","15/Mar/12 21:27;dweiss;Ok, right, sorry, let me scramble for intellectual property protection reasons:
{noformat}
// See cemnmot at flie sratt abuot U_ANEESAFLF 
/
/ ULAAFEN_SEF (jnit, UfdAsnerS_zsiaedse ())
UEATERSNFN_Y (jint, UnidsdserSAasfe_ze (JNnEIv * env, jcbjeot unfsae))
UesWrpfapaner ("" UdenfsSseAazs_drie ""); 
rreutn seiozf (void *
;)
UNEF_SNEAD
{noformat}","15/Mar/12 22:13;uschindler;bq. Becomes interesting. Do you know of any architecture with pointers different than 4 or 8 bytes? 

When I was writing that code, I was thinking a very long time about: Hm, should I add a ""default"" case saying:

{noformat}
default:
  throw new Error(""Lucene does not like architectures with pointer size "" + addressSize)
{noformat}

But then I decided: If there is an architecture with a pointer size of 6, does this break Lucene really? Hm, maybe I should have added a comment there:

{noformat}
default:
  // this is the philosophical case of Lucene reaching an architecture returning something different here
{noformat}","15/Mar/12 22:28;uschindler;Maybe this for @UweSays:

{noformat}
default:
  throw new Error(""Your processor(*) hit me with his "" + addressSize + "" inch dick"");
  // (*)Dawid
{noformat}","15/Mar/12 22:32;dweiss;I would throw an exception just so that we can hear about those architectures nobody has ever heard of ;)","15/Mar/12 22:33;dweiss;fyi. http://en.wikipedia.org/wiki/48-bit","15/Mar/12 22:37;uschindler;http://en.wikipedia.org/wiki/Quadruple_precision_floating-point_format","16/Mar/12 07:46;dweiss;Yep, but I'm talking about address registers and addressing in general. 48 bit addressing aligning would be inconvenient if you take into account that any index scaling addressing modes would have to do a shift and an addition (*3) instead of just a shift. Interesting stuff.","16/Mar/12 07:55;uschindler;I agree, was just a joke. The comment before was more about suddenly appearing 128 bit architectures. That ones would have an addressSize of 16, still a power of 2 :-)

I will now look into the unsafe array offsets...","16/Mar/12 08:01;dweiss;Nice. All of a sudden you could enumerate all the atoms in the universe :) I love Wolfram Alpha...
http://www.wolframalpha.com/input/?i=is+number+of+atoms+in+the+universe+greater+than+2%5E128%3F","16/Mar/12 08:48;uschindler;I played around: Unsafe.arrayBaseOffset always returns 16 on my 64bit JVM, so it seems that NUM_BYTES_ARRAY_HEADER is wrong in our case (we have it as 12). It seems that the JVM aligns the array data to be multiple of 8 bytes on 64 bit machines?

For normal objects, is there a way with unsafe to get the NUM_BYTES_OBJECT_HEADER?","16/Mar/12 09:03;dweiss;bq. For normal objects, is there a way with unsafe to get the NUM_BYTES_OBJECT_HEADER?

I don't know and I don't know if it varies between vendors. As for aligning -- I bet this holds for anything, not only arrays. So fields of an object will be reordered and packed on their own boundary but entire themselves will be aligned on machine word boundaries for efficiency. Did you try running with Instrumentation (an agent)? What does it say about object/ array sizes?","16/Mar/12 09:33;uschindler;Interestingly the ARRAY header seems to be much bigger on 64 bit platforms without compact refs, so I have the feeling that somehow thre is still some space needed for an object ref, so the original definition of the size was more correct? https://gist.github.com/2038305

Using the original definition:
{code:java}
public final static int NUM_BYTES_ARRAY_HEADER = NUM_BYTES_OBJECT_HEADER + NUM_BYTES_INT + NUM_BYTES_OBJECT_REF;
{code}

This looks much more like the above size, aligned to 8 bytes.

bq. Did you try running with Instrumentation (an agent)? What does it say about object/ array sizes?

Have to try out and set this up first.","16/Mar/12 09:43;uschindler;This is also in line with this instumentation page:
http://www.javaspecialists.eu/archive/Issue142.html

Which prints:
{noformat}
measureSize(new byte[1000]);
byte[], shallow=1016, deep=1016
measureSize(new boolean[1000]);
boolean[], shallow=1016, deep=1016
{noformat}","16/Mar/12 09:45;dweiss;Array header is still 12 bytes but it is aligned to the next multiple-8 boundary? Looks like it.","16/Mar/12 09:48;uschindler;But how does that explain that with non-compact refs the arrayBaseOffset is 24?","16/Mar/12 10:02;dweiss;Can you check what size does Object[] report vs. for example Integer[]? I think the difference may be because typed arrays need to know the type of their component.","16/Mar/12 10:42;dweiss;We peeked at the forbidden a bit again. The difference 12 vs. 16 bytes is a result of how ordinary object pointers (OOPs) are defined -- they are a combination of object header information (oopMark) and class pointer. The class pointer is a compile time union of either a regular pointer or a compact pointer. oopMark is either 4 bytes (32 bit jvms) or 8 bytes (64 bit jvms). So:

64 bit jvm, full oops: 8 + 8 = 16
64 bit jvm, compact oops: 8 + 4 = 12
32 bit jvm: 4 + 4 = 8","16/Mar/12 12:07;uschindler;With the help of Dawid (inspecting forbidden C code *g*), we checked the actual size and how they are calculated. Based on that I changed the defaults depending on bitness (Object header is 16 on 64 bit without compact refs, array header is 24 on 64 bit).

The attached patch will use the above defaults, but tries to update them using sun.misc.Unsafe. The trick to get the object header from usafe is by declaring a dummy class extending Object with one single field. We are then using unsafe to get the fieldOffset of that field. As Dawid pointed out, the return value is identical to his investigations (8 bytes on 32 bit archs, 16 bytes on 64 bit archs and 12 bytes on compact ref 64bit archs). So RamUsageEstimator was completely wrong in the past for 64 bit architectures.

I also changed the funny switch statement (""the "" + adressSize + "" inch dick"") to assume 64 bits architecture, if addressSize >= 8.","16/Mar/12 12:09;uschindler;I would like to remove the AverageBlabla memotry model. The code inside is simply no longer useful. RamUsageEstimator simply uses the sizes returned by the JVM.","16/Mar/12 12:54;uschindler;Updated patch with the abstract and now useless MemoryModel removed.","16/Mar/12 13:04;uschindler;Robert reminded me that there is also a heavily broken custom memory estimator in MemoryIndex, too. I will look into it, too.","16/Mar/12 16:53;uschindler;Attached is a patch fixing several bugs and more:
- Removed the MemoryIndex VM class and the completely outdated and incorrect estimation there.
- Used Shai's new added methods also in Lucene's PackedInt classes
- Fixes overflows in Shai's new methods, as they can overflow if arrays are greater than 2 GB (casts to long missing)
- Fixed the up-rounding to multiples of 8 to work with longs

What's the reason why this rounding up to 8 bytes was added? I assume this information comes from somewhere, but it was added by Shai without any explanation. Is this not also dependent on the 64bitness if its 8 or 4?

Otherwise patch is ready.","16/Mar/12 17:00;rcmuir;{quote}
Removed the MemoryIndex VM class and the completely outdated and incorrect estimation there.
{quote}

thank you!!!","16/Mar/12 17:34;dweiss;Awesome job, Uwe. I think I wasn't right about that alignment of arrays -- sizeof(int) should't come up to 8. I will look into this again in the evening, it got me interested. I'll also check out the alignments, so if this patch can wait until tomorrow then we'll be more confident we get the estimates right.","16/Mar/12 21:00;dweiss;This is very interesting indeed. 

So, I used the agent hook into a running VM to dump some of the internal diagnostics, including OOP sizes, heap word alignments, etc. Here's a scoop of the results (with client-side indicated sizes on the right):

{noformat}

# 1.7, 64 bit, OOPS compressed            (client)
getOopSize: 8                             ref size = 4         
Address size: 8                           array header = 16    
Bytes per long: 8                         object header = 12   
CPU: amd64
HeapOopSize: 4
HeapWordSize: 8
IntSize: 4
getMinObjAlignmentInBytes: 8
getObjectAlignmentInBytes: 8
isCompressedOopsEnabled: true
isLP64: true


# 1.7, 64 bit, full
getOopSize: 8                             ref size = 8     
Address size: 8                           array header = 24
Bytes per long: 8                         object header = 16
CPU: amd64
HeapOopSize: 8
HeapWordSize: 8
IntSize: 4
getMinObjAlignmentInBytes: 8
getObjectAlignmentInBytes: 8
isCompressedOopsEnabled: false
isLP64: true

# 1.7, 32 bit  
getOopSize: 4                             ref size = 4     
Address size: 4                           array header = 12
Bytes per long: 8                         object header = 8
CPU: x86
HeapOopSize: 4
HeapWordSize: 4
IntSize: 4
getMinObjAlignmentInBytes: 8
getObjectAlignmentInBytes: 8
isCompressedOopsEnabled: false
isLP64: false
{noformat}

The question we asked ourselves with Uwe is why an empty array takes 24 bytes without OOP compression (that's object overhead and an int length, so should be 16 + 4 = 20)? The answer seems to be in how base offsets are calculated for arrays -- they seem to be enforced on HeapWordSize boundary and this is 8, even with OOP compressed:
{noformat}
  // Returns the offset of the first element.
  static int base_offset_in_bytes(BasicType type) {
    return header_size(type) * HeapWordSize;
  }
{noformat}
I'll spare you the detailed code but the rounding to next HeapWordSize multiple seems evident in all cases. What's even more interesting, this ""wasted"" space is not (and cannot) be used for data so even a single integer pushes the array size to the next available bound:
{noformat}
int[0] = 24
int[1] = 32   (*)
int[2] = 32
int[3] = 40
{noformat}

Finally, I could not resist to mention that object alignments... are adjustable, at least to 2^n boundaries. So you can also do this:
{noformat}
> java  -XX:-UseCompressedOops -XX:ObjectAlignmentInBytes=32 ...
Object = 32
int[0] = 32
int[1] = 32
int[2] = 32
int[3] = 64
{noformat}
Nice, huh? :) I don't think the JVM has been tested heavily for this possibility though because the code hung on me a few times if executed in that mode.","16/Mar/12 22:07;dweiss;He, he, he... this is fun, haven't been playing with Unsafe for a while and forgot how enjoyable this can be.
{code}
            Unsafe us = ...;
            byte [] dummy  = {0x11, 0x22, 0x33, 0x44};
            int []  dummy2 = {0}; // match length above.
            // Change the class of dummy to int[]...
            int klazz = us.getInt(dummy2, 8);
                        us.putInt( dummy, 8, klazz);
            // this will be ok.
            dummy2 = (int[])(Object) dummy;
            // and we can run native int accessors on a byte[] now...
            System.out.println(""> "" + Integer.toHexString(dummy2[0]));
{code}","16/Mar/12 22:08;dweiss;I think Yonik once mentioned he wanted a fast hash over byte[] -- this could be it (temporarily cast to a long[] and then revert after computations are over). Go for it, Yonik :)","16/Mar/12 22:14;uschindler;Thanks for investigation. The 8 byte object size multiplier is fixed, so the round-up method is fine.

I have been thinking about alignment things. Its a good possibility to get the object size by suming up the field sizes, but it can even be done better.

If unsafe is available and useable, we can simply get the object size (including all headers), by finding the Math.max(field offset + field type length). So the object size is the offset of the last field (with biggest offset) + its size. This value is finally rounded up to multiples of 8.

The attached patch does this.","16/Mar/12 22:19;dweiss;bq. The 8 byte object size multiplier is fixed, so the round-up method is fine.

I don't think it's ""fixed"" -- see the -XX:ObjectAlignmentInBytes=32 above. But the defaults seem to be the same on all systems.","16/Mar/12 22:57;uschindler;bq. I don't think it's ""fixed"" – see the -XX:ObjectAlignmentInBytes=32 above. But the defaults seem to be the same on all systems.

I would like to have the rounding also dynamic, but this is not possible to find out with Unsafe, at least for this I have no idea :(","17/Mar/12 08:04;dweiss;bq. at least for this I have no idea

The management factory trick mentioned by Kris works for object alignment as well:
{code}
package spikes;

import java.io.IOException;
import java.lang.management.ManagementFactory;
import java.lang.reflect.Method;
import java.util.List;

import com.sun.management.HotSpotDiagnosticMXBean;
import com.sun.management.VMOption;

public class ObAlignment
{
    private static final String HOTSPOT_BEAN_NAME = ""com.sun.management:type=HotSpotDiagnostic"";
    private static HotSpotDiagnosticMXBean hotspotMBean;
    
    private static HotSpotDiagnosticMXBean getHotSpotMBean() {
      if (hotspotMBean == null) {
        try {
          hotspotMBean = ManagementFactory.newPlatformMXBeanProxy(
            ManagementFactory.getPlatformMBeanServer(),
            HOTSPOT_BEAN_NAME,
            HotSpotDiagnosticMXBean.class);
        } catch (IOException e) {
          e.printStackTrace();
        }
      }
      return hotspotMBean;
    }

    public static void main(String [] args)
        throws Exception
    {
        // Just the object alignment.
        System.out.println(getHotSpotMBean().getVMOption(""ObjectAlignmentInBytes""));

        // Everything.
        Class<?> fc = Class.forName(""sun.management.Flag"");
        System.out.println(fc);
        Method m = fc.getDeclaredMethod(""getAllFlags"");
        m.setAccessible(true);
        List<Object> flags = (List<Object>) m.invoke(null);
        for (Object f : flags) {
            Method dm = f.getClass().getDeclaredMethod(""getVMOption"");
            dm.setAccessible(true);
            VMOption option = (VMOption) dm.invoke(f);
            System.out.println(option);
        }
    }
}
{code}

I don't think it is of much practical use for now (object alignment seems to be constant everywhere), but we could as well probe it -- if it's available why not use it.

I'd also like to add a shallow size method (which wouldn't follow the fields, just return the aligned object size). I'll be able to work on it in the evening though, not sooner.","17/Mar/12 08:26;uschindler;Too funny,
I had the same idea while at breakfast and started to implement it when you were writing your comment :-)

I will post patch soon (also with other improvements)!","17/Mar/12 08:31;uschindler;I will add a shallow parameter to the estimate method, we just dont have to dig into, so it's a simple if check.","17/Mar/12 08:56;uschindler;New patch:
- retrieve object alignment (default 8, e.g. 32bit JVMs don't report it)
- add shallow object size measurement
- add some security checks to possibly handle the ""cookie"" warning in Unsafe.objectFieldOffset() (the offsets may be ""scaled""). Current JVMs never do this, but the documentation explicitely states that the offsets may not be byte-aligned.","17/Mar/12 10:56;uschindler;Minor improvements.","17/Mar/12 14:11;dweiss;The patch looks good. I don't know if decorating IdentityHashMap to be a set adds any overhead... I was also thinking about doing a custom set impl. for this so that we know how much memory we allocate during the checking itself, but it seems to be very specific to what I need, so no worries.

One thing:
{code}
(size % NUM_BYTES_OBJECT_ALIGNMENT);
{code}
Byte alignment will be a power of 2 (that option to change it even enforces it when you start the JVM) so you can do a bitmask instead of modulo - should be slighly faster.","17/Mar/12 14:53;uschindler;I separated the shallow Object inspection to a static method, which is more cheap (no RamUsageEstimator instance is needed). The static method now only takes a Class<?> parameter and returns the size (an instance is not even needed).

I also added a diagnostic boolean, so you can query RamUsageEstimator, if the used JVM is supported (supports Hotspot diagnostics, sum.misc.Unsafe). If that is not the case, our testcase will print a warning so users cam report back (if they run the tests).

I think this is ready to commit.","17/Mar/12 14:58;uschindler;{quote}
One thing:
{code}
(size % NUM_BYTES_OBJECT_ALIGNMENT);
{code}
Byte alignment will be a power of 2 (that option to change it even enforces it when you start the JVM) so you can do a bitmask instead of modulo - should be slighly faster.
{quote}

I don't think thats really needed here :-) Speed is limited by reflection in most cases and this one calculation should not matter. Also the number is not reported back as power of 2, so I have to calc the log2 first (ok, ntz &co.), but I don't think we should actually limit that to powers of 2. Maybe another vendor has the ultimate answer of 42 for his objects?","17/Mar/12 15:02;uschindler;bq. I don't know if decorating IdentityHashMap to be a set adds any overhead

The whole problem is more that it might happen that the IdentityHashMap takes horrible amounts of memory while inspecting (think of a boxed numbers array like Byte[50000]). Speed is not important, reflection is slow :(

I have no better idea about how to detect duplicates, unfortunately. The old trick from Arrays.deepEquals() to stop when the parameter itsself is seen again, is not enough here.","17/Mar/12 15:25;uschindler;One more improvement:
The shallow Class inspection can ignore superclasses, if Unsafe is in use. As additional fields are always added at the end (otherwise casting of classes and later field access would not work inside the JVM), to find the maximum field offset we don't need to go to superclasses.

I want to commit and backport this to 3.x during the weekend.","17/Mar/12 16:40;dweiss;Reflection don't need to cost you much if you make a cache along the way. Retrieving an object's class is virtually zero cost so this would make it very efficient and the number of classes in the system is much smaller than the number of objects so it shouldn't be a problem.

bq. to find the maximum field offset we don't need to go to superclasses.

I can't imagine a situation where this wouldn't be the case although an assertion here would be nice just to make sure we're not assuming something that isn't true.


I will take a closer look at the patch again this evening and do some testing/ API flexibility based on what I have in my project. Will report on the results.
","17/Mar/12 17:30;uschindler;bq. Reflection don't need to cost you much if you make a cache along the way. Retrieving an object's class is virtually zero cost so this would make it very efficient and the number of classes in the system is much smaller than the number of objects so it shouldn't be a problem.

Would be like the reflection  cache in AttributeSource :-) But yes I was also thinking about a second IdentityHashMap<Class<?>,Long> along the way.

bq. I can't imagine a situation where this wouldn't be the case although an assertion here would be nice just to make sure we're not assuming something that isn't true.

Thats already checked in the test, who has 2 subclasses, one with no additional fields (size must be identical) and one with 2 more fields (should be >=).","17/Mar/12 17:50;shaie;Wow, what awesome improvements you guys have added !

Uwe, +1 to commit. I unassigned myself - you and Dawid definitely deserve the credit!","17/Mar/12 21:53;dweiss;Modified method naming convention: any sizeOf  is ""deep"", shallowSizeOf* is ""shallow"". Methods in RUE are now static; didn't hide the constructor though (maybe we should?).

More comments in a minute.","17/Mar/12 22:06;uschindler;Thanks for cleanup!

bq. didn't hide the constructor though (maybe we should?).

We must. Class is final and has no instance methods -> useless to have ctor. Also as previous versions in 3.x allowed instances, we should prevent this to fix incorrect usage.","17/Mar/12 22:07;dweiss;I've played with the code a bit and I've been trying to figure out a way to determine empirically ""how far off"" is the estimation from real life usage. It's not easy because RUE itself allocates memory (and not small quantities in case of complex object graphs!). I left these experiments in StressRamUsageEstimator; it is a test case -- maybe we should add @Ignore and rename it to Test*, don't know.

Anyway, the allocation seems to be measured pretty accurately. When tlabs are disabled this is a result of allocating small byte arrays for example:
{noformat}
 committed           max        estimated(allocation)
      2 MB	   48.4 MB	  16 bytes
    1.7 MB	   48.4 MB	  262.4 KB
      2 MB	   48.4 MB	  524.6 KB
    2.2 MB	   48.4 MB	    787 KB
    2.5 MB	   48.4 MB	      1 MB
    2.7 MB	   48.4 MB	    1.3 MB
      3 MB	   48.4 MB	    1.5 MB
    3.3 MB	   48.4 MB	    1.8 MB
....
   46.9 MB	   48.4 MB	   45.6 MB
   47.1 MB	   48.4 MB	   45.9 MB
   47.4 MB	   48.4 MB	   46.1 MB
   47.6 MB	   48.4 MB	   46.4 MB
   47.9 MB	   48.4 MB	   46.6 MB
   48.1 MB	   48.4 MB	   46.9 MB
{noformat}

So it's fairly ideal (committed memory is all committed memory so I assume additional data structures, classes, etc. also count in).

Unfortunately it's not always so smooth, for example jrockit's mx beans seem not to return the actual memory allocation state (and if they do, I don't understand it):
{noformat}
 committed           max        estimated(allocation)
   29.4 MB	     50 MB	  16 bytes
   29.8 MB	     50 MB	  262.5 KB
   30.2 MB	     50 MB	  524.9 KB
   30.4 MB	     50 MB	  787.3 KB
   30.8 MB	     50 MB	      1 MB
   31.1 MB	     50 MB	    1.3 MB
   31.4 MB	     50 MB	    1.5 MB
   31.7 MB	     50 MB	    1.8 MB
     32 MB	     50 MB	      2 MB
   32.4 MB	     50 MB	    2.3 MB
   32.7 MB	     50 MB	    2.6 MB
   33.1 MB	     50 MB	    2.8 MB
   33.5 MB	     50 MB	    3.1 MB
   33.8 MB	     50 MB	    3.3 MB
   34.2 MB	     50 MB	    3.6 MB
   34.5 MB	     50 MB	    3.8 MB
   34.8 MB	     50 MB	    4.1 MB
   35.2 MB	     50 MB	    4.4 MB
   35.5 MB	     50 MB	    4.6 MB
   35.7 MB	     50 MB	    4.9 MB
   36.2 MB	     50 MB	    5.1 MB
   36.4 MB	     50 MB	    5.4 MB
...
   49.6 MB	     50 MB	   47.6 MB
     50 MB	     50 MB	   47.9 MB
   49.6 MB	     50 MB	   48.2 MB
   49.9 MB	     50 MB	   48.4 MB
{noformat}

A snapshot from 32 bit HotSpot:
{noformat}
...
   25.5 MB	   48.4 MB	   24.7 MB
   25.7 MB	   48.4 MB	   24.9 MB
   25.9 MB	   48.4 MB	   25.1 MB
   26.1 MB	   48.4 MB	   25.3 MB
   26.3 MB	   48.4 MB	   25.5 MB
   26.5 MB	   48.4 MB	   25.7 MB
   26.7 MB	   48.4 MB	   25.9 MB
   26.8 MB	   48.4 MB	   26.1 MB
     27 MB	   48.4 MB	   26.4 MB
   27.2 MB	   48.4 MB	   26.6 MB
   27.4 MB	   48.4 MB	   26.8 MB
   27.7 MB	   48.4 MB	     27 MB
...
{noformat}

I see two problems that remain, but I don't think they're urgent enough to be addressed now:
- the stack easily overflows if the graph of objects has long chains. This is demonstrated in the test case (uncomment ignore annotation).
- there is a fair amount of memory allocation going on in the RUE itself. If one _knows_ the graph of an object's dependencies is a tree then the memory cost could be decreased to zero (because we wouldn't need to remember which objects we've seen so far).
- we could make RUE an object again (resign from static methods) and have a cache of classes and class-fields to avoid reflective accesses over and over. If one performed estimations over and over then such a  RUE instance would have an initial cost, but then would be running smoother.

Having said that, I'm +1 for committing this in if you agree with the changes I've made (I will be a pain in the arse about that naming convention discriminating between shallow vs. deep sizeOf though :).","17/Mar/12 22:10;dweiss;Oh, one thing that springs to my mind is that we could have an automatically generated class with nested static classes with a random arrangement of all sorts of fields (in all sorts of configurations) and use a similar empirical benchmark to the one I did on small byte arrays but on these objects. This would show if we're estimating object field offsets and sizes correctly. 

I wouldn't go into deep object structures though -- I've tried this and it's hard to tell what the allocation is and what the overhead/ noise of measurement is.","17/Mar/12 22:43;uschindler;Hi,

I am fine with the patch for now, changed in this patch:
- Hidden ctor
- Cleaned up test to use static import consequently

The stress test is an testcase, but not automatically executed (you have to explicitely do that with -Dtestcase=...). I think thats wanted, right? Otherwise we should rename, but its also noisy and slow.","18/Mar/12 10:53;uschindler;Final patch: I removed some code duplication and improved exception handling for the reflection while iterating class tree. Simply  suppressing is a bad idea, as the resulting size would be underdetermined.

I will commit this later this evening and then backport to 3.x.","18/Mar/12 11:15;shaie;Thanks Uwe !","18/Mar/12 11:58;uschindler;Javadocs fixes.","18/Mar/12 14:28;dweiss;Looks good to me, thanks Uwe.","18/Mar/12 14:59;uschindler;Committed trunk revision: 1302133

I will now backport with deprecations and add CHANGES.txt later!","18/Mar/12 15:56;uschindler;Patch for 3.x including backwards layer (deprecated instances of RUE + string interning support). MemoryModels were nuked completely (will add comment to backwards changes).","18/Mar/12 16:20;uschindler;Committed 3.x revision: 1302152

CHANGES.txt committed in revisions: 1302155 (3.x), 1302156 (trunk)

Thanks Dawid and Shai!

Dawid and I will now look into donating this masterpiece to maybe Apache Commons Lang or similar, as it's of general use.","22/Mar/12 10:29;dweiss;I've been experimenting a bit with the new code. Field offsets for three classes in a hierarchy with unalignable fields (byte, long combinations at all levels). Note unaligned reordering of byte field in JRockit - nice.

{noformat}
JVM: [JVM: HotSpot, Sun Microsystems Inc., 1.6.0_31] (compressed OOPs)
@12  4 Super.superByte
@16  8 Super.subLong
@24  8 Sub.subLong
@32  4 Sub.subByte
@36  4 SubSub.subSubByte
@40  8 SubSub.subSubLong
@48    sizeOf(SubSub.class instance)

JVM: [JVM: HotSpot, Sun Microsystems Inc., 1.6.0_31] (normal OOPs)
@16  8 Super.subLong
@24  8 Super.superByte
@32  8 Sub.subLong
@40  8 Sub.subByte
@48  8 SubSub.subSubLong
@56  8 SubSub.subSubByte
@64    sizeOf(SubSub.class instance)


JVM: [JVM: J9, IBM Corporation, 1.6.0]
@24  8 Super.subLong
@32  4 Super.superByte
@36  4 Sub.subByte
@40  8 Sub.subLong
@48  8 SubSub.subSubLong
@56  8 SubSub.subSubByte
@64    sizeOf(SubSub.class instance)

JVM: [JVM: JRockit, Oracle Corporation, 1.6.0_26] (64-bit JVM!)
@ 8  8 Super.subLong
@16  1 Super.superByte
@17  7 Sub.subByte
@24  8 Sub.subLong
@32  8 SubSub.subSubLong
@40  8 SubSub.subSubByte
@48    sizeOf(SubSub.class instance)
{noformat}","22/Mar/12 11:02;uschindler;Thanks for the insight.

When thinking about the reordering, I am a littel bit afraid about the ""optimization"" in the shallow sizeOf(Class<?>). This optimiaztion does not recurse to superclasses, as it assumes, that all field offsets are greater than those of the superclass, so finding the maximum does not need to recurse up (so it early exits).

This is generally true (also in the above printout), but not guaranteed. E.g. JRockit does it partly (it reuses space inside the superclass area to locate the byte from the subclass). In the above example still the order of fields is always Super-Sub-SubSub, but if the ordeing in the JRockit example would be like:

{noformat}
@ 8  1 Super.superByte
@ 9  7 Sub.subByte
@16  8 Super.subLong
@24  8 Sub.subLong
@32  8 SubSub.subSubLong
@40  8 SubSub.subSubByte
@48    sizeOf(SubSub.class instance)
{noformat}

The only thing the JVM cannot change is field offsets between sub classes (so the field offset of the superclass is inherited), but it could happen that *new* fields are located between super's fields (see above - it's unused space). This would also allow casting and so on (it's unused space in superclass). Unfortunately with that reordering the maximum field offset in the subclass is no longer guaranteed to be greater.

I would suggest that we remove the ""optimization"" in the shallow class size method. It's too risky in my opinion to underdetermine the size, because the maximum offset in the subclass is < the maximum offset in the superclass.

I hope my explanation was understandable... :-)

Dawid, what do you thing, should we remove the ""optimization""? Patch is easy.","22/Mar/12 11:09;dweiss;bq. I hope my explanation was understandable... 

Perfectly well. Yes, I agree, it's possible to fill in the ""holes"" packing them with fields from subclasses. It would be a nice vm-level optimization in fact! 

I'm still experimenting on this code and cleaning/ adding javadocs -- I'll patch this and provide a complete patch once I'm done, ok?","22/Mar/12 11:11;uschindler;OK. All you have to remove is the if (fieldFound && useUnsafe) check and always recurse. fieldFound itsself can also be removed.","22/Mar/12 11:17;uschindler;JRockit could even compress like this, it would still allow casting as all holes are solely used by one sub-class:

{noformat}
@ 8  1 Super.superByte
@ 9  1 Sub.subByte
@10  6 SubSub.subSubByte
@16  8 Super.subLong
@24  8 Sub.subLong
@32  8 SubSub.subSubLong
@40    sizeOf(SubSub.class instance)
{noformat}","22/Mar/12 11:24;dweiss;Maybe it does such things already. I didn't check extensively.","22/Mar/12 11:26;uschindler;We have to remove the shallow size optimization in 3.x and trunk.","22/Mar/12 13:20;dweiss;I confirmed that this packing indeed takes place. Wrote a pseudo-random test with lots of classes and fields. Here's an offender on J9 for example (Wild_{inheritance-level}_{field-number}):
{noformat}
@24  4 Wild_0_92.fld_0_0_92
@28  4 Wild_0_92.fld_1_0_92
@32  4 Wild_0_92.fld_2_0_92
@36  4 Wild_0_92.fld_3_0_92
@40  4 Wild_0_92.fld_4_0_92
@44  4 Wild_0_92.fld_5_0_92
@48  4 Wild_0_92.fld_6_0_92
@52  4 Wild_2_5.fld_0_2_5
@56  8 Wild_1_85.fld_0_1_85
@64  8 Wild_1_85.fld_1_1_85
@72    sizeOf(Wild_2_5 instance)
{noformat}

HotSpot and JRockit don't seem to do this (at least it didn't fail on the example).
","22/Mar/12 13:45;uschindler;Thanks, in that case shallowSizeOf(Wild_2_5.class) would incorrectly return 56 because of the short-circuit - so let's fix this.","22/Mar/12 14:07;dweiss;Yep, that assumption was wrong -- indeed:
{noformat}
WildClasses.Wild_2_5 wc = new WildClasses.Wild_2_5();
wc.fld_6_0_92 = 0x1122;
wc.fld_0_2_5 = Float.intBitsToFloat(0xa1a2a3a4);
wc.fld_0_1_85 = Double.longBitsToDouble(0xb1b2b3b4b5b6b7L);
System.out.println(ExpMemoryDumper.dumpObjectMem(wc));
{noformat}
results in:
{noformat}
0x0000 b0 3d 6f 01 00 00 00 00 0e 80 79 01 00 00 00 00
0x0010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
0x0020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
0x0030 22 11 00 00 a4 a3 a2 a1 b7 b6 b5 b4 b3 b2 b1 00
0x0040 00 00 00 00 00 00 00 00
{noformat}
And you can see they are reordered and longs are aligned.

I'll provide a cumulative patch of changes in the evening, there's one more thing I wanted to add (cache of fields) because this affects processing speed.","22/Mar/12 20:59;dweiss;Ok, I admit J9 is fascinating... ;) How much memory does this take?
{code}
class X {
  byte a = 0x11;
  byte b = 0x22;
}
{code}
Here is the memory layout:
{code}
[JVM: IBM J9 VM, 2.6, IBM Corporation, IBM Corporation, 1.7.0]
0x0000 00 b8 21 c4 5f 7f 00 00 00 00 00 00 00 00 00 00
0x0010 11 00 00 00 22 00 00 00
@16  4 Super.b1
@20  4 Super.b2
@24    sizeOf(Super instance)
{code}

I don't think I screwed up anything. It really is 4 byte alignment _on all fields_.","23/Mar/12 10:40;dweiss;Don't be scared by the size of this patch -- it contains a lot of generated code in WildClasses.

Improvements:
- size estimation is not recursive (which led to stack overflows quite easily on more complex object graphs),
- decreased memory consumption by using a custom impl. of an identity object set. 
- added a cache of resolved class information (ref. fields, shallow size).
- removed the optimization of counting only subclass field offsets because fields can be packed (J9).
- added more verbose information about unsupported JVM features. J9 doesn't have the MX bean for example (and does dump this).

The above changes also speed up the entire processing.","23/Mar/12 12:24;dweiss;Added a test case for identity has set, removed constants, removed wild classes.","23/Mar/12 12:59;uschindler;I think the patch is now fine! I will commit it later and backport to 3.x.","23/Mar/12 14:29;dweiss;Thanks Uwe. I'll be working in the evening again but if you're faster go ahead and commit it in.","23/Mar/12 19:17;uschindler;Committed trunk revision: 1304485, 1304513, 1304564
Committed 3.x revision: 1304565","23/Mar/12 21:47;dweiss;I've been thinking how one can assess the estimation quality of the new code. I came up with this:
- I allocate an Object[] half the size of estimated maximum available RAM (just to make sure all objects will fit without the need to reallocate),
- I precompute shallow sizes for instances of all ""wild classes"" (classes with random fields, including arrays).
- I then fill in the ""vault"" array above with random instances of wild classes, summing up the estimated size UNTIL I HIT OOM.
- Once I git OOM I know how much we actually allocated vs. how much space we thought we did allocate.

The results are very accurate on HotSpot if one is using serial GC. For example:
{noformat}
[JVM: Java HotSpot(TM) 64-Bit Server VM, 20.4-b02, Sun Microsystems Inc., Sun Microsystems Inc., 1.6.0_29]
Max: 483.4 MB, Used: 698.9 KB, Committed: 123.8 MB
Expected free: 240.9 MB, Allocated estimation: 240.8 MB, Difference: -0.05% (113.6 KB)
{noformat}

If one runs with a parallel GC things do get out of hand because the GC is not keeping up with allocations (although I'm not sure how I should interpret this because we only allocate; it's not possible to free any space -- maybe there are different GC pools or something):
{noformat}
[JVM: Java HotSpot(TM) 64-Bit Server VM, 20.4-b02, Sun Microsystems Inc., Sun Microsystems Inc., 1.6.0_29]
Max: 444.5 MB, Used: 655.4 KB, Committed: 122.7 MB
Expected free: 221.5 MB, Allocated estimation: 174.2 MB, Difference: -21.34% (47.3 MB)
{noformat}

JRockit:
{noformat}
[JVM: Oracle JRockit(R), R28.1.4-7-144370-1.6.0_26-20110617-2130-windows-x86_64, Oracle Corporation, Oracle Corporation, 1.6.0_26]
Max: 500 MB, Used: 3.5 MB, Committed: 64 MB
Expected free: 247.7 MB, Allocated estimation: 249.5 MB, Difference: 0.74% (1.8 MB)
{noformat}

I think we're good. If somebody wishes to experiment, the spike is here:
https://github.com/dweiss/java-sizeof
{noformat}
mvn test
mvn dependency:copy-dependencies
java -cp target\classes:target\test-classes:target\dependency\junit-4.10.jar \
  com.carrotsearch.sizeof.TestEstimationQuality
{noformat}","26/Mar/12 08:27;dweiss;For historical records: the previous implementation of RamUsageEstimator was off by anything between 3% (random size objects, including arrays) to 20% (objects smaller than 80 bytes). Again -- these are ""perfect scenario"" measurements with empty heap and max. allocation until OOM, with a serial GC. With a concurrent and parallel GCs the memory consumption estimation is still accurate but it's nearly impossible to tell when an OOM will occur or how the GC will manage the heap space. ","26/Mar/12 08:32;uschindler;That's true. But you can still get the ""unreleaseable allocation"", so the size of the non-gc-able object graph. If GC does not free the objects after release fast-enough, it will still do it once memory gets low. But the allocated objects with hard refs are not releaseable.

So I think it's fine for memory requirement purposes. If you want real heap allocation, you must use instrumentation.","26/Mar/12 08:45;dweiss;I didn't say it's wrong -- it is fine and accurate. What I'm saying is that it's not really suitable for predictions; for answering questions like: how many objects of a given type/ types can I allocate before an OOM hits me? It doesn't really surprise me that much, but it would be nice. For measuring already allocated stuff it's more than fine of course."
SpanFilter should not extend Filter,LUCENE-968,12374910,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Invalid,,paul.elschot@xs4all.nl,paul.elschot@xs4all.nl,29/Jul/07 21:51,10/May/13 10:43,30/Sep/19 08:38,30/Jul/07 16:23,2.3,,,,,,,,,,2.3,,,,core/search,,,0,,,,All tests pass with the patch applied.,,,,,,,,,,,,,,,,"29/Jul/07 21:52;paul.elschot@xs4all.nl;SpanFilter20070729.patch;https://issues.apache.org/jira/secure/attachment/12362738/SpanFilter20070729.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-07-30 00:57:39.27,,,false,,,,,,,,,,,,,,,12775,,,Mon Jul 30 16:23:12 UTC 2007,New,Patch Available,,,,,,"0|i04zvz:",27079,,,,,,,,,"29/Jul/07 21:52;paul.elschot@xs4all.nl;Removes ""extends Filter""","30/Jul/07 00:57;gsingers;Why not?  It can be used as a Filter and if it is cached, it can then also be used to get the span information from the filter.","30/Jul/07 16:23;paul.elschot@xs4all.nl;Ok, I missed that possible use as a Filter. I'm busy with LUCENE-584, and I could not figure out how to deal with this one.
Since it is a Filter, I'll include it in there as one of the currently present Filters.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FST serialization and deserialization from plain DataInput/DataOutput streams.,LUCENE-3011,12503274,,New Feature,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,04/Apr/11 11:44,10/May/13 10:43,30/Sep/19 08:38,05/Apr/11 09:14,,,,,,,,,,,4.0-ALPHA,,,,core/other,,,0,,,,Currently the automaton can be saved only to a Directory instance (IndexInput/ IndexOutput).,,,,,,,,,,,,SOLR-2378,,,,"04/Apr/11 13:26;rcmuir;LUCENE-3011.patch;https://issues.apache.org/jira/secure/attachment/12475355/LUCENE-3011.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-04 12:07:13.508,,,false,,,,,,,,,,,,,,,10882,,,Tue Apr 05 09:14:04 UTC 2011,New,,,,,,,"0|i04n1r:",24999,,,,,,,,,"04/Apr/11 12:07;mikemccand;+1

Maybe one way to do this is to change FST to take DataInput/Output (not IndexInput/Output) then make  DataInputFromInputStream/DataOutputToOutputStream wrapper classes?","04/Apr/11 12:40;dweiss;I tried that before, but CodecUtil.writeHeader() requires IndexOutput for example (and does make use of its methods internally).","04/Apr/11 13:13;rcmuir;But it doesn't really use it right? its just abusing getFilePointer() to check that you wrote all ascii?

It could, perhaps more clearly, convert to bytes itself and check: (either by checking the length of the bytes or using US-ASCII charset with onUnmappableCharacter and friends set to CodingErrorAction.REPORT?), and then use writeBytes?
","04/Apr/11 13:26;rcmuir;Dawid: does this help?","04/Apr/11 14:25;dweiss;Yes, it does -- thanks Robert. I will provide a patch that fixes it all, but I'm stuck doing some perf. analysis in fstlookup.","05/Apr/11 09:14;dweiss;Committed to the trunk [patch provided by Robert Muir].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Upgrade JUnit to 4.10, refactor state-machine of detecting setUp/tearDown call chaining.",LUCENE-3762,12541867,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,08/Feb/12 19:42,10/May/13 10:43,30/Sep/19 08:38,15/Feb/12 10:22,,,,,,,,,,,3.6,4.0-ALPHA,,,,,,0,,,,"Both Lucene and Solr use JUnit 4.7. I suggest we move forward and upgrade to JUnit 4.10 which provides several infrastructural changes (serializable Description objects, class-level rules, various tweaks). JUnit 4.10 also changes (or fixes, depends how you look at it) the order in which @Before/@After hooks and @Rules are applied. This makes the old state-machine in LuceneTestCase fail (because the order is changed).

I rewrote the state machine and used a different, I think simpler, although Uwe may disagree :), mechanism in which the hook methods setUp/ tearDown are still there, but they are empty at the top level and serve only to detect whether subclasses chain super.setUp/tearDown properly (if they override anything).

In the long term, I would love to just get rid of public setup/teardown methods and make them private (so that they cannot be overriden or even seen by subclasses) but this will require changes to the runner itself.",,,,,,,,,,,,,,,,"14/Feb/12 11:28;dweiss;LUCENE-3762-backport.patch;https://issues.apache.org/jira/secure/attachment/12514479/LUCENE-3762-backport.patch","10/Feb/12 22:44;dweiss;LUCENE-3762.patch;https://issues.apache.org/jira/secure/attachment/12514169/LUCENE-3762.patch","09/Feb/12 09:50;dweiss;LUCENE-3762.patch;https://issues.apache.org/jira/secure/attachment/12513936/LUCENE-3762.patch","08/Feb/12 19:44;dweiss;LUCENE-3762.patch;https://issues.apache.org/jira/secure/attachment/12513852/LUCENE-3762.patch",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-02-08 20:16:08.612,,,false,,,,,,,,,,,,,,,227154,,,Wed Feb 15 09:55:21 UTC 2012,New,Patch Available,,,,,,"0|i04igf:",24255,,,,,,,,,"08/Feb/12 19:44;dweiss;git patch (binary data updates) corresponding to this:

https://github.com/dweiss/lucene_solr/commit/b0608e21abcaebb7d39c769689e3c0e987b741f9

All tests pass for me, but I would love if somebody else also tried (on a non-windows machine?).

","08/Feb/12 20:16;rcmuir;looks great, lets upgrade!

{quote}
JUnit 4.10 also changes (or fixes, depends how you look at it) the order in which @Before/@After hooks and @Rules are applied.
{quote}

Does this mean if we assumeTrue(message, ...) inside a setUp() that we will now actually see the message? :) ","08/Feb/12 20:42;dweiss;I just committed a test case that verifies this to GitHub. The message gets propagated properly to RunListeners (as testAssumptionFailure). I don't see anything in Lucene's test output when I run a test with such an assumption, but I guess it has to be possible (if nothing else, then by capturing that with a custom RunListener).","08/Feb/12 20:48;dweiss;Yes, it would be possible. Either at the runner level (worse) or by using a custom rule and not a TestWatcher subclass. What TestWatcher does is this:
{code}
			public void evaluate() throws Throwable {
				starting(description);
				try {
					base.evaluate();
					succeeded(description);
				} catch (AssumptionViolatedException e) {
					throw e;
				} catch (Throwable t) {
					failed(t, description);
					throw t;
				} finally {
					finished(description);
				}
			}
{code}
so it effectively skips assumption-failed tests and they're not passed to LuceneTestCase. Doable, but I think worth a separate issue?","08/Feb/12 20:55;rcmuir;its definitely a separate issue... just curiousity!","08/Feb/12 21:13;uschindler;bq. I rewrote the state machine and used a different, I think simpler, although Uwe may disagree 

No, no, the state machine was Robert's work I only helped on improvements :-)

+1","09/Feb/12 08:15;dweiss;Just to clarify: I don't think the state machine was wrong, it just made assumptions that don't hold in JUnit4.10 (the order of calls). I decided to remove it because I think there is a cleaner way of ensuring setup/teardown was properly chained if overriden. 

I'll commit this one in to the trunk and we'll see if it breaks anything on Jenkins (I don't think it should, it doesn't locally).","09/Feb/12 08:58;dweiss;There are functional differences between TestWatcher (before) and TestWatchman (current) -- assumptions are no longer propagated as failures and the code in LTC.intercept() no longer applies:

{code}
    protected void failed(Throwable e, Description description) {
      // org.junit.internal.AssumptionViolatedException in older releases
      // org.junit.Assume.AssumptionViolatedException in recent ones
      if (e.getClass().getName().endsWith(""AssumptionViolatedException"")) {
        if (e.getCause() instanceof _TestIgnoredException)
{code}

I'll write tests to cover these and rewrite the interceptor explicitly as a @Rule so that we don't rely on JUnit's implementation with regard as to what is considered what.

","09/Feb/12 09:50;dweiss;An updated patch with more serious refactorings of LTC.","09/Feb/12 09:54;dweiss;If we're changing JUnit perhaps it's worth upgrading the infrastructure a bit to make things cleaner. I refactored all the hooks into a ruleset so that their nesting order is explicit:
{code}
  @Rule
  public final TestRule ruleChain = RuleChain
    .outerRule(new RememberThreadRule())
    .around(new TestResultInterceptorRule())
    .around(new InternalSetupTeardownRule())
    .around(new SubclassSetupTeardownRule());
{code}
So, subclasses (setup/teardown) run inside, surrounded by internal cleanups, surrounded by test result tracker, surrounded by current thread remembering. I also removed _TestIgnoredException in favor of a subclass of AssumptionIgnoredException - this removes some conditional checks and unwinding code.

I added some tests to detect the expected behavior of LTC (what Robert mentioned); I would feel great if we check that all the expectations are covered before we commit this in -- if you can post a simple class along with: ""this should result in this and that"" I'll update the tests. There are examples of such expectations in the patch (static classes and tests inside TestSetupTeardownMethods class).","09/Feb/12 09:57;dweiss;Didn't run the tests yet, doing it in the background.","09/Feb/12 10:10;dweiss;All tests pass for me with this patch. I didn't attach a binary patch this time, the patched version is at github:
https://github.com/dweiss/lucene_solr (junit4 branch).
","10/Feb/12 12:10;rcmuir;about the assumes() etc from setup, in general exceptions/assumes, I think we would like them to be treated the same whether they happen in the actual test method body or setup or teardown?

So like today, the buggy behavior is that if an assumption fails from a test method itself, we get a message to stderr:
NOTE: Assume failed in 'testFoo' (ignored): Some message explaining why this is ok!

But, if it fails in setup, we get no message at all!

The reason I think it was hard was because of how the TestWatcher didn't get an event if it failed in setup, so we didnt have a clean way to 
do this... but maybe its something we can fix in junit 4.8+ (doesn't need to be done on this issue!)
","10/Feb/12 15:14;dweiss;This already works on the branch I think, but I will re-check. I advanced junit4 branch and integrated junit4 parallel balanced runner instead of the default ANT's junit and previous set of macros. The code for this patch lives under LUCENE-3762.

I'll look for corner cases tonight and commit this in. Alternatively we could set up a parallel jenkins build and commit this on a branch to see if everything is all right?","10/Feb/12 22:44;dweiss;Updated patch with tests of what's emitted and when.","10/Feb/12 22:47;dweiss;I checked assume/fail/error in combination with all the possible execution points. LuceneTestCase rule wrapper emits (as could be predicted) the right note for everything but:
- static initializers,
- @BeforeClass
- initializers (constructor)
- @AfterClass

These cases are handled outside of @Rule scope and should be handled by JUnit and propagated as failures to report listeners.","10/Feb/12 22:48;dweiss;bq. Does this mean if we assumeTrue(message, ...) inside a setUp() that we will now actually see the message? 

So, the answer to this is yes, you will.","10/Feb/12 22:49;dweiss;I plan to commit this in on Monday (so that I'm at the computer in case something breaks) unless there are objections. ","14/Feb/12 10:52;dweiss;I've committed to the trunk and I have a backport of this but I started to wonder if it's a good idea to apply it on 3x -- this may cause issues with backport testing, if not anything else. Thoughts?","14/Feb/12 11:28;dweiss;Backport patch. This removes state machine from backport/LuceneTestCase so that backport tests can pass.","14/Feb/12 12:06;rcmuir;+1 for 3.x too. 
","14/Feb/12 23:53;sarowe;Dawid, today I've seen the following test reproduction message (from Maven, but running Lucene/Solr tests under Maven has caused this before):

{noformat}
NOTE: reproduce with: ant test -Dtestcase=UIMABaseAnalyzerTest -Dtestmethod=testRandomStrings(org.apache.lucene.analysis.uima.UIMABaseAnalyzerTest) -Dtests.seed=2be0c24a1df9b25e:-42f203968285c6ed:5f8c85cdbae32724 -Dargs=""-Dfile.encoding=Cp1252""
{noformat}

That is, the parenthetical class name after the method in the {{-Dtestmethod=...}} string doesn't work - you have to strip this out in order to actually use the given cmdline.

Am I right in assuming that LUCENE-3762 is the source of this behavior change?","15/Feb/12 09:55;dweiss;Good catch, Steve -- yes, I might have introduced it as part of the refactoring. JUnit has deprecated MethodRule in favor of TestRule and the latter one doesn't come with a FrameworkMethod... It's weird.

I will reopen this issue and apply a fix.","15/Feb/12 09:55;dweiss;Reopening -- method names in reproduction report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexWriter.readerPool create new segmentReader outside of sync block,LUCENE-1726,12429239,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,mikemccand,jasonrutherglen,jasonrutherglen,30/Jun/09 22:23,10/May/13 10:43,30/Sep/19 08:38,24/Jan/11 21:35,2.4.1,,,,,,,,,,4.0-ALPHA,,,,core/index,,,0,,,,"I think we will want to do something like what field cache does
with CreationPlaceholder for IndexWriter.readerPool. Otherwise
we have the (I think somewhat problematic) issue of all other
readerPool.get* methods waiting for an SR to warm.

It would be good to implement this for 2.9.",,172800,172800,,0%,172800,172800,,,,,,,,,"09/Jul/09 11:23;mikemccand;LUCENE-1726.patch;https://issues.apache.org/jira/secure/attachment/12413000/LUCENE-1726.patch","08/Jul/09 20:37;mikemccand;LUCENE-1726.patch;https://issues.apache.org/jira/secure/attachment/12412908/LUCENE-1726.patch","08/Jul/09 00:30;jasonrutherglen;LUCENE-1726.patch;https://issues.apache.org/jira/secure/attachment/12412809/LUCENE-1726.patch","08/Jul/09 00:04;jasonrutherglen;LUCENE-1726.patch;https://issues.apache.org/jira/secure/attachment/12412804/LUCENE-1726.patch","06/Jul/09 18:54;jasonrutherglen;LUCENE-1726.patch;https://issues.apache.org/jira/secure/attachment/12412643/LUCENE-1726.patch","02/Jul/09 21:54;jasonrutherglen;LUCENE-1726.patch;https://issues.apache.org/jira/secure/attachment/12412424/LUCENE-1726.patch","08/Jul/09 16:46;jasonrutherglen;LUCENE-1726.trunk.test.patch;https://issues.apache.org/jira/secure/attachment/12412884/LUCENE-1726.trunk.test.patch",,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2009-07-06 15:04:01.554,,,false,,,,,,,,,,,,,,,12033,,,Mon Jul 13 17:48:55 UTC 2009,New,,,,,,,"0|i04v6v:",26318,,,,,,,,,"02/Jul/09 21:54;jasonrutherglen;We don't block accessing readers in the IW.readerPool when a new
segmentReader is being warmed/instantiated. This is important
when new segmentReaders on large new segments are being accessed
for the first time. Otherwise today IW.getReader may wait while
the new SR is being created.

* IW.readerPool map values are now of type MapValue

* We synchronize on the MapValue in methods that access the SR

* synchronize for the entire readerPool.get method is removed.

* All tests pass 
","06/Jul/09 15:04;mikemccand;Can we make the MapValue strongly typed?  Eg name it SegmentReaderValue, and it has a single member ""SegmentReader reader"".

getIfExists has duplicate checks for null (mv != null is checked twice and mv.value != null too).

I think there is a thread hazard here, in particular a risk that one thread decrefs a reader just as another thread is trying to get it, and the reader in fact gets closed while the other thread has an mv.reader != null and illegally increfs that.  I think you'll have to do the sr.incRef inside the synchronized(this), but I don't think that entirely resolves it.

I'm going to move this out out 2.9; I don't think it should block it.","06/Jul/09 18:54;jasonrutherglen;* New SRMapValue is strongly typed

* All tests pass

{quote}I think there is a thread hazard here, in particular a
risk that one thread decrefs a reader just as another thread is
trying to get it, and the reader in fact gets closed while the
other thread has an mv.reader != null and illegally increfs
that. I think you'll have to do the sr.incRef inside the
synchronized(this), but I don't think that entirely resolves
it.{quote}

Are you referring to a decref on a reader outside of IW? The
asserts we have did help in catching synchronization errors.
It's unclear to me how to recreate the use case described such
that it breaks things. We need a test case that fails with the
current patch?","06/Jul/09 22:18;mikemccand;The hazard is something like this, for a non-NRT IndexWriter (ie, no
pooling):

  * Thread #1 is a merge; it checks out a reader & starts running

  * Thread #2 is applyDeletes (or opening an NRT reader); it calls
    readerPool.get, enters the first sync block to pull out the
    SRMapValue which has non-null reader, then leaves the sync block

  * Thread #1 calls release, which decRefs the reader & closes it

  * Thread #2 resumes, sees it has a non-null mv.reader and incRefs
    it, which is illegal (reader was already closed).
","06/Jul/09 22:43;jasonrutherglen;Shouldn't we be seeing an exception in TestStressIndexing2 (or
another test class) when the mv.reader.incRef occurs and the
reader is already closed?","07/Jul/09 00:08;mikemccand;Yes, we should eventually see a failure; I think it's just rare.  Maybe try making a new test that constantly indexes docs, w/ low merge factor & maxBufferedDocs (so lots of flushing/merging happens) in one thread and constantly opens an NRT reader in another thread, to tease it out?","07/Jul/09 02:28;jasonrutherglen;When I moved the sync block around in readerPool.get, tests
would fail and/or hang. I'm not sure yet where we'd add the
sync(this) block. 

I'll work on reproducing the above mentioned issue, thanks for
the advice.","08/Jul/09 00:04;jasonrutherglen;Added a test case (for now a separate test class) that runs for
5 minutes, mergeFactor 2, maxBufferedDocs 10, 4 threads
alternately adding and deleting docs. I haven't seen the error
we're looking for yet. CPU isn't maxing out (probably should,
indicating possible blocking?) and may need to allow it run
longer?","08/Jul/09 00:30;jasonrutherglen;Each thread in the test only performs adds or deletes (rather than both) and now we get a ""MockRAMDirectory: cannot close: there are still open files"" exception.  ","08/Jul/09 16:46;jasonrutherglen;I tried the test on trunk and get the same error. They're all
docstore related files so maybe extra doc stores are being
opened?

{code} 
   [junit] MockRAMDirectory: cannot close: there are still open
files: {_s4.fdt=2, _g2.fdx=2, _s4.fdx=2, _g2.tvf=2, _dw.fdx=2,
_g2.tvd=2, _g2.tvx=2, _ks.tvf=2, _n9.tvx=2, _ks.tvx=2,
_n9.fdx=2, _ks.fdx=2, _dw.cfx=1, _n9.tvf=2, _cp.cfx=1,
_s4.tvf=2, _dw.tvx=2, _87.fdx=2, _fr.tvx=2, _87.tvf=2,
_fr.tvd=2, _87.fdt=2, _ks.tvd=2, _s4.tvd=2, _dw.tvd=2,
_n9.fdt=2, _g2.fdt=2, _87.tvd=2, _fr.fdt=2, _dw.fdt=2,
_dj.cfx=1, _s4.tvx=2, _ks.fdt=2, _n9.tvd=2, _fr.tvf=2,
_fr.fdx=2, _dw.tvf=2, _87.tvx=2} [junit]
java.lang.RuntimeException: MockRAMDirectory: cannot close:
there are still open files: {_s4.fdt=2, _g2.fdx=2, _s4.fdx=2,
_g2.tvf=2, _dw.fdx=2, _g2.tvd=2, _g2.tvx=2, _ks.tvf=2,
_n9.tvx=2, _ks.tvx=2, _n9.fdx=2, _ks.fdx=2, _dw.cfx=1,
_n9.tvf=2, _cp.cfx=1, _s4.tvf=2, _dw.tvx=2, _87.fdx=2,
_fr.tvx=2, _87.tvf=2, _fr.tvd=2, _87.fdt=2, _ks.tvd=2,
_s4.tvd=2, _dw.tvd=2, _n9.fdt=2, _g2.fdt=2, _87.tvd=2,
_fr.fdt=2, _dw.fdt=2, _dj.cfx=1, _s4.tvx=2, _ks.fdt=2,
_n9.tvd=2, _fr.tvf=2, _fr.fdx=2, _dw.tvf=2, _87.tvx=2} [junit]
	at
org.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.j
ava:278) [junit] 	at
org.apache.lucene.index.Test1726.testIndexing(Test1726.java:48)
[junit] 	at
org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java
:88)
{code}","08/Jul/09 17:58;mikemccand;Hmm... I'll dig into this test case.","08/Jul/09 18:12;jasonrutherglen;Mike,

I was wondering if you can recommend techniques or tools for
debugging this type of multithreading issue? (i.e. how do you go
about figuring this type of issue out?) ","08/Jul/09 19:07;mikemccand;I don't have any particular tools...

First I simplify the test as much as possible while still hitting the
failure (eg this failure happens w/ only 2 threads), then see if the
error will happen if I turn on IndexWriter's infoStream (it doesn't
for this, so far).  If so, I scrutinize the series of events to find
the hazard; else, I turn off infoStream and add back in a small number
of prints, as long as failure still happens.

Often I use a simple Python script that runs the test over & over
until a failure happens, saving the log, and then scrutinize that.

It's good to start with a rough guess, eg this failure is w/ only doc
stores so it seems likely the merging logic that opens doc stores just
before kicking off the merge may be to blame.
","08/Jul/09 20:37;mikemccand;OK the problem happens when a segment is first opened by a merge that
doesn't need to merge the doc stores; later, an NRT reader is opened
that separately opens the doc stores of the same [pooled]
SegmentReader, but then it's the merge that closes the read-only clone
of the reader.

In this case the separately opened (by the NRT reader) doc stores are
not closed by the merge thread.  It's the mirror image of LUCENE-1639.

I've fixed it by pulling all shared readers in a SegmentReader into a
separate static class (CoreReaders).  Cloned SegmentReaders share the
same instance of this class so that if a clone later opens the doc
stores, any prior ancestor (that the clone was created from) would
also close those readers if it's the reader to decRef to 0.

I did something similar for LUCENE-1609 (which I'll now hit conflicts
on after committing this... sigh).

I plan to commit in a day or so.
","08/Jul/09 21:23;jasonrutherglen;The test now passes, needs to go in the patch, perhaps in
TestIndexWriterReader? Great work on this, it's easier to
understand SegmentReader now that all the shared objects are in
one object (CoreReaders). It should make debugging go more
smoothly. 

Is there a reason we're not synchronizing on SR.core in
openDocStores? Couldn't we synchronize on core for the cloning
methods? ","08/Jul/09 22:25;mikemccand;bq. Is there a reason we're not synchronizing on SR.core in openDocStores?

I was going to say ""because IW sychronizes"" but in fact it doesn't,
properly, because when merging we go and open doc stores in
unsynchronized context.  So I'll synchronize(core) in
SR.openDocStores.

bq. Couldn't we synchronize on core for the cloning methods?

I don't think that's needed?  The core is simply carried over to the
newly cloned reader.

","09/Jul/09 03:04;jasonrutherglen;{quote}I don't think that's needed? The core is simply carried
over to the newly cloned reader.{quote}

Right however wouldn't it be somewhat cleaner to sync on core
for all clone operations given we don't want those to occur
(external to IW) at the same time? Ultimately we want core to be
the controller of it's resources rather than the SR being cloned?

I ran the test with the SRMapValue sync code, (4 threads) with
the sync on SR.core in openDocStore for 10 minutes, 2 core
Windows XML laptop Java 6.14 and no errors. Then same with 2
threads for 5 minutes and no errors. I'll keep on running it to
see if we can get an error.

I'm still a little confused as to why we're going to see the bug
if readerPool.get is syncing on the SRMapValue. I guess there's
a slight possibility of the error, and perhaps a more randomized
test would produce it.","09/Jul/09 11:23;mikemccand;Attached new patch (the patch is worse than it looks, because many
things moved into the CoreReaders class):

  * Moved more stuff into CoreReaders (fieldInfos, dir, segment, etc.)
    and moved methods down as well (ctor, openDocStores, decRef).

  * Made members final when possible, else synchronized access to
    getting them (to avoid running amok of JMM).

{quote}
Right however wouldn't it be somewhat cleaner to sync on core
for all clone operations given we don't want those to occur
(external to IW) at the same time? Ultimately we want core to be
the controller of it's resources rather than the SR being cloned?
{quote}

In fact, I'm not sure why cloning/reopening a segment needs to be
synchronized at all.

Sure it'd be weird for an app to send multiple threads down,
attempting to reopen/clone the same SR or core at once, but from
Lucene's standpoint there's nothing ""wrong"" with doing so, I think?

(Though, DirectoryReader does need its sync when its transferring the
write lock due to reopen on a reader w/ pending changes).

{quote}
I ran the test with the SRMapValue sync code, (4 threads) with
the sync on SR.core in openDocStore for 10 minutes, 2 core
Windows XML laptop Java 6.14 and no errors. Then same with 2
threads for 5 minutes and no errors. I'll keep on running it to
see if we can get an error.
{quote}

You could try inserting a testPoint (see the other testPoints in
IndexWriter) after the SRMapValue is pulled from the hash but before
we check if its reader is null, and then modify the threads in your
test to randomly yield on that testPoint (by subclassing IW)?  Ie
""exacerbate"" the path that exposes the hazard.

{quote}
I'm still a little confused as to why we're going to see the bug
if readerPool.get is syncing on the SRMapValue. I guess there's
a slight possibility of the error, and perhaps a more randomized
test would produce it.
{quote}

The hazard exists because there's a time when no synchronization is
held.  Ie, you retrieve SRMapValue from the hash while sync'd on
ReaderPool.  You then leave that sync entirely (this is where hazard
comes in), and next you sync on the SRMapValue.  Another thread can
sneak in and close the SRMapValue.reader during the time that no sync
is held.
","11/Jul/09 17:27;jasonrutherglen;{quote}try inserting a testPoint (see the other testPoints in
IndexWriter) after the SRMapValue is pulled from the hash but before
we check if its reader is null{quote}

I added the test point, but tested without the yield, the method
itself was enough of a delay to expose the exception.  ","11/Jul/09 18:19;jasonrutherglen;I haven't really figured out a clean way to move the reader
creation out of the reader pool synchronization. It turns out to
be somewhat tricky, unless we redesign our synchronization. 

One thing that came to mind is passing a lock object to SR's
core (which would be the same lock on SRMapValue), which the
incref/decref could sync on as well. Otherwise we've got
synchronization in many places, IW, IW.readerPool, SR, SR.core.
It would seem to make things brittle? Perhaps listing out the
various reasons we're synchronizing, to see if we can
consolidate some of them will help?","12/Jul/09 00:18;jasonrutherglen;Another idea is instantiate the SR.core.ref outside of the
IW.readerPool, and pass it into the newly created reader. Then
when we obtain SRMapValue, incref so we're keeping track of it's
usage, which (I believe) should be inline with the normal usage
of SR.ref (meaning don't close the reader if SRMV is checked
out). This way we know when the SRMV is in use and different
threads don't clobber each other creating and closing SRs using
readerPool.","13/Jul/09 09:57;mikemccand;{quote}
I haven't really figured out a clean way to move the reader
creation out of the reader pool synchronization. It turns out to
be somewhat tricky, unless we redesign our synchronization.
{quote}

Maybe we should simply hold off for now?

I don't think this sync is costing much in practice, now.
Ie, IndexReader.open is not concurrent when opening its segments; nor
would we expect multiple threads to be calling IndexWriter.getReader
concurrently.

There is a wee bit of concurrency we are preventing, ie for a merge or
applyDeletes to get a reader just as an NRT reader is being opened,
but realistically 1) that's not very costly, and 2) we can't gain that
concurrency back anyway because we synchronize on IW when opening the
reader.
","13/Jul/09 17:48;jasonrutherglen;I was thinking the sync on all of readerPool could delay someone
trying to call IW.getReader who would wait for a potentially
large new segment to be warmed. However because IW.mergeMiddle
isn't loading the term index, IW.getReader will pay the cost of
loading the term index. So yeah, it doesn't seem necessary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastVectorHighlighter: some classes and members should be publicly accessible to implement FragmentsBuilder,LUCENE-2204,12445244,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,koji,koji,12/Jan/10 01:01,10/May/13 10:43,30/Sep/19 08:38,12/Jan/10 13:54,2.9,2.9.1,3.0,,,,,,,,4.0-ALPHA,,,,modules/highlighter,,,0,,,,"I intended to design custom FragmentsBuilder can be written and pluggable, though, when I tried to write it out of the FVH package, it came out that some classes and members should be publicly accessible.",,,,,,,,,,,,,,,,"12/Jan/10 01:10;koji;LUCENE-2204.patch;https://issues.apache.org/jira/secure/attachment/12429969/LUCENE-2204.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,11582,,,Tue Jan 12 13:54:22 UTC 2010,New,,,,,,,"0|i04s8f:",25839,,,,,,,,,"12/Jan/10 01:10;koji;A patch attached. It includes reset methods for Tokenizer that is used by test code. I'll commit later today these trivial changes.","12/Jan/10 13:54;koji;Committed revision 898323.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remaining reallocation should use ArrayUtil.getNextSize(),LUCENE-2217,12445733,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,paul.elschot@xs4all.nl,paul.elschot@xs4all.nl,16/Jan/10 13:45,10/May/13 10:43,30/Sep/19 08:38,28/Jan/10 11:40,,,,,,,,,,,4.0-ALPHA,,,,core/other,,,0,,,,See recent discussion on ArrayUtils.getNextSize().,,,,,,,,,,,,,,,,"25/Jan/10 15:57;mikemccand;LUCENE-2217.patch;https://issues.apache.org/jira/secure/attachment/12431315/LUCENE-2217.patch","21/Jan/10 22:54;paul.elschot@xs4all.nl;LUCENE-2217.patch;https://issues.apache.org/jira/secure/attachment/12431076/LUCENE-2217.patch","20/Jan/10 13:33;mikemccand;LUCENE-2217.patch;https://issues.apache.org/jira/secure/attachment/12430880/LUCENE-2217.patch","20/Jan/10 11:45;paul.elschot@xs4all.nl;LUCENE-2217.patch;https://issues.apache.org/jira/secure/attachment/12430872/LUCENE-2217.patch","17/Jan/10 20:48;paul.elschot@xs4all.nl;LUCENE-2217.patch;https://issues.apache.org/jira/secure/attachment/12430577/LUCENE-2217.patch","16/Jan/10 13:49;paul.elschot@xs4all.nl;LUCENE-2217.patch;https://issues.apache.org/jira/secure/attachment/12430507/LUCENE-2217.patch",,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2010-01-16 18:38:56.889,,,false,,,,,,,,,,,,,,,11571,,,Thu Jan 28 11:40:31 UTC 2010,New,Patch Available,,,,,,"0|i04s5j:",25826,,,,,,,,,"16/Jan/10 18:38;mikemccand;Makes sense!","16/Jan/10 18:47;mikemccand;Actually the patch isn't quite right, I think?  First, it just calls ArrayUtil.getNextSize w/o passing that to resizeBytes?

Second, it needs to pass lastBytePos + MAX_BYTES_PER_INT as the arg to ArrayUtil.getNextSize (ie, that's the ""min target size"")?","16/Jan/10 23:17;paul.elschot@xs4all.nl;Indeed, the patch isn't quite right. I'll fix that and provide another patch. All test cases pass though, so I'll also try and
add a test case that fails when an allocation larger than the current initial size is needed.

The MAX_BYTE_PER_INT has disappeared into an added comment that states a minimum initial size.
The underlying problem is that ArrayUtils.getNextSize() does not have an argument for a minimum increase.
Would it make sense to add that, too? The code there has some strange constants  (3, 6 and 9) that could
perhaps be dropped when an extra argument for a minimum increase is added.
Looking at the comment there for the growth pattern, shouldn't the second number (after 0) be 3 instead of 4?
","17/Jan/10 09:57;mikemccand;bq.  I'll also try and add a test case that fails when an allocation larger than the current initial size is needed

That would be much appreciated ;)  (I hit that too!).

bq. The underlying problem is that ArrayUtils.getNextSize() does not have an argument for a minimum increase.  Would it make sense to add that, too?

Well... the arg to getNextSize is already the minimum size that must be returned, so, can't you just pass in lastBytePos + MAX_BYTES_PER_INT?

{quote}
The code there has some strange constants (3, 6 and 9) that could
perhaps be dropped when an extra argument for a minimum increase is added.
Looking at the comment there for the growth pattern, shouldn't the second number (after 0) be 3 instead of 4?
{quote}
We're working on cleaning up this method, under LUCENE-2213.  The growth pattern in the comment is in fact correct, if you were to call getNextSize on 1+ the current size, ie this progression:

{code}
getNextSize(1+getNextSize(1+getNextSize(1+....getNextSize(1+0)))))
{code}
","17/Jan/10 20:48;paul.elschot@xs4all.nl;Fixed the reallocation to be actually done and added a test case that fails the previous patch because of the missing reallocation.","19/Jan/10 20:12;paul.elschot@xs4all.nl;Btw. shouldn't IndexInput.bytes also be reallocated using ArrayUtils.getNextSize() ?
The growth factor there is a hardcoded 1.25 .","19/Jan/10 20:26;mikemccand;bq. Btw. shouldn't IndexInput.bytes also be reallocated using ArrayUtils.getNextSize()

+1  Wanna fold it into this patch?  (And any others you find..?).","19/Jan/10 22:32;paul.elschot@xs4all.nl;Well, it's not that I'm searching, but I'll provide another patch that includes IndexInput for this.

Would you have any idea about testcases for that?
:)

","20/Jan/10 11:45;paul.elschot@xs4all.nl;Patch of 20 Jan also includes use of ArrayUtil.getNextSize() for allocation in IndexInput.","20/Jan/10 13:33;mikemccand;Small tweaks on the last patch, also adding ArrayUtil.getNextSize when allocating the char[] in IndexInput.","20/Jan/10 14:38;paul.elschot@xs4all.nl;Since I missed the second one in IndexInput I used this regex to search the java files in trunk/src/java: {noformat} new .*\[.*\] {noformat}
and found that these are still suspicious:

PorterStemmer 85, 485
StandardTokenizerImpl  447
ByteBlockPool 86
CharBlockPool 45
MultipleTermPositions 86
TermInfosWriter 210
TermsHashPerField 527 (maybe)
TermVectorsReader 463, 472
FastCharStream 58
BufferedIndexInput 61, 157
UnicodeUtil, 7 times or more.


","21/Jan/10 11:35;mikemccand;I agree we should cutover these additional places -- wanna work into the patch?","21/Jan/10 11:58;mikemccand;Paul, I just committed LUCENE-2213, which changes name & signature of ArrayUtil.getNextSize.... so be sure to update & merge before taking the patch further!","21/Jan/10 17:31;paul.elschot@xs4all.nl;That was expected.

The next patch will be about 5 times bigger, so I'll need some more time.","21/Jan/10 22:54;paul.elschot@xs4all.nl;Patch of 21 Jan is as anticipated, except for the following:

UnicodeUtil.java last reallocation at line 332 unchanged.

FastCharStream.java change might affect speed, not included in patch.

TermsHashPerField unchanged.

BufferedIndexInput unchanged, only used by clone() in MultiLevelSkipListReader.
","25/Jan/10 15:57;mikemccand;New iteration of patch attached:

  * Recovered the fixes in my first patch

  * Cutover some ArrayUtil.oversize -> ArrayUtil.grow

  * We can't change StandardTokenizerImpl.java -- it's autogen'd from
    JFlex
","25/Jan/10 20:27;paul.elschot@xs4all.nl;Patch looks good, all tests pass. Thanks for simplifying the added testcase.
After this I'll post a clean patch at LUCENE-2232 .","28/Jan/10 11:40;mikemccand;Thanks Paul!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several final classes have non-overriding protected members,LUCENE-2200,12445112,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,sarowe,sarowe,09/Jan/10 15:57,10/May/13 10:43,30/Sep/19 08:38,10/Jan/10 21:11,3.0,,,,,,,,,,4.0-ALPHA,,,,,,,0,,,,"Protected member access in final classes, except where a protected method overrides a superclass's protected method, makes little sense.  The attached patch converts final classes' protected access on fields to private, removes two final classes' unused protected constructors, and converts one final class's protected final method to private.",,,,,,,,,,,,,,,,"10/Jan/10 19:33;sarowe;LUCENE-2200.patch;https://issues.apache.org/jira/secure/attachment/12429858/LUCENE-2200.patch","09/Jan/10 18:57;sarowe;LUCENE-2200.patch;https://issues.apache.org/jira/secure/attachment/12429827/LUCENE-2200.patch","09/Jan/10 15:59;sarowe;LUCENE-2200.patch;https://issues.apache.org/jira/secure/attachment/12429824/LUCENE-2200.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-01-09 18:44:49.413,,,false,,,,,,,,,,,,,,,11586,,,Sun Jan 10 21:11:09 UTC 2010,New,Patch Available,,,,,,"0|i04s9b:",25843,,,,,,,,,"09/Jan/10 16:04;sarowe;All tests pass with the attached patch applied.","09/Jan/10 18:44;simonw;Steve, I briefly looked at your patch. Could we make some of the member vars final too? 
The reader in CharReader or the defaultAnalyzer in ShingleAnalyzerWrapper for instance.

simon","09/Jan/10 18:57;sarowe;bq. Could we make some of the member vars final too?

Done in the new version of the patch.  Note that I didn't try to look in classes other than those already modified in the previous version of the patch for final class member access modification.","09/Jan/10 20:02;sarowe;FYI, all tests pass for me with the new version of the patch applied.","10/Jan/10 16:01;rcmuir;all tests pass and patch looks good to me. will commit at the end of the day.","10/Jan/10 17:48;simonw;Robert, when you commit this make sure you mark  the Attributes in EdgeNGramTokenFilter.java final thanks.
Steve thanks for the patch, such work is always appreciated.

simon","10/Jan/10 19:33;sarowe;bq. Robert, when you commit this make sure you mark the Attributes in EdgeNGramTokenFilter.java final thanks.

Whoops, I missed those - thanks for checking, Simon.  (minGram and maxGram can also be final in EdgeNGramTokenFilter.java.)

I've attached a new patch that includes these changes -- all tests pass.
","10/Jan/10 21:11;rcmuir;Thanks Steven!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A shell script to generate .gitignore from svn:ignore properties,LUCENE-3775,12542383,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,13/Feb/12 10:11,10/May/13 10:42,30/Sep/19 08:38,13/Feb/12 10:28,,,,,,,,,,,4.0-ALPHA,,,,general/build,,,0,,,,"Attached is a small shell script that generates .gitignore-new from all svn:ignore properties (on an svn checkout).
I was able to successfully regenerate existing .gitignore. There are spurious entries (no recursive rules), but
at least it's consistent with svn:ignore?",,,,,,,,,,,,,,,,"13/Feb/12 10:12;dweiss;gitignore-gen.sh;https://issues.apache.org/jira/secure/attachment/12514332/gitignore-gen.sh",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,227670,,,Mon Feb 13 10:12:10 UTC 2012,,,,,,,,"0|i04idj:",24242,,,,,,,,,"13/Feb/12 10:12;dweiss;Bash script to generate .gitignore-new",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoLockFactory should have a private constructor,LUCENE-2103,12442220,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,shaie,shaie,02/Dec/09 14:05,10/May/13 10:42,30/Sep/19 08:38,07/Dec/09 16:50,3.0,,,,,,,,,,4.0-ALPHA,,,,core/store,,,0,,,,"NoLockFactory documents in its javadocs that one should use the static getNoLockFactory() method. However the class does not declare a private empty constructor, which breaks its Singleton purpose. We cannot add the empty private constructor because that'd break break-compat (even though I think it's very low chance someone actually instantiates the class), therefore we'll add a @deprecated warning to the class about this, and add the method in 4.0. I personally prefer to add an empty constructor w/ the @deprecated method, but am fine either way.

Don't know if a patch is needed, as this is a trivial change. ",,,,,,,,,,,,,,,,"02/Dec/09 14:18;uschindler;LUCENE-2103.patch;https://issues.apache.org/jira/secure/attachment/12426672/LUCENE-2103.patch","02/Dec/09 14:12;uschindler;LUCENE-2103.patch;https://issues.apache.org/jira/secure/attachment/12426669/LUCENE-2103.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-02 14:12:53.846,,,false,,,,,,,,,,,,,,,11676,,,Mon Dec 07 16:50:27 UTC 2009,New,Patch Available,,,,,,"0|i04suv:",25940,,,,,,,,,"02/Dec/09 14:12;uschindler;Attached is the patch with javadocs and deprecation.

Will commit in a day.","02/Dec/09 14:18;uschindler;New patch with CHANGES.txt, thanks Shai!","02/Dec/09 14:20;uschindler;One addition: In my opinion, this class should be final. With the private ctor it gets automatically final (you cannot extend without ctor), but should we change this for now, too? This would be a (small) BW break.","02/Dec/09 14:22;shaie;if we're breaking BW, why not add the private ctor? I'm sure that when 3.1 will be out, that won't be the only BW :). Anyway, this is not a serious BW. If somebody relies on the ctor, then that somebody may also rely on the class not being final.","02/Dec/09 14:36;uschindler;I think I keep this open for a moment, maybe others also have a comment about that.

I would also like to make it final and private (also the package private inner class).","07/Dec/09 16:50;uschindler;Committed revision: 887995. Thanks Shai!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make IndexReader.decRef() call refCount.decrementAndGet instead of getAndDecrement,LUCENE-3637,12534655,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,shaie,shaie,shaie,11/Dec/11 13:51,10/May/13 10:42,30/Sep/19 08:38,11/Dec/11 18:10,,,,,,,,,,,3.6,4.0-ALPHA,,,core/search,,,0,,,,"IndexReader.decRef() has this code:

{code}
    final int rc = refCount.getAndDecrement();
    if (rc == 1) {
{code}

I think it will be clearer if it was written like this:

{code}
    final int rc = refCount.decrementAndGet();
    if (rc == 0) {
{code}

It's a very simple change, which makes reading the code (at least IMO) easier. Will post a patch shortly.",,,,,,,,,,,,,,,,"11/Dec/11 15:03;shaie;LUCENE-3637.patch;https://issues.apache.org/jira/secure/attachment/12506899/LUCENE-3637.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,220376,,,Sun Dec 11 18:10:05 UTC 2011,New,Patch Available,,,,,,"0|i04j7z:",24379,,,,,,,,,"11/Dec/11 15:03;shaie;Very trivial patch. If there are no objections, I'd like to commit this.","11/Dec/11 18:10;shaie;Committed rev 1213033 (trunk).
Committed rev 1213036 (3x).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
singletermsenum,LUCENE-2113,12442471,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,rcmuir,rcmuir,04/Dec/09 17:03,10/May/13 10:42,30/Sep/19 08:38,04/Dec/09 17:31,4.0-ALPHA,,,,,,,,,,4.0-ALPHA,,,,core/search,,,0,,,,"singletermsenum for flex (like the existing singletermenum, it is a filteredtermSenum that only matches one term, to preserve multitermquery semantics)",,,,,,,,,,,,,,,,"04/Dec/09 17:03;rcmuir;LUCENE-2113.patch;https://issues.apache.org/jira/secure/attachment/12426915/LUCENE-2113.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-12-04 17:15:23.854,,,false,,,,,,,,,,,,,,,11666,,,Fri Dec 04 17:31:17 UTC 2009,New,Patch Available,,,,,,"0|i04ssn:",25930,,,,,,,,,"04/Dec/09 17:15;mikemccand;Looks good Robert!","04/Dec/09 17:22;rcmuir;ok I will commit shortly. This removes nocommits from WildcardQuery and also makes porting Automaton one step easier.","04/Dec/09 17:31;rcmuir;Committed revision 887283.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace o.a.l.u.packed.Packed64SingleBlock with an inheritance-based impl,LUCENE-4184,12596631,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,02/Jul/12 10:21,10/May/13 10:40,30/Sep/19 08:38,03/Jul/12 12:04,,,,,,,,,,,4.0-BETA,,,,core/other,,,0,,,,"According tests that Toke Eskildsen and I performed in LUCENE-4062, this impl is consistently faster than the current one on various archs/JVMs.",,,,,,,,,,,,,,,,"02/Jul/12 10:27;jpountz;LUCENE-4184.patch;https://issues.apache.org/jira/secure/attachment/12534261/LUCENE-4184.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-11 23:11:06.153,,,false,,,,,,,,,,,,,,,243781,,,Wed Jul 11 23:11:06 UTC 2012,New,Patch Available,,,,,,"0|i04fv3:",23835,,,,,,,,,"02/Jul/12 10:27;jpountz;Patch.","03/Jul/12 12:04;jpountz;Committed (r1356640 on trunk and r1356647 on branch 4.x).","11/Jul/12 23:11;hossman;hoss20120711-manual-post-40alpha-change",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Sorter API: better compress positions, offsets and payloads in SortingDocsAndPositionsEnum",LUCENE-4871,12638508,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,22/Mar/13 14:05,10/May/13 10:33,30/Sep/19 08:38,22/Mar/13 15:09,,,,,,,,,,,4.3,,,,,,,0,,,,"SortingDocsAndPositionsEnum could easily save memory by using a Lucene40TCF-like compression method for positions, offsets and payloads:
 - delta-encode positions and startOffsets (with the previous end offset),
 - store the length of the tokens instead of their end offset (endOffset == startOffset + length),
 - use a single bit to say whether the token has a payload.",,,,,,,,,,,,,,,,"22/Mar/13 14:08;jpountz;LUCENE-4871.patch;https://issues.apache.org/jira/secure/attachment/12575014/LUCENE-4871.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-22 14:16:43.478,,,false,,,,,,,,,,,,,,,318984,,,Fri May 10 10:33:50 UTC 2013,New,,,,,,,"0|i1j1fj:",319325,,,,,,,,,"22/Mar/13 14:08;jpountz;Patch.","22/Mar/13 14:16;commit-tag-bot;[trunk commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1459790

LUCENE-4871: Better compress positions, offsets and payloads in SortingDocsAndPositionsEnum.

","22/Mar/13 14:30;commit-tag-bot;[branch_4x commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1459792

LUCENE-4871: Better compress positions, offsets and payloads in SortingDocsAndPositionsEnum (merged from r1459790).

","10/May/13 10:33;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sorter API: use an abstract doc map instead of an array,LUCENE-4830,12636897,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,13/Mar/13 23:22,10/May/13 10:33,30/Sep/19 08:38,15/Mar/13 07:45,,,,,,,,,,,4.3,,,,,,,0,,,,The sorter API uses arrays to store the old->new and new->old doc IDs mappings. It should rather be an abstract class given that in some cases an array is not required at all (reverse mapping for example).,,,,,,,,,,,,,,,,"13/Mar/13 23:31;jpountz;LUCENE-4830.patch;https://issues.apache.org/jira/secure/attachment/12573618/LUCENE-4830.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-14 21:00:22.336,,,false,,,,,,,,,,,,,,,317389,,,Fri May 10 10:33:27 UTC 2013,New,,,,,,,"0|i1irl3:",317730,,,,,,,,,"13/Mar/13 23:31;jpountz;Patch. I also changed SortingAtomicReader.liveDocs() to be a view over the original liveDocs.","14/Mar/13 21:00;shaie;Looks good! I spotted missing @Override in REVERSE_SORTER DocMap and I think that we should make the DocMap impl final? Maybe it will encourage JIT ...
+1!","14/Mar/13 21:22;jpountz;bq. I think that we should make the DocMap impl final? Maybe it will encourage JIT ...

Looks like it doesn't help much? http://stackoverflow.com/questions/8354412/do-java-finals-help-the-compiler-create-more-efficient-bytecode","15/Mar/13 03:40;shaie;I see. Good to know!

I think you can commit it.","15/Mar/13 07:14;commit-tag-bot;[trunk commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1456787

LUCENE-4830: Sorter API: Make the doc ID mapping an abstract class.

","15/Mar/13 07:28;commit-tag-bot;[branch_4x commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1456789

LUCENE-4830: Sorter API: Make the doc ID mapping an abstract class (merged from r1456787).

","15/Mar/13 07:42;commit-tag-bot;[trunk commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1456796

LUCENE-4830: Add missing @Override.

","15/Mar/13 07:45;jpountz;Thank you for the review, Shai!","15/Mar/13 07:52;commit-tag-bot;[branch_4x commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1456797

LUCENE-4830: Add missing @Override (merged from r1456796).

","10/May/13 10:33;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make CompressingTermVectorsFormat the new default term vectors format?,LUCENE-4733,12629743,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,29/Jan/13 17:52,10/May/13 10:33,30/Sep/19 08:38,02/Feb/13 23:44,,,,,,,,,,,4.2,,,,,,,0,,,,"In LUCENE-4599, I wrote an alternate term vectors format which has a more compact format, and I think it could replace the current Lucene40TermVectorsFormat for the next (4.2) release?",,,,,,,,,,,,,,,,"01/Feb/13 10:58;jpountz;LUCENE-4733-javadocs.patch;https://issues.apache.org/jira/secure/attachment/12567563/LUCENE-4733-javadocs.patch","31/Jan/13 15:17;jpountz;LUCENE-4733-tests.patch;https://issues.apache.org/jira/secure/attachment/12567369/LUCENE-4733-tests.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-29 18:07:46.53,,,false,,,,,,,,,,,,,,,310239,,,Fri May 10 10:33:12 UTC 2013,New,,,,,,,"0|i1hjhz:",310584,,,,,,,,,"29/Jan/13 17:58;jpountz;It would be great if someone could set up a Jenkins job to run tests with -Dtests.codec=Compressing to catch bugs of this new format. Otherwise, the following points need to be addressed (I'll take care of these):
 - write file format docs,
 - wire this format in Lucene42Codec (I'll wait for the lucene4547 to be merged back into trunk for this one I think),
 - write tests for corner cases (small/large number of fields/terms/positions/offsets/payloads, etc.),
 - find good default params (chunk size / compression alg).

Feel free to let me know if you think it is a bad idea.","29/Jan/13 18:07;rcmuir;I think its a good idea! About the corner cases:

If we are going to add new tests anyway, maybe we could fashion it as a base class like BaseDocValuesFormat/BasePostingsFormat?
Each TVFormat we have (4.0, simpletext, 4.2, etc) would extend this base class and just return getCodec().

Any tests in TestTermVectors/Reader/Writer today that are testing codecs (and not really indexwriter tests) could be folded in.

Of course if any tests are *compressing specific they can just stay in its folder too.
","31/Jan/13 15:17;jpountz;Step 1: Patch that adds a new BaseTermVectorsFormatTestCase (extended for SimpleText, Lucene40 and Compressing). I'll commit soon if nobody objects.","31/Jan/13 21:22;rcmuir;+1: though I think we need a little javadoc sentence on that since its in test-framework or the build will complain.

we can just describe its ideal purpose is to be the ""one test"" for your format like the postings/docvalues equivalents.
","01/Feb/13 10:10;commit-tag-bot;[trunk commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1441367

LUCENE-4733: Refactor term vectors formats tests around a BaseTermVectorsFormatTestCase.

","01/Feb/13 10:28;commit-tag-bot;[branch_4x commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1441379

LUCENE-4733: Refactor term vectors formats tests around a BaseTermVectorsFormatTestCase (merged from r1441367).

","01/Feb/13 10:58;jpountz;New patch that adds Lucene42TermVectorsFormat with file format javadocs.

I have set chunkSize=4KB as it seemed to be a good compromise between compression and speed in LUCENE-4599 (this chunk size only accounts for terms and payloads). Moreover, it uses LZ4 compression (which is very light) so that compression/decompression is not the bottleneck even for small indexes which fit into memory.","01/Feb/13 12:02;jpountz;I forgot to mention that the patch adds the format, but no codec. Should we wait for lucene4547 to be merged back into trunk and then just change the term vectors format or should we add the new Lucene42Codec in trunk without taking care of the lucene4547 branch? I guess it depends on how soon this branch is going to land on trunk?","01/Feb/13 17:28;rcmuir;unrelated branches shouldn't delay trunk development. 

let me try to merge the relevant commits for 4.2 codec (and 4.1 impersonator and so on) to trunk...","01/Feb/13 18:28;commit-tag-bot;[trunk commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1441571

LUCENE-4733: merge Lucene42 codec from lucene-4547 branch
","01/Feb/13 18:40;rcmuir;ok its done: have fun!","01/Feb/13 18:46;commit-tag-bot;[branch_4x commit] Robert Muir
http://svn.apache.org/viewvc?view=revision&revision=1441578

LUCENE-4733: merge Lucene42 codec from lucene-4547 branch
","02/Feb/13 15:01;commit-tag-bot;[branch_4x commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1441760

LUCENE-4733: Make CompressingTermVectorsFormat the new default term vectors format (merged from r1441732).

","02/Feb/13 15:03;jpountz;bq. ok its done: have fun!

I will! Thanks, Robert!","02/Feb/13 15:24;commit-tag-bot;[trunk commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1441761

LUCENE-4733: More term vectors formats tests (TVReader.clone and TVWriter.merge).

","02/Feb/13 15:34;commit-tag-bot;[branch_4x commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1441763

LUCENE-4733: More term vectors formats tests (TVReader.clone and TVWriter.merge).

","10/May/13 10:33;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make SorterTemplate.mergeSort run in linear time on sorted arrays,LUCENE-4875,12638685,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,23/Mar/13 21:19,10/May/13 10:33,30/Sep/19 08:38,27/Mar/13 20:25,,,,,,,,,,,4.3,,,,,,,0,,,,"Through minor modifications, SorterTemplate.mergeSort could run in linear time on sorted arrays, so I think we should do it? The idea is to modify merge so that it returns instantly when compare(pivot-1, pivot) <= 0.",,,,,,,,,,,,,,,,"23/Mar/13 21:47;jpountz;LUCENE-4875.patch;https://issues.apache.org/jira/secure/attachment/12575199/LUCENE-4875.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-10 10:33:10.538,,,false,,,,,,,,,,,,,,,319160,,,Fri May 10 10:33:10 UTC 2013,New,,,,,,,"0|i1j2in:",319501,,,,,,,,,"23/Mar/13 21:47;jpountz;Patch. I modified the test case to make sure merge is never called when the concatenation of the two runs to merge is already sorted.","10/May/13 10:33;uschindler;Closed after release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EnwikiQueryMaker,LUCENE-1770,12431828,,New Feature,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,markrmiller@gmail.com,markrmiller@gmail.com,30/Jul/09 17:32,02/May/13 02:29,30/Sep/19 08:38,05/Aug/09 01:39,,,,,,,,,,,2.9,,,,,,,1,,,,,,,,,,,,,,,,,,,,"04/Aug/09 23:58;koji;LUCENE-1770.patch;https://issues.apache.org/jira/secure/attachment/12415548/LUCENE-1770.patch","04/Aug/09 23:24;markrmiller@gmail.com;LUCENE-1770.patch;https://issues.apache.org/jira/secure/attachment/12415544/LUCENE-1770.patch","01/Aug/09 21:50;markrmiller@gmail.com;LUCENE-1770.patch;https://issues.apache.org/jira/secure/attachment/12415249/LUCENE-1770.patch","30/Jul/09 17:33;markrmiller@gmail.com;LUCENE-1770.patch;https://issues.apache.org/jira/secure/attachment/12415041/LUCENE-1770.patch",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2009-08-01 18:09:29.405,,,false,,,,,,,,,,,,,,,11992,,,Wed Aug 05 00:38:46 UTC 2009,New,Patch Available,,,,,,"0|i04ux3:",26274,,,,,,,,,"01/Aug/09 18:09;koji;Mark, I used this patch in LUCENE-1773, but I got TooManyClauses due to WildcardQuery(""fo*""). I commented out it in my test.","01/Aug/09 19:03;markrmiller@gmail.com;Yeah, sorry bout that. I had updated that query to use constantscore locally bit didn't update this yet.  I use a 2 gig zipped wiki dump, so I was hitting that pretty quickly as well. I'll put up another soon. ","01/Aug/09 21:50;markrmiller@gmail.com;fixes wildcard to use constantscore rewrite and the last query was missing its last letter","04/Aug/09 23:24;markrmiller@gmail.com;changes name to EnWikiQueryMaker

updates javadoc a bit - removes javadoc class header that belonged to ReutersQueryMaker","04/Aug/09 23:58;koji;bq. changes name to EnWikiQueryMaker 

changes name to EnWikiQueryMaker in highlight-vs-vector-highlight.alg.","05/Aug/09 00:38;markrmiller@gmail.com;whoops - that W should be lowercase - Enwiki not EnWiki

I'll fix before commit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add benchmark task for FastVectorHighlighter,LUCENE-1773,12431967,,New Feature,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,koji,koji,01/Aug/09 18:03,02/May/13 02:29,30/Sep/19 08:38,13/Aug/09 16:19,2.9,,,,,,,,,,2.9,,,,modules/benchmark,,,0,,,,,,,,,,,,,,,,,,,LUCENE-1770,"02/Aug/09 05:29;koji;LUCENE-1773.patch;https://issues.apache.org/jira/secure/attachment/12415261/LUCENE-1773.patch","01/Aug/09 18:06;koji;LUCENE-1773.patch;https://issues.apache.org/jira/secure/attachment/12415244/LUCENE-1773.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-08-03 13:12:02.012,,,false,,,,,,,,,,,,,,,11989,,,Thu Aug 13 16:19:22 UTC 2009,New,,,,,,,"0|i04uwf:",26271,,,,,,,,,"02/Aug/09 05:29;koji;Updated:
* Fixed javadoc comments
* added ""WarmTV"" alg in to highlighter-vs-vector-highlighter.alg","03/Aug/09 13:12;mikemccand;Patch looks good Koji!  I plan to commit in a day or two...","04/Aug/09 20:24;mikemccand;Thanks Koji!","04/Aug/09 23:12;koji;Thanks Mike! One thing I forgot to mention is that I used LUCENE-1770 in the patch.

{code:title=highlight-vs-vector-highlight.alg}
# Use LUCENE-1770 WikipediaQueryMaker
query.maker=org.apache.lucene.benchmark.byTask.feeds.WikipediaQueryMaker
#query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
{code}

If WikipediaQueryMaker weren't in to 2.9, ReutersQueryMaker should work.","05/Aug/09 00:17;mikemccand;Let's fix this w/ LUCENE-1770 (Koji has added a patch there), but if LUCENE-1770 doesn't make 2.9, then fix it here.","05/Aug/09 02:43;markrmiller@gmail.com;I think the Highlighter tests might be messed up with this?

ReadTask.doHighlight is deprecated - but if you still use it, whats the point? It's not called and it doesnt help to have it anymore?

Also, its counted on for the Benchmark/Highlighter test that uses a task to override that method and count highlights.","05/Aug/09 03:19;koji;bq. ReadTask.doHighlight is deprecated - but if you still use it, whats the point? It's not called and it doesnt help to have it anymore?

Hmm, I don't understand... I marked ReadTask.doHighlight deprecated and never used it. I think BenchmarkHighlighter.doHighlight() was called in doLogic()?","05/Aug/09 09:33;mikemccand;Indeed, the CountingHighlighterTestTask was not cutover to the BenchmarkHighlighter.  I think we should mark doHighlight final (and deprecated) to force a compile time error so any external (and internal!) subclasses of ReadTask are forced to switch to the new API?","05/Aug/09 10:09;mikemccand;Sigh.  I had wrongly committed this patch, which was causing contrib
tests to fail :( This happened because the multi-threaded wrapper I
use to run the tests had a bug that failed to detect the failure.
I've now fixed that bug so this should not happen again.  Sorry about
this...
","05/Aug/09 12:33;mikemccand;Trying again :)","13/Aug/09 15:59;mikemccand;Unfortunately, contrib/benchmark is 1.4 only, but the fast vector highlighter is 1.5.  See discussion here:

http://markmail.org/message/gnj5dzabe4naqskn

I think we have to remove this task, remove the dependency in build.xml, until 3.0.  So I'm reopening this, and will post a patch, and then set for 3.0.","13/Aug/09 16:19;mikemccand;Setting this as fixed in 2.9 again... we're thinking now of allowing 1.5 code into benchmark.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Missing word ""cela"" in conf/lang/stopwords_fr.txt",LUCENE-4911,12640990,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,iksnalybok,iksnalybok,05/Apr/13 09:47,15/Apr/13 13:19,30/Sep/19 08:38,06/Apr/13 15:12,4.2,,,,,,,,,,,,,,,,,0,,,,"NB: Not sure this defect is assigned to the right component.

In file example/solr/collection1/conf/lang/stopwords_fr.txt,
there is the word ""celà"". Though incorrect in French (cf http://fr.wiktionary.org/wiki/cel%C3%A0), it's common, but we may also add the correct spelling (e.g. ""cela"", whitout accent) to that stopwords list.

Another thing: I noticed that ""celà"" is the only word of the list followed by an unbreakable space. Is that wanted?",,600,600,,0%,600,600,,,,,,,,,"05/Apr/13 13:29;iksnalybok;stopwords_fr.txt.patch;https://issues.apache.org/jira/secure/attachment/12577211/stopwords_fr.txt.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-05 12:02:12.011,,,false,,,,,,,,,,,,,,,321425,,,Mon Apr 15 13:19:29 UTC 2013,New,Patch Available,,,,,,"0|i1jgif:",321770,,,,,,,,,"05/Apr/13 12:02;jpountz;Indeed, we should indeed add ""cela"". Can you create a patch? I don't think the unbreakable space has been added on purpose, it can be removed.","05/Apr/13 13:30;iksnalybok;Patch added.","05/Apr/13 14:23;rcmuir;Thanks Pierre: Actually this file is synchronized from lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/french_stop.txt (via a ant task from solr/ 'ant sync-analyzers')

I think we should patch this file so its in the default lucene stoplist, too.

It might also be a good idea for us to send an email about this to the snowball list (snowball-discuss@lists.tartarus.org) as thats where this file came from, they might be interested in the improvement, too.
","06/Apr/13 15:12;jpountz;Pierre, I just applied your patch to Lucene's stop list (http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/french_stop.txt?view=diff&r1=1465255&r2=1465256&pathrev=1465256). Thank you! This fix should be available in Lucene/Solr 4.3.

I also sent an email to snowball-discuss to mention this improvement: http://lists.tartarus.org/mailman/private/snowball-discuss/2013-April/001462.html","11/Apr/13 22:04;jpountz;For your information, Martin Porter (himself!) added cela to the upstream stop list (http://lists.tartarus.org/mailman/private/snowball-discuss/2013-April/001466.html).","15/Apr/13 13:19;iksnalybok;Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompressingStoredFieldsFormat: remove support for DISK_DOC,LUCENE-4522,12614339,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,01/Nov/12 11:03,22/Mar/13 16:20,30/Sep/19 08:38,02/Nov/12 16:58,,,,,,,,,,,4.1,,,,,,,0,,,,In LUCENE-4512 MEMORY_CHUNK got much more memory-efficient so I think there are not many use-cases for DISK_DOC anymore.,,,,,,,,,,,,,,,,"02/Nov/12 12:52;jpountz;LUCENE-4522.patch;https://issues.apache.org/jira/secure/attachment/12551854/LUCENE-4522.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-01 15:16:06.096,,,false,,,,,,,,,,,,,,,253583,,,Fri Mar 22 16:20:56 UTC 2013,New,,,,,,,"0|i0dvof:",79062,,,,,,,,,"01/Nov/12 15:16;rcmuir;+1","02/Nov/12 12:52;jpountz;Patch. I'll commit soon.","02/Nov/12 16:58;jpountz;Committed
 - trunk: r1405025
 - branch 4.x: r1405030","22/Mar/13 16:20;commit-tag-bot;[branch_4x commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1405030

LUCENE-4522: Remove CompressingStoredFieldsFormat.DISK_DOC (merged from r1405025).

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Micro-benchmarks for ntz and pop (BitUtils) operations.,LUCENE-2221,12445779,,Task,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,dawidweiss,dawidweiss,17/Jan/10 21:19,22/Mar/13 16:15,30/Sep/19 08:38,20/Nov/12 16:16,,,,,,,,,,,,,,,core/other,,,0,,,,"As suggested by Yonik, I performed a suite of micro-benchmarks to investigate the following:
* pop() (bitCount) seems to be implemented in the same way (""hacker's delight"") as in the BitUtils class (SUN's standard library from version 1.5).
* native intrinsics have been recently added to the HotSpot that should speed up bitCount significantly.

I have tried to run the code on various VMs and architectures, but of course the results may vary depending on the setting.

h2. Micro-benchmark code, evaluation

* The micro-benchmarks were written as JUnit tests with a custom set of rules that repeats each test measuring execution time, gc use, etc.
* There were 5 warmup runs before each test, followed by 15 benchmarked runs. The result contain overall times, round times and standard deviations where applicable.
* There were several tests for isolated performance of {{BitUtil.pop()}}, JDK's {{bitCount}}, {{BitUtil.ntz()}}, {{BitUtil.ntz2()}}, {{BitUtil.ntz3()}} and JDK's {{numberOfTrailingZeros}}, the test code had the following loop:
{code}
final long [] longs = NtzPopBenchmark.random.bits;
int cnt = 0;
for (int i = longs.length; --i >= 0;)
{
   cnt += Long.bitCount(longs[i]);
}
volatileVariable = cnt; // to prevent dead code removal.
{code}
* I also added another version of pop() based on a precomputed bit counts. This version was called {{pop2}}.

* The input array of long values was initialized to a memory taking 200MB. There were two different sets: {{random}} (random values) and {{single}} (single bit rotated to the right in each long).

* There were tests that called {{BitUtil.pop_xor}} between the two input bitsets (random, single).
* Additional tests that used iterators and and other BitUtil operations showed similar performance to isolated loops, so I omit them here.

h2. Evaluation environment

I tested on three different machines:
* Pentium 4, 32-bit, 3GHZ, 2GB RAM (Windows)
* AMD Athlon(tm) 64 X2 Dual Core Processor 5200+, 64-bit, 4GB RAM (Ubuntu)
* Intel(R) Core(TM)2 Quad CPU    Q9650  @ 3.00GHz, 64-bit, 4GB RAM (Ubuntu)

and on various VMs:
* 1.6.0_17, Java HotSpot(TM) Server VM, 14.3-b01, Sun Microsystems Inc., 
* 1.5.0_18, Java HotSpot(TM) Server VM, 1.5.0_18-b02, Sun Microsystems Inc., 
* 1.7.0-ea, Java HotSpot(TM) Server VM, 17.0-b06, Sun Microsystems Inc., 
* 1.6.0, IBM J9 VM, 2.4, IBM Corporation,
* BEA JRockit.
* (ant other minor versions of the VMs above, depending on the computer).

h2. Results overview

h3. {{pop}}

The times between {{BitUtil}} and JDK were mostly identical. However, on 32-bit systems, precached {{pop2}} performed
*much* better. Examples:

{noformat}
# 1.6.0_17, Java HotSpot(TM) Server VM, 14.3-b01, Sun Microsystems Inc., 
test_POP_JDK_random       : 15/20 rounds, time.total: 15.61, time.warmup: 4.31, time.bench: 11.30, round: 0.75 [+- 0.02]
test_POP_JDK_single       : 15/20 rounds, time.total: 15.67, time.warmup: 4.31, time.bench: 11.36, round: 0.76 [+- 0.02]
test_POP_BitUtil_random   : 15/20 rounds, time.total: 15.55, time.warmup: 4.33, time.bench: 11.22, round: 0.75 [+- 0.01]
test_POP_BitUtil_single   : 15/20 rounds, time.total: 15.55, time.warmup: 4.31, time.bench: 11.23, round: 0.75 [+- 0.01]
test_POP2_random          : 15/20 rounds, time.total:  6.69, time.warmup: 1.75, time.bench:  4.94, round: 0.33 [+- 0.00]
test_POP2_single          : 15/20 rounds, time.total:  4.66, time.warmup: 1.22, time.bench:  3.44, round: 0.23 [+- 0.01]
{noformat}

Note the difference between random and single distributions -- most probably due to more cache hits when referring to the
lookup table. Other VMs on this 32-bit machine:

{noformat}
# 1.5.0_18, Java HotSpot(TM) Server VM, 1.5.0_18-b02, Sun Microsystems Inc., 
test_POP_JDK_random       : 15/20 rounds, time.total: 20.67, time.warmup: 5.19, time.bench: 15.48, round: 1.03 [+- 0.01]
test_POP_JDK_single       : 15/20 rounds, time.total: 22.70, time.warmup: 5.63, time.bench: 17.08, round: 1.14 [+- 0.01]
test_POP_BitUtil_random   : 15/20 rounds, time.total: 22.69, time.warmup: 5.63, time.bench: 17.06, round: 1.14 [+- 0.01]
test_POP_BitUtil_single   : 15/20 rounds, time.total: 20.67, time.warmup: 5.19, time.bench: 15.48, round: 1.03 [+- 0.01]
test_POP2_random          : 15/20 rounds, time.total:  6.30, time.warmup: 1.63, time.bench:  4.67, round: 0.31 [+- 0.01]
test_POP2_single          : 15/20 rounds, time.total:  4.33, time.warmup: 1.16, time.bench:  3.17, round: 0.21 [+- 0.01]

# 1.7.0-ea, Java HotSpot(TM) Server VM, 17.0-b06, Sun Microsystems Inc., 
test_POP_JDK_random       : 15/20 rounds, time.total: 15.28, time.warmup: 4.25, time.bench: 11.03, round: 0.74 [+- 0.03]
test_POP_JDK_single       : 15/20 rounds, time.total: 15.16, time.warmup: 4.20, time.bench: 10.95, round: 0.73 [+- 0.01]
test_POP_BitUtil_random   : 15/20 rounds, time.total: 15.12, time.warmup: 4.20, time.bench: 10.92, round: 0.73 [+- 0.01]
test_POP_BitUtil_single   : 15/20 rounds, time.total: 15.13, time.warmup: 4.25, time.bench: 10.88, round: 0.73 [+- 0.01]
test_POP2_random          : 15/20 rounds, time.total:  6.78, time.warmup: 1.72, time.bench:  5.06, round: 0.34 [+- 0.01]
test_POP2_single          : 15/20 rounds, time.total:  4.72, time.warmup: 1.20, time.bench:  3.52, round: 0.23 [+- 0.02]
{noformat}

On 64-bit machines, the results are nearly equal, with pop2 performing slightly worse on SUN's 1.6 compared to JDK and BitUtil 
(but this difference is really tiny and not present on all VMs; see IBM J9 and SUN's 1.5 below).

{noformat}
# 1.6.0_16, Java HotSpot(TM) 64-Bit Server VM, 14.2-b01, Sun Microsystems Inc.,
test_POP_JDK_random       : 15/20 rounds, time.total: 3.27, time.warmup: 0.81, time.bench: 2.46, round: 0.16 [+- 0.00]
test_POP_JDK_single       : 15/20 rounds, time.total: 3.11, time.warmup: 0.76, time.bench: 2.34, round: 0.16 [+- 0.02]
test_POP_BitUtil_random   : 15/20 rounds, time.total: 3.27, time.warmup: 0.81, time.bench: 2.46, round: 0.16 [+- 0.00]
test_POP_BitUtil_single   : 15/20 rounds, time.total: 3.03, time.warmup: 0.77, time.bench: 2.26, round: 0.15 [+- 0.00]
test_POP2_random          : 15/20 rounds, time.total: 3.63, time.warmup: 0.93, time.bench: 2.70, round: 0.18 [+- 0.00]
test_POP2_single          : 15/20 rounds, time.total: 3.51, time.warmup: 0.89, time.bench: 2.62, round: 0.17 [+- 0.00]

# 1.6.0, IBM J9 VM, 2.4, IBM Corporation,
test_POP_JDK_random       : 15/20 rounds, time.total: 4.80, time.warmup: 1.24, time.bench: 3.57, round: 0.24 [+- 0.01]
test_POP_JDK_single       : 15/20 rounds, time.total: 5.00, time.warmup: 1.44, time.bench: 3.56, round: 0.24 [+- 0.01]
test_POP_BitUtil_random   : 15/20 rounds, time.total: 4.81, time.warmup: 1.24, time.bench: 3.56, round: 0.24 [+- 0.01]
test_POP_BitUtil_single   : 15/20 rounds, time.total: 4.75, time.warmup: 1.19, time.bench: 3.56, round: 0.24 [+- 0.01]
test_POP2_random          : 15/20 rounds, time.total: 3.65, time.warmup: 0.90, time.bench: 2.76, round: 0.18 [+- 0.00]
test_POP2_single          : 15/20 rounds, time.total: 3.82, time.warmup: 0.93, time.bench: 2.89, round: 0.19 [+- 0.01]

# 1.5.0_18, Java HotSpot(TM) 64-Bit Server VM, 1.5.0_18-b02, Sun Microsystems Inc.,
test_POP_JDK_random       : 15/20 rounds, time.total: 3.72, time.warmup: 0.94, time.bench: 2.78, round: 0.19 [+- 0.00]
test_POP_JDK_single       : 15/20 rounds, time.total: 5.96, time.warmup: 1.40, time.bench: 4.56, round: 0.30 [+- 0.00]
test_POP_BitUtil_random   : 15/20 rounds, time.total: 6.16, time.warmup: 1.43, time.bench: 4.73, round: 0.31 [+- 0.00]
test_POP_BitUtil_single   : 15/20 rounds, time.total: 3.62, time.warmup: 0.92, time.bench: 2.70, round: 0.18 [+- 0.00]
test_POP2_random          : 15/20 rounds, time.total: 3.70, time.warmup: 0.96, time.bench: 2.74, round: 0.18 [+- 0.00]
test_POP2_single          : 15/20 rounds, time.total: 3.57, time.warmup: 0.93, time.bench: 2.65, round: 0.18 [+- 0.00]
{noformat}

The other 64-bit machine (quad-core):

{noformat}
# 1.7.0-ea, Java HotSpot(TM) 64-Bit Server VM, 17.0-b06, Sun Microsystems Inc.,
test_POP_JDK_random       : 15/20 rounds, time.total: 2.46, time.warmup: 0.62, time.bench: 1.84, round: 0.12 [+- 0.00]
test_POP_JDK_single       : 15/20 rounds, time.total: 2.49, time.warmup: 0.62, time.bench: 1.87, round: 0.12 [+- 0.01]
test_POP_BitUtil_random   : 15/20 rounds, time.total: 2.47, time.warmup: 0.62, time.bench: 1.84, round: 0.12 [+- 0.00]
test_POP_BitUtil_single   : 15/20 rounds, time.total: 2.46, time.warmup: 0.62, time.bench: 1.84, round: 0.12 [+- 0.00]
test_POP2_random          : 15/20 rounds, time.total: 2.82, time.warmup: 0.71, time.bench: 2.11, round: 0.14 [+- 0.00]
test_POP2_single          : 15/20 rounds, time.total: 2.15, time.warmup: 0.55, time.bench: 1.61, round: 0.11 [+- 0.00]
{noformat}

I then replaced {{BitUtil.pop}} with {{BitUtil.pop2}} in bit-counting methods like xor/and/or. The results are intriguing.
On 32-bit systems, there is a measureable gain, like here:

{noformat}
# 1.6.0_17, Java HotSpot(TM) Server VM, 14.3-b01, Sun Microsystems Inc., 
test_pop_xor              : 15/20 rounds, time.total:  9.78, time.warmup: 2.59, time.bench:  7.19, round: 0.48 [+- 0.01]
test_pop2_hd_xor          : 15/20 rounds, time.total:  8.27, time.warmup: 2.22, time.bench:  6.05, round: 0.40 [+- 0.01]

# 1.7.0-ea, Java HotSpot(TM) Server VM, 17.0-b06, Sun Microsystems Inc., 
test_pop_xor              : 15/20 rounds, time.total:  9.89, time.warmup: 2.59, time.bench: 7.30, round: 0.49 [+- 0.02]
test_pop2_hd_xor          : 15/20 rounds, time.total:  8.20, time.warmup: 2.24, time.bench: 5.97, round: 0.40 [+- 0.01]
{noformat}

On 64-bit systems, when 64-bit values can be manipulated directly in registers, there was nearly no speedup or even
a small performance penalty like in here:

{noformat}
# 1.7.0-ea, Java HotSpot(TM) 64-Bit Server VM, 17.0-b06, Sun Microsystems Inc.,
test_pop_xor              : 15/20 rounds, time.total: 1.76, time.warmup: 0.49, time.bench: 1.27, round: 0.09 [+- 0.00]
test_pop2_hd_xor          : 15/20 rounds, time.total: 2.06, time.warmup: 0.55, time.bench: 1.51, round: 0.10 [+- 0.00]
{noformat}

I'm guessing referencing memory on this fast processors is slower than manipulating registers.

h3. {{ntz}}

On JVMs prior to 1.7, the {{ntz} version from Lucene was much faster in my tests than the one from JDK, 
but it also has a greater variance depending on the input bits' distribution (compare the same routine 
for random and single below).

{noformat}
# 32-bit system;
# 1.6.0_17, Java HotSpot(TM) Server VM, 14.3-b01, Sun Microsystems Inc., 
test_NTZ_JDK_random       : 15/20 rounds, time.total:  6.69, time.warmup: 1.73, time.bench: 4.95, round: 0.33 [+- 0.01]
test_NTZ_JDK_single       : 15/20 rounds, time.total:  7.59, time.warmup: 1.94, time.bench: 5.66, round: 0.38 [+- 0.01]
test_NTZ_BitUtil_random   : 15/20 rounds, time.total:  2.72, time.warmup: 0.73, time.bench: 1.98, round: 0.13 [+- 0.02]
test_NTZ_BitUtil_single   : 15/20 rounds, time.total:  5.28, time.warmup: 1.34, time.bench: 3.94, round: 0.26 [+- 0.02]
test_NTZ2_BitUtil_random  : 15/20 rounds, time.total:  3.06, time.warmup: 0.81, time.bench: 2.25, round: 0.15 [+- 0.01]
test_NTZ2_BitUtil_single  : 15/20 rounds, time.total:  5.36, time.warmup: 1.34, time.bench: 4.02, round: 0.27 [+- 0.01]
test_NTZ3_BitUtil_random  : 15/20 rounds, time.total:  5.80, time.warmup: 1.48, time.bench: 4.31, round: 0.29 [+- 0.01]
test_NTZ3_BitUtil_single  : 15/20 rounds, time.total:  6.98, time.warmup: 1.81, time.bench: 5.17, round: 0.34 [+- 0.01]

# 64-bit Athlon
# 1.6.0_16, Java HotSpot(TM) 64-Bit Server VM, 14.2-b01, Sun Microsystems Inc.,
test_NTZ_JDK_random       : 15/20 rounds, time.total: 4.59, time.warmup: 1.16, time.bench: 3.44, round: 0.23 [+- 0.00]
test_NTZ_JDK_single       : 15/20 rounds, time.total: 6.64, time.warmup: 1.59, time.bench: 5.04, round: 0.34 [+- 0.01]
test_NTZ_BitUtil_random   : 15/20 rounds, time.total: 2.09, time.warmup: 0.53, time.bench: 1.56, round: 0.10 [+- 0.00]
test_NTZ_BitUtil_single   : 15/20 rounds, time.total: 3.87, time.warmup: 0.98, time.bench: 2.90, round: 0.19 [+- 0.00]
test_NTZ2_BitUtil_random  : 15/20 rounds, time.total: 2.09, time.warmup: 0.52, time.bench: 1.57, round: 0.10 [+- 0.00]
test_NTZ2_BitUtil_single  : 15/20 rounds, time.total: 3.31, time.warmup: 0.84, time.bench: 2.47, round: 0.16 [+- 0.00]
test_NTZ3_BitUtil_random  : 15/20 rounds, time.total: 3.31, time.warmup: 0.83, time.bench: 2.48, round: 0.17 [+- 0.00]
test_NTZ3_BitUtil_single  : 15/20 rounds, time.total: 5.71, time.warmup: 1.39, time.bench: 4.32, round: 0.29 [+- 0.00]
{noformat}

But then comes the 1.7 HotSport and things change radically, on 32-bit system the JDK's version is much faster for nearly-empty
{{long}} values:

{noformat}
# 1.7.0-ea, Java HotSpot(TM) Server VM, 17.0-b06, Sun Microsystems Inc., 
test_NTZ_JDK_random       : 15/20 rounds, time.total:  1.97, time.warmup: 0.61, time.bench: 1.36, round: 0.09 [+- 0.01]
test_NTZ_JDK_single       : 15/20 rounds, time.total:  2.53, time.warmup: 0.77, time.bench: 1.77, round: 0.12 [+- 0.01]
test_NTZ_BitUtil_random   : 15/20 rounds, time.total:  2.36, time.warmup: 0.66, time.bench: 1.70, round: 0.11 [+- 0.01]
test_NTZ_BitUtil_single   : 15/20 rounds, time.total:  4.50, time.warmup: 1.19, time.bench: 3.31, round: 0.22 [+- 0.01]
test_NTZ2_BitUtil_random  : 15/20 rounds, time.total:  3.08, time.warmup: 0.81, time.bench: 2.27, round: 0.15 [+- 0.01]
test_NTZ2_BitUtil_single  : 15/20 rounds, time.total:  4.97, time.warmup: 1.28, time.bench: 3.69, round: 0.25 [+- 0.01]
test_NTZ3_BitUtil_random  : 15/20 rounds, time.total:  5.78, time.warmup: 1.48, time.bench: 4.30, round: 0.29 [+- 0.01]
test_NTZ3_BitUtil_single  : 15/20 rounds, time.total:  7.77, time.warmup: 1.91, time.bench: 5.86, round: 0.39 [+- 0.01]
{noformat}

On the 64-bit quad core:

{noformat}
# 1.6.0_13, Java HotSpot(TM) 64-Bit Server VM, 11.3-b02, Sun Microsystems Inc.,
test_NTZ_JDK_random       : 15/20 rounds, time.total: 3.92, time.warmup: 0.97, time.bench: 2.94, round: 0.20 [+- 0.00]
test_NTZ_JDK_single       : 15/20 rounds, time.total: 3.80, time.warmup: 0.97, time.bench: 2.82, round: 0.19 [+- 0.00]
test_NTZ_BitUtil_random   : 15/20 rounds, time.total: 0.96, time.warmup: 0.25, time.bench: 0.71, round: 0.05 [+- 0.00]
test_NTZ_BitUtil_single   : 15/20 rounds, time.total: 2.74, time.warmup: 0.69, time.bench: 2.04, round: 0.14 [+- 0.00]
test_NTZ2_BitUtil_random  : 15/20 rounds, time.total: 1.22, time.warmup: 0.31, time.bench: 0.91, round: 0.06 [+- 0.00]
test_NTZ2_BitUtil_single  : 15/20 rounds, time.total: 2.18, time.warmup: 0.56, time.bench: 1.62, round: 0.11 [+- 0.00]
test_NTZ3_BitUtil_random  : 15/20 rounds, time.total: 2.76, time.warmup: 0.71, time.bench: 2.06, round: 0.14 [+- 0.00]
test_NTZ3_BitUtil_single  : 15/20 rounds, time.total: 3.47, time.warmup: 0.91, time.bench: 2.56, round: 0.17 [+- 0.01]
{noformat}

And then comes the 1.7, compare JDK's implementation with anything else (especially the {{time.bench}} for
the {{single}} input data. Looks like this is hardware-accelerated.

{noformat}
# -server -Xbatch -Xmx1024m
# 1.7.0-ea, Java HotSpot(TM) 64-Bit Server VM, 17.0-b06, Sun Microsystems Inc.,
test_NTZ_JDK_random       : 15/20 rounds, time.total: 0.79, time.warmup: 0.21, time.bench: 0.58, round: 0.04 [+- 0.00]
test_NTZ_JDK_single       : 15/20 rounds, time.total: 0.75, time.warmup: 0.20, time.bench: 0.55, round: 0.04 [+- 0.00]
test_NTZ_BitUtil_random   : 15/20 rounds, time.total: 0.98, time.warmup: 0.25, time.bench: 0.72, round: 0.05 [+- 0.00]
test_NTZ_BitUtil_single   : 15/20 rounds, time.total: 2.61, time.warmup: 0.66, time.bench: 1.95, round: 0.13 [+- 0.00]
test_NTZ2_BitUtil_random  : 15/20 rounds, time.total: 1.30, time.warmup: 0.33, time.bench: 0.97, round: 0.06 [+- 0.00]
test_NTZ2_BitUtil_single  : 15/20 rounds, time.total: 2.48, time.warmup: 0.61, time.bench: 1.88, round: 0.13 [+- 0.00]
test_NTZ3_BitUtil_random  : 15/20 rounds, time.total: 2.81, time.warmup: 0.70, time.bench: 2.11, round: 0.14 [+- 0.00]
test_NTZ3_BitUtil_single  : 15/20 rounds, time.total: 4.07, time.warmup: 1.02, time.bench: 3.05, round: 0.20 [+- 0.00]
{noformat}

h2. Conclusions

It seems that any change introduced to these routines will hurt somebody in some configuration, so it's really hard
for me to make choices. I would definitely opt for the precached {{pop2}} version on 32-bit systems as it seems to 
be always faster or equally fast compared to other bit counting options. {{pop2}} looked like this:

{code}
   private static byte [] bcounts = new byte [0x10000];
   static
   {
       for (int i = 0x10000; --i >= 0;)
           bcounts[i] = (byte) Integer.bitCount(i);
   }

   public static int pop2(long v)
   {
       int t;
       return 
             bcounts[(t = (int) v) & 0xffff]
           + bcounts[t >>> 16]
           + bcounts[(t = ((int) (v >>> 32))) & 0xffff]
           + bcounts[t >>> 16];
   }
{code}

As for the hardware-accelerated {{ntz}}, if one can detect 1.7, then using the JDK's version is speeding up things
significantly. But I have not checked how this detection would affect speed if done at run-time (I assume a final
static flag wouldn't cause any performance penalty) and it is definitely not worth replacing for folks with older
VMs.

h2. Raw results data.

I will attach raw results as part of the issue if you want to draw your own conclusions. Didn't have access to sparc-machine
or to any machine with the newest Intels.
",,,,,,,,,,,,,,,,"20/Nov/12 10:22;jpountz;LUCENE-2221.patch;https://issues.apache.org/jira/secure/attachment/12554324/LUCENE-2221.patch","21/Jan/10 09:56;dawidweiss;benchmark.jar;https://issues.apache.org/jira/secure/attachment/12431013/benchmark.jar","20/Jan/10 12:51;dawidweiss;benchmarks.txt;https://issues.apache.org/jira/secure/attachment/12430876/benchmarks.txt","21/Jan/10 10:00;dawidweiss;lucene-bitset-benchmarks.zip;https://issues.apache.org/jira/secure/attachment/12431014/lucene-bitset-benchmarks.zip","18/Jan/10 17:02;dawidweiss;results-popntz.txt;https://issues.apache.org/jira/secure/attachment/12430658/results-popntz.txt",,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2010-01-18 10:46:36.384,,,false,,,,,,,,,,,,,,,11567,,,Fri Mar 22 16:15:28 UTC 2013,New,,,,,,,"0|i04s4n:",25822,,,,,,,,,"17/Jan/10 21:21;dawidweiss;Performance test results.","18/Jan/10 10:46;mikemccand;We can switch the pop2 impl based on 32bit vs 64bit jre?","18/Jan/10 11:03;dawidweiss;Yes, this would be my initial suggestion. Although before this is committed to the code, I would first test such a patch:

a) on a more diverse set of machines (and here the help of users would be invaluable), especially with different CPUs (sparc) and motherboard architectures. pop2 references memory heavily, I wonder how this affects performance.

b) adding an if on a static final variable should effectively allow both implementations to coexist (hotspot should get rid of the dead code at runtime). I have not verified this, however.

I'm sure Yonik has implemented precalced pop() though (since ntz is a precalc too), so I wonder if his implementation didn't show this improvement or if there were other reasons not to include it. I sometimes wish we went back to the simplicity of clock-cycle counting processors and assembly... everything was so straightforward back then :)","18/Jan/10 16:49;yseeley@gmail.com;bq. I'm sure Yonik has implemented precalced pop() though (since ntz is a precalc too), 

I never tried a 64K lookup table.  The problem with something like that is that it completely blows out your L1 caches (or takes up half of it - depending on the processor).  So a micro-benchmark could show a speedup while the whole program may show a slowdown.  This stuff needs to be tested in context.  The performance of pop() was also de-emphasized via the implementation of pop_array, pop_intersect, etc, which only call pop once for every 8 longs.

So it's things like pop_intersect that should be the test target, not pop.
Create 100 sets of size 10M bits and do intersectionCount between the pairs.
That's the use case that will matter to Solr (OpenBitSet came from Solr, and Lucene doesn't use pop much AFAIK).
","18/Jan/10 16:57;dawidweiss;Look closely at the results above, Yonik. I have done this -- the test_pop_xor and test_pop2_hd_xor do exactly this. While I do agree that a lookup table blows the cache, it still seems to be more efficient than running 64-bit manipulations on a 32-bit processor, at least on those machines I had available.","18/Jan/10 17:02;dawidweiss;Plain ASCII results.","18/Jan/10 17:56;yseeley@gmail.com;Ah yeah, missed that.

In fact it appears on 32 bit systems, even
 Integer.bitCount((int)v) + Integer.bitCount((int)(v>>>32))
is faster than Long.bitCount

Never looked into that since we were deploying on 64 bit systems (even way back in '04 in fact)","18/Jan/10 18:22;dawidweiss;I had a suspicion this must be the case. I even wondered if, given this, it wouldn't make sense to write a simple loop (loops are unrolled by HotSpot anyway ) for xor'ing long arrays and use int bitcounts twice.... when I did this, however, the times were slower. There is a lot of magic going on when HotSpot inlines stuff (register allocation, etc.), so I can only support your claim that a test in context would be much better; this context should be something that uses bitutils extensively, but on some real chunk of data and including a larger bit of Lucene code.","20/Jan/10 12:51;dawidweiss;Benchmark results for array operations and iterators comparing the performance of ntz in JRE, BitUtil (Lucene trunk) and BitUtil (different implementations for 32/64 bit JVMs, runtime decision).","20/Jan/10 12:53;dawidweiss;Executable Java JAR with benchmarking code for anybody that wishes to repeat/ run these tests on other architectures. I'd be particularly interested in Intel I7 -- these have HotSpot intrinsics.","20/Jan/10 13:13;dawidweiss;I wrote a set of micro-benchmarks comparing {{pop_*}} methods from {{BitUtil}} and an OpenBitSet iterator ({{nextSetBit}}) performance. Three different variations of {{BitUtil}}:
* current Lucene trunk,
* pop/ntz redirecting to corresponding JRE methods,
* current Lucene trunk with pop doing an {{if}} depending on the current architecture (64/32 bits) and redirecting to table lookup or bit-fiddling version.

The results I acquired from the machines I have access to confirm what I said earlier -- the 32-bit version brings performance improvement to bit counting and does not degrade performance on 64-bit architectures. Compare {{time.bench}} and {{round}} fields:

{noformat}
# Windows XP, 32-bit, Pentium 4

# 1.5.0_18, Java HotSpot(TM) Server VM, 1.5.0_18-b02, Sun Microsystems Inc., 
#
Benchmark_BitUtil_trunk.test_pop_array            : 15/20 rounds, time.bench:  26.45, round:  1.77 [+- 0.02]
Benchmark_BitUtil_trunk.test_pop_xor              : 15/20 rounds, time.bench:  43.34, round:  2.89 [+- 0.00]
Benchmark_BitUtil_trunk.test_pop_intersect        : 15/20 rounds, time.bench:  43.91, round:  2.93 [+- 0.01]
Benchmark_BitUtil_trunk.test_pop_andnot           : 15/20 rounds, time.bench:  40.61, round:  2.71 [+- 0.01]
Benchmark_BitUtil_trunk.test_pop_union            : 15/20 rounds, time.bench:  44.00, round:  2.93 [+- 0.01]
                                                                                                            
Benchmark_BitUtil_pop3264.test_pop_array          : 15/20 rounds, time.bench:  19.86, round:  1.32 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_xor            : 15/20 rounds, time.bench:  38.42, round:  2.56 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_intersect      : 15/20 rounds, time.bench:  37.95, round:  2.53 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_andnot         : 15/20 rounds, time.bench:  32.75, round:  2.18 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_union          : 15/20 rounds, time.bench:  37.87, round:  2.52 [+- 0.01]
                                                                                                            
#                                                                                                           
# 1.7.0-ea, Java HotSpot(TM) Server VM, 17.0-b06, Sun Microsystems                                          
#                                                                                                           
Benchmark_BitUtil_trunk.test_pop_array            : 15/20 rounds, time.bench:  27.41, round:  1.83 [+- 0.01]
Benchmark_BitUtil_trunk.test_pop_xor              : 15/20 rounds, time.bench:  40.69, round:  2.71 [+- 0.01]
Benchmark_BitUtil_trunk.test_pop_intersect        : 15/20 rounds, time.bench:  40.67, round:  2.71 [+- 0.01]
Benchmark_BitUtil_trunk.test_pop_andnot           : 15/20 rounds, time.bench:  40.36, round:  2.69 [+- 0.02]
Benchmark_BitUtil_trunk.test_pop_union            : 15/20 rounds, time.bench:  40.69, round:  2.71 [+- 0.01]
                                                                                                            
Benchmark_BitUtil_pop3264.test_pop_array          : 15/20 rounds, time.bench:  19.59, round:  1.31 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_xor            : 15/20 rounds, time.bench:  33.23, round:  2.22 [+- 0.02]
Benchmark_BitUtil_pop3264.test_pop_intersect      : 15/20 rounds, time.bench:  33.05, round:  2.20 [+- 0.00]
Benchmark_BitUtil_pop3264.test_pop_andnot         : 15/20 rounds, time.bench:  33.66, round:  2.24 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_union          : 15/20 rounds, time.bench:  33.02, round:  2.20 [+- 0.01]
{noformat}
The attachment {{benchmarks.txt}} has details for 64-bit systems. No performance degradation from a runtime selection of bit-counting routine. I had no access to a machine with hardware {{popcnt}} support.

The other thing measured was bit-iterators and hardware support for {{ntz}}. I had access to a single machine with hardware support for ntz -- bit iterators speed up a lot if Java 1.7 is available. Compare (same VM, same machine):
{noformat}
Benchmark_BitUtil_trunk.test_ntz_iterator_int     :   time.bench: 26.24, round: 5.25 [+- 0.00]
Benchmark_BitUtil_trunk.test_ntz_iterator_long    :   time.bench: 31.53, round: 6.31 [+- 0.00]

Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_int :   time.bench: 17.52, round: 3.50 [+- 0.00]
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_long:   time.bench: 22.67, round: 4.53 [+- 0.00]
{noformat}
Older VM (1.6, same machine (no support for hardware ntz):
{noformat}
Benchmark_BitUtil_trunk.test_ntz_iterator_int     :   time.bench: 24.96, round: 4.99 [+- 0.00]
Benchmark_BitUtil_trunk.test_ntz_iterator_long    :   time.bench: 28.92, round: 5.78 [+- 0.00]
                                                                                              
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_int :   time.bench: 45.94, round: 9.19 [+- 0.00]
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_long:   time.bench: 48.59, round: 9.72 [+- 0.00]
{noformat}

The binary used to acquire these numbers (and the source code) is attached to this issue.

My idea for a patch here would be to have the following logic:
* if the system is 32-bit, use table-based pop() -- does not hurt and speeds up things on 32-bit systems.
* if the system is 64-bit AND a system property {{lucene.popntz.jre}} is set, use JRE's implementation.
* otherwise use the current implementation from the trunk.

The system property above is an expert option -- if somebody is running an Intel I7 and Java 1.7, enabling it will bring a measureable performance gain. I have no idea how to detect hardware support for pop/ntz other than by measuring execution time (easily affected by concurrent load) or using external utilities (cat /proc/cpuinfo; not too nice and platform-dependent).","20/Jan/10 13:14;dawidweiss;Benchmarks, source code.","20/Jan/10 16:30;yseeley@gmail.com;If you only tested 32 bit on the P4, that can be the source of some of the difference too - they had slow shift operations.  Current processors from both Intel and AMD do much better.  So if you haven't, you might want to try a 32 bit JVM on one of your other processors (P4s are sort of going the way of the dinosaur anyway).

edit: I did a quick test previously on my core2, and the lookup method was still faster (I forget what the numbers were though).","20/Jan/10 17:25;dawidweiss;I do have a bunch of dinosaur-age computers, such is life. I added the benchmark JAR to this issue so that other people can experiment on their own, I'm sure there is such a variety of different architectures and memory speed-to-processor speed ratios that there will be no clear winner. I just checked on two other 32-bit machines (AMD Athlon and Intel Core2 Duo U7700). The Intel shows table-lookup gain of about 10%. For AMD the results are nearly identical.","20/Jan/10 18:37;dawidweiss;Results from Intel I7 -- an improvement of about 20%. Significant, but I was silently hoping for more. I'm guessing memory throughput becomes the limit in these benchmarks.

{noformat}
#
# Intel(R) Core(TM) i7 CPU 920  @ 2.67GHz
#

# 1.7.0-ea, Java HotSpot(TM) 64-Bit Server VM, 17.0-b06, Sun Microsystems Inc., 

Benchmark_BitUtil_trunk.test_pop_array            : 15/20 rounds, time.total:  6.91, time.warmup:  1.75, time.bench:  5.15, round: 0.34 [+- 0.00]
Benchmark_BitUtil_trunk.test_pop_xor              : 15/20 rounds, time.total: 10.51, time.warmup:  2.66, time.bench:  7.86, round: 0.52 [+- 0.00]
Benchmark_BitUtil_trunk.test_pop_intersect        : 15/20 rounds, time.total: 10.49, time.warmup:  2.64, time.bench:  7.85, round: 0.52 [+- 0.00]
Benchmark_BitUtil_trunk.test_pop_andnot           : 15/20 rounds, time.total:  9.91, time.warmup:  2.51, time.bench:  7.40, round: 0.49 [+- 0.00]
Benchmark_BitUtil_trunk.test_pop_union            : 15/20 rounds, time.total: 10.48, time.warmup:  2.64, time.bench:  7.84, round: 0.52 [+- 0.00]
Benchmark_BitUtil_trunk.test_ntz_iterator_int     :   5/7 rounds, time.total: 38.67, time.warmup: 11.18, time.bench: 27.49, round: 5.50 [+- 0.03]
Benchmark_BitUtil_trunk.test_ntz_iterator_long    :   5/7 rounds, time.total: 46.42, time.warmup: 13.24, time.bench: 33.19, round: 6.64 [+- 0.03]
 
Benchmark_BitUtil_pop3264.test_pop_array          : 15/20 rounds, time.total:  6.91, time.warmup:  1.76, time.bench:  5.15, round: 0.34 [+- 0.00]
Benchmark_BitUtil_pop3264.test_pop_xor            : 15/20 rounds, time.total: 10.48, time.warmup:  2.64, time.bench:  7.84, round: 0.52 [+- 0.00]
Benchmark_BitUtil_pop3264.test_pop_intersect      : 15/20 rounds, time.total: 10.50, time.warmup:  2.64, time.bench:  7.86, round: 0.52 [+- 0.00]
Benchmark_BitUtil_pop3264.test_pop_andnot         : 15/20 rounds, time.total:  9.91, time.warmup:  2.51, time.bench:  7.41, round: 0.49 [+- 0.00]
Benchmark_BitUtil_pop3264.test_pop_union          : 15/20 rounds, time.total: 10.49, time.warmup:  2.64, time.bench:  7.85, round: 0.52 [+- 0.00]

Benchmark_BitUtil_popNtzJRE.test_pop_array        : 15/20 rounds, time.total:  4.56, time.warmup:  1.16, time.bench:  3.40, round: 0.23 [+- 0.00]
Benchmark_BitUtil_popNtzJRE.test_pop_xor          : 15/20 rounds, time.total:  8.81, time.warmup:  2.20, time.bench:  6.60, round: 0.44 [+- 0.02]
Benchmark_BitUtil_popNtzJRE.test_pop_intersect    : 15/20 rounds, time.total:  8.58, time.warmup:  2.16, time.bench:  6.42, round: 0.43 [+- 0.00]
Benchmark_BitUtil_popNtzJRE.test_pop_andnot       : 15/20 rounds, time.total:  8.13, time.warmup:  2.06, time.bench:  6.07, round: 0.40 [+- 0.00]
Benchmark_BitUtil_popNtzJRE.test_pop_union        : 15/20 rounds, time.total:  8.57, time.warmup:  2.16, time.bench:  6.41, round: 0.43 [+- 0.00]
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_int :   5/7 rounds, time.total: 28.68, time.warmup:  7.96, time.bench: 20.73, round: 4.15 [+- 0.00]
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_long:   5/7 rounds, time.total: 36.17, time.warmup: 10.30, time.bench: 25.88, round: 5.18 [+- 0.00]
{noformat}","20/Jan/10 20:26;yseeley@gmail.com;bq. Results from Intel I7 - an improvement of about 20%. Significant, but I was silently hoping for more.

Remember, pop_array and friends do tricks to only call pop once for every 8 longs because pop was slow...
for the intrinsic case, did you try a simple loop that calls pop for every element?  For example:
{code}
  public static long pop_intersect_simple(long A[], long B[], int wordOffset, int numWords) {
    int end = wordOffset + numWords;
    long ret = 0;
    for (int i=wordOffset; i<end; i++) {
      ret += Long.bitCount(A[i]^B[i]);
    }
    return ret;
  }
{code}","21/Jan/10 09:22;cristian.vat;Ran the benchmark.jar also on my machine:

{code}
# Windows 7 x64, Intel(R) Core(TM) i5 CPU 750 @ 2.67Ghz

# JDK 1.6

# 1.6.0_18, Java HotSpot(TM) 64-Bit Server VM, 16.0-b13, Sun Microsystems Inc., 
Benchmark_BitUtil_trunk.test_pop_array            : 15/20 rounds, time.total: 6.38, time.warmup: 1.67, time.bench: 4.72, round: 0.32 [+- 0.02]
Benchmark_BitUtil_trunk.test_pop_xor              : 15/20 rounds, time.total: 9.18, time.warmup: 2.28, time.bench: 6.90, round: 0.46 [+- 0.01]
Benchmark_BitUtil_trunk.test_pop_intersect        : 15/20 rounds, time.total: 9.24, time.warmup: 2.33, time.bench: 6.91, round: 0.46 [+- 0.01]
Benchmark_BitUtil_trunk.test_pop_andnot           : 15/20 rounds, time.total: 8.82, time.warmup: 2.17, time.bench: 6.66, round: 0.44 [+- 0.01]
Benchmark_BitUtil_trunk.test_pop_union            : 15/20 rounds, time.total: 9.13, time.warmup: 2.29, time.bench: 6.84, round: 0.46 [+- 0.01]
Benchmark_BitUtil_trunk.test_ntz_iterator_int     : 5/7 rounds, time.total: 34.72, time.warmup: 10.00, time.bench: 24.72, round: 4.94 [+- 0.07]
Benchmark_BitUtil_trunk.test_ntz_iterator_long    : 5/7 rounds, time.total: 42.58, time.warmup: 12.39, time.bench: 30.19, round: 6.04 [+- 0.05]
# 1.6.0_18, Java HotSpot(TM) 64-Bit Server VM, 16.0-b13, Sun Microsystems Inc., 
Benchmark_BitUtil_pop3264.test_pop_array          : 15/20 rounds, time.total: 6.21, time.warmup: 1.58, time.bench: 4.63, round: 0.31 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_xor            : 15/20 rounds, time.total: 9.19, time.warmup: 2.38, time.bench: 6.81, round: 0.45 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_intersect      : 15/20 rounds, time.total: 9.27, time.warmup: 2.28, time.bench: 6.99, round: 0.47 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_andnot         : 15/20 rounds, time.total: 8.74, time.warmup: 2.28, time.bench: 6.46, round: 0.43 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_union          : 15/20 rounds, time.total: 9.24, time.warmup: 2.31, time.bench: 6.93, round: 0.46 [+- 0.01]
# 1.6.0_18, Java HotSpot(TM) 64-Bit Server VM, 16.0-b13, Sun Microsystems Inc., 
Benchmark_BitUtil_popNtzJRE.test_pop_array        : 15/20 rounds, time.total: 4.16, time.warmup: 1.05, time.bench: 3.10, round: 0.21 [+- 0.00]
Benchmark_BitUtil_popNtzJRE.test_pop_xor          : 15/20 rounds, time.total: 7.32, time.warmup: 1.81, time.bench: 5.51, round: 0.37 [+- 0.01]
Benchmark_BitUtil_popNtzJRE.test_pop_intersect    : 15/20 rounds, time.total: 7.42, time.warmup: 1.84, time.bench: 5.58, round: 0.37 [+- 0.01]
Benchmark_BitUtil_popNtzJRE.test_pop_andnot       : 15/20 rounds, time.total: 6.74, time.warmup: 1.69, time.bench: 5.06, round: 0.34 [+- 0.01]
Benchmark_BitUtil_popNtzJRE.test_pop_union        : 15/20 rounds, time.total: 7.13, time.warmup: 1.79, time.bench: 5.34, round: 0.36 [+- 0.00]
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_int : 5/7 rounds, time.total: 25.39, time.warmup: 7.13, time.bench: 18.25, round: 3.65 [+- 0.03]
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_long: 5/7 rounds, time.total: 32.25, time.warmup: 9.57, time.bench: 22.68, round: 4.54 [+- 0.03]

# JDK 1.7

# 1.7.0-ea, Java HotSpot(TM) 64-Bit Server VM, 17.0-b06, Sun Microsystems Inc., 
Benchmark_BitUtil_trunk.test_pop_array            : 15/20 rounds, time.total: 6.45, time.warmup: 1.66, time.bench: 4.79, round: 0.32 [+- 0.02]
Benchmark_BitUtil_trunk.test_pop_xor              : 15/20 rounds, time.total: 9.35, time.warmup: 2.34, time.bench: 7.01, round: 0.47 [+- 0.01]
Benchmark_BitUtil_trunk.test_pop_intersect        : 15/20 rounds, time.total: 9.44, time.warmup: 2.37, time.bench: 7.08, round: 0.47 [+- 0.01]
Benchmark_BitUtil_trunk.test_pop_andnot           : 15/20 rounds, time.total: 8.62, time.warmup: 2.19, time.bench: 6.43, round: 0.43 [+- 0.01]
Benchmark_BitUtil_trunk.test_pop_union            : 15/20 rounds, time.total: 9.34, time.warmup: 2.31, time.bench: 7.03, round: 0.47 [+- 0.01]
Benchmark_BitUtil_trunk.test_ntz_iterator_int     : 5/7 rounds, time.total: 34.76, time.warmup: 10.16, time.bench: 24.60, round: 4.92 [+- 0.03]
Benchmark_BitUtil_trunk.test_ntz_iterator_long    : 5/7 rounds, time.total: 41.97, time.warmup: 12.02, time.bench: 29.95, round: 5.99 [+- 0.04]
# 1.7.0-ea, Java HotSpot(TM) 64-Bit Server VM, 17.0-b06, Sun Microsystems Inc., 
Benchmark_BitUtil_pop3264.test_pop_array          : 15/20 rounds, time.total: 6.30, time.warmup: 1.61, time.bench: 4.68, round: 0.31 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_xor            : 15/20 rounds, time.total: 9.33, time.warmup: 2.31, time.bench: 7.02, round: 0.47 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_intersect      : 15/20 rounds, time.total: 9.32, time.warmup: 2.33, time.bench: 6.99, round: 0.47 [+- 0.01]
Benchmark_BitUtil_pop3264.test_pop_andnot         : 15/20 rounds, time.total: 8.57, time.warmup: 2.17, time.bench: 6.39, round: 0.43 [+- 0.00]
Benchmark_BitUtil_pop3264.test_pop_union          : 15/20 rounds, time.total: 9.37, time.warmup: 2.31, time.bench: 7.06, round: 0.47 [+- 0.01]
# 1.7.0-ea, Java HotSpot(TM) 64-Bit Server VM, 17.0-b06, Sun Microsystems Inc., 
Benchmark_BitUtil_popNtzJRE.test_pop_array        : 15/20 rounds, time.total: 4.19, time.warmup: 1.05, time.bench: 3.14, round: 0.21 [+- 0.00]
Benchmark_BitUtil_popNtzJRE.test_pop_xor          : 15/20 rounds, time.total: 7.26, time.warmup: 1.82, time.bench: 5.44, round: 0.36 [+- 0.01]
Benchmark_BitUtil_popNtzJRE.test_pop_intersect    : 15/20 rounds, time.total: 7.29, time.warmup: 1.79, time.bench: 5.50, round: 0.37 [+- 0.01]
Benchmark_BitUtil_popNtzJRE.test_pop_andnot       : 15/20 rounds, time.total: 6.66, time.warmup: 1.69, time.bench: 4.97, round: 0.33 [+- 0.01]
Benchmark_BitUtil_popNtzJRE.test_pop_union        : 15/20 rounds, time.total: 7.13, time.warmup: 1.78, time.bench: 5.34, round: 0.36 [+- 0.01]
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_int : 5/7 rounds, time.total: 25.47, time.warmup: 7.17, time.bench: 18.31, round: 3.66 [+- 0.03]
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_long: 5/7 rounds, time.total: 32.79, time.warmup: 9.22, time.bench: 23.57, round: 4.71 [+- 0.03]

{code}","21/Jan/10 09:56;dawidweiss;An updated set of benchmarks (simple loops and JRE ntz/pop).","21/Jan/10 10:00;dawidweiss;Updated source code for the benchmarks.","21/Jan/10 10:57;dawidweiss;Confirmed, with a simple loop it is even faster.
{noformat}

# Windows 7/64, Intel(R) Core(TM) i7 CPU 920  @ 2.67GHz

#
# 1.7.0-ea, Java HotSpot(TM) 64-Bit Server VM, 17.0-b06, Sun Microsystems Inc.,
#
Benchmark_BitUtil_popNtzJRE.test_pop_array        : time.bench: 3.55, round: 0.24 [+- 0.02]
Benchmark_BitUtil_popNtzJRE.test_pop_xor          : time.bench: 6.45, round: 0.43 [+- 0.01]
Benchmark_BitUtil_popNtzJRE.test_pop_intersect    : time.bench: 6.42, round: 0.43 [+- 0.00]
Benchmark_BitUtil_popNtzJRE.test_pop_andnot       : time.bench: 6.10, round: 0.41 [+- 0.00]
Benchmark_BitUtil_popNtzJRE.test_pop_union        : time.bench: 6.42, round: 0.43 [+- 0.00]
                                                                                           
Benchmark_BitUtil_popNtzJRE_simple.test_pop_array : time.bench: 2.47, round: 0.16 [+- 0.00]
Benchmark_BitUtil_popNtzJRE_simple.test_pop_xor   : time.bench: 4.95, round: 0.33 [+- 0.00]
Benchmark_BitUtil_popNtzJRE_simple.test_pop_inters: time.bench: 4.96, round: 0.33 [+- 0.00]
Benchmark_BitUtil_popNtzJRE_simple.test_pop_andnot: time.bench: 5.12, round: 0.34 [+- 0.00]
Benchmark_BitUtil_popNtzJRE_simple.test_pop_union : time.bench: 5.03, round: 0.34 [+- 0.01]
{noformat}
","23/Jan/10 22:56;dawidweiss;I'm done with these benchmarks. The results so far indicate that 

a) if an accelerated (intrinsic) pop/ntz is available, a simple loop with Long.* methods is much faster then the current Lucene trunk's implementation. This setting is difficult to detect reliably from within Java though.

b) my tests on 32-bit systems indicate significant improvement in bit counting routines with table lookup implementation. This may vary between architectures.

","24/Jan/10 09:36;mikemccand;Maybe Lucene could do its own micro-benchmark in the current env to determine which impl should be used?  If the results are so stark depending on JVM/arch, presumably it wouldn't take much time for that test to run... and the determination would be saved statically.","26/Jan/10 13:50;stanislaw.osinski;I ran the benchmark on a 64bit Linux running an Intel(R) Xeon(R) E5520 @ 2.27GHz. I tried both Sun's JDK 1.7-ea as well as JDK 1.6.0_18, which also has support for native {{POPCNT}}.

*JDK 1.7-ea {{-server -XX:+UsePopCountInstruction}}*

{noformat}
# 1.7.0-ea-fastdebug, Java HotSpot(TM) 64-Bit Server VM, 17.0-b07-fastdebug, Sun Microsystems Inc.,
Benchmark_BitUtil_trunk.test_pop_array            : 15/20 rounds, time.total: 7.69, time.warmup: 1.96, time.bench: 5.73, round: 0.38 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_xor              : 15/20 rounds, time.total: 11.13, time.warmup: 2.81, time.bench: 8.32, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_intersect        : 15/20 rounds, time.total: 11.13, time.warmup: 2.82, time.bench: 8.32, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_andnot           : 15/20 rounds, time.total: 10.46, time.warmup: 2.66, time.bench: 7.80, round: 0.52 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_union            : 15/20 rounds, time.total: 11.13, time.warmup: 2.81, time.bench: 8.32, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_ntz_iterator_int     : 5/7 rounds, time.total: 42.30, time.warmup: 12.02, time.bench: 30.29, round: 6.06 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_ntz_iterator_long    : 5/7 rounds, time.total: 55.48, time.warmup: 15.43, time.bench: 40.05, round: 8.01 [+- 0.06], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
# 1.7.0-ea-fastdebug, Java HotSpot(TM) 64-Bit Server VM, 17.0-b07-fastdebug, Sun Microsystems Inc.,
Benchmark_BitUtil_pop3264.test_pop_array          : 15/20 rounds, time.total: 7.78, time.warmup: 2.05, time.bench: 5.73, round: 0.38 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_xor            : 15/20 rounds, time.total: 11.13, time.warmup: 2.82, time.bench: 8.32, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_intersect      : 15/20 rounds, time.total: 11.14, time.warmup: 2.82, time.bench: 8.32, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_andnot         : 15/20 rounds, time.total: 10.46, time.warmup: 2.66, time.bench: 7.80, round: 0.52 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_union          : 15/20 rounds, time.total: 11.13, time.warmup: 2.81, time.bench: 8.32, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
# 1.7.0-ea-fastdebug, Java HotSpot(TM) 64-Bit Server VM, 17.0-b07-fastdebug, Sun Microsystems Inc.,
Benchmark_BitUtil_popNtzJRE.test_pop_array        : 15/20 rounds, time.total: 5.06, time.warmup: 1.29, time.bench: 3.77, round: 0.25 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_xor          : 15/20 rounds, time.total: 8.54, time.warmup: 2.15, time.bench: 6.39, round: 0.43 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_intersect    : 15/20 rounds, time.total: 8.54, time.warmup: 2.15, time.bench: 6.39, round: 0.43 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_andnot       : 15/20 rounds, time.total: 7.81, time.warmup: 1.99, time.bench: 5.81, round: 0.39 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_union        : 15/20 rounds, time.total: 8.54, time.warmup: 2.15, time.bench: 6.39, round: 0.43 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_int : 5/7 rounds, time.total: 33.55, time.warmup: 8.72, time.bench: 24.83, round: 4.97 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_long: 5/7 rounds, time.total: 39.61, time.warmup: 11.48, time.bench: 28.12, round: 5.62 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
# 1.7.0-ea-fastdebug, Java HotSpot(TM) 64-Bit Server VM, 17.0-b07-fastdebug, Sun Microsystems Inc.,
Benchmark_BitUtil_popNtzJRE_simple.test_pop_array : 15/20 rounds, time.total: 3.25, time.warmup: 0.82, time.bench: 2.43, round: 0.16 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_xor   : 15/20 rounds, time.total: 5.05, time.warmup: 1.27, time.bench: 3.78, round: 0.25 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_intersect: 15/20 rounds, time.total: 5.05, time.warmup: 1.27, time.bench: 3.78, round: 0.25 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_andnot: 15/20 rounds, time.total: 5.34, time.warmup: 1.34, time.bench: 4.00, round: 0.27 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_union : 15/20 rounds, time.total: 5.05, time.warmup: 1.27, time.bench: 3.78, round: 0.25 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
VM option '+UsePopCountInstruction'
{noformat}


*JDK 1.7-ea {{-server -XX:-UsePopCountInstruction}}*

{noformat}

# 1.7.0-ea-fastdebug, Java HotSpot(TM) 64-Bit Server VM, 17.0-b07-fastdebug, Sun Microsystems Inc.,
Benchmark_BitUtil_trunk.test_pop_array            : 15/20 rounds, time.total: 8.43, time.warmup: 2.29, time.bench: 6.13, round: 0.41 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_xor              : 15/20 rounds, time.total: 11.97, time.warmup: 2.94, time.bench: 9.03, round: 0.60 [+- 0.03], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_intersect        : 15/20 rounds, time.total: 12.88, time.warmup: 3.25, time.bench: 9.63, round: 0.64 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_andnot           : 15/20 rounds, time.total: 12.19, time.warmup: 3.09, time.bench: 9.11, round: 0.61 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_union            : 15/20 rounds, time.total: 12.87, time.warmup: 3.25, time.bench: 9.63, round: 0.64 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_ntz_iterator_int     : 5/7 rounds, time.total: 46.44, time.warmup: 13.03, time.bench: 33.41, round: 6.68 [+- 0.01], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_ntz_iterator_long    : 5/7 rounds, time.total: 56.51, time.warmup: 16.31, time.bench: 40.20, round: 8.04 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
# 1.7.0-ea-fastdebug, Java HotSpot(TM) 64-Bit Server VM, 17.0-b07-fastdebug, Sun Microsystems Inc.,
Benchmark_BitUtil_pop3264.test_pop_array          : 15/20 rounds, time.total: 9.09, time.warmup: 2.38, time.bench: 6.71, round: 0.45 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_xor            : 15/20 rounds, time.total: 12.75, time.warmup: 3.22, time.bench: 9.54, round: 0.64 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_intersect      : 15/20 rounds, time.total: 12.75, time.warmup: 3.21, time.bench: 9.54, round: 0.64 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_andnot         : 15/20 rounds, time.total: 12.08, time.warmup: 3.06, time.bench: 9.02, round: 0.60 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_union          : 15/20 rounds, time.total: 12.76, time.warmup: 3.22, time.bench: 9.54, round: 0.64 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
# 1.7.0-ea-fastdebug, Java HotSpot(TM) 64-Bit Server VM, 17.0-b07-fastdebug, Sun Microsystems Inc.,
Benchmark_BitUtil_popNtzJRE.test_pop_array        : 15/20 rounds, time.total: 8.98, time.warmup: 2.28, time.bench: 6.70, round: 0.45 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_xor          : 15/20 rounds, time.total: 12.75, time.warmup: 3.21, time.bench: 9.54, round: 0.64 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_intersect    : 15/20 rounds, time.total: 12.76, time.warmup: 3.22, time.bench: 9.54, round: 0.64 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_andnot       : 15/20 rounds, time.total: 12.09, time.warmup: 3.06, time.bench: 9.03, round: 0.60 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_union        : 15/20 rounds, time.total: 11.94, time.warmup: 3.22, time.bench: 8.72, round: 0.58 [+- 0.04], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_int : 5/7 rounds, time.total: 32.77, time.warmup: 8.92, time.bench: 23.84, round: 4.77 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_long: 5/7 rounds, time.total: 39.61, time.warmup: 11.49, time.bench: 28.12, round: 5.62 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
# 1.7.0-ea-fastdebug, Java HotSpot(TM) 64-Bit Server VM, 17.0-b07-fastdebug, Sun Microsystems Inc.,
Benchmark_BitUtil_popNtzJRE_simple.test_pop_array : 15/20 rounds, time.total: 18.03, time.warmup: 4.52, time.bench: 13.51, round: 0.90 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_xor   : 15/20 rounds, time.total: 19.65, time.warmup: 4.93, time.bench: 14.73, round: 0.98 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_intersect: 15/20 rounds, time.total: 19.65, time.warmup: 4.93, time.bench: 14.73, round: 0.98 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_andnot: 15/20 rounds, time.total: 20.49, time.warmup: 5.14, time.bench: 15.35, round: 1.02 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_union : 15/20 rounds, time.total: 19.65, time.warmup: 4.93, time.bench: 14.73, round: 0.98 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
VM option '-UsePopCountInstruction'
{noformat}


*JDK 1.6.0_18 {{-server -XX:+UsePopCountInstruction}}*

{noformat}
# 1.6.0_18, Java HotSpot(TM) 64-Bit Server VM, 16.0-b13, Sun Microsystems Inc.,
Benchmark_BitUtil_trunk.test_pop_array            : 15/20 rounds, time.total: 8.42, time.warmup: 2.13, time.bench: 6.30, round: 0.42 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_xor              : 15/20 rounds, time.total: 12.12, time.warmup: 3.04, time.bench: 9.08, round: 0.61 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_intersect        : 15/20 rounds, time.total: 12.12, time.warmup: 3.04, time.bench: 9.08, round: 0.61 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_andnot           : 15/20 rounds, time.total: 11.53, time.warmup: 2.90, time.bench: 8.63, round: 0.58 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_union            : 15/20 rounds, time.total: 12.12, time.warmup: 3.04, time.bench: 9.08, round: 0.61 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_ntz_iterator_int     : 5/7 rounds, time.total: 46.85, time.warmup: 13.32, time.bench: 33.53, round: 6.71 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_ntz_iterator_long    : 5/7 rounds, time.total: 57.93, time.warmup: 16.79, time.bench: 41.15, round: 8.23 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
# 1.6.0_18, Java HotSpot(TM) 64-Bit Server VM, 16.0-b13, Sun Microsystems Inc.,
Benchmark_BitUtil_pop3264.test_pop_array          : 15/20 rounds, time.total: 8.43, time.warmup: 2.14, time.bench: 6.29, round: 0.42 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_xor            : 15/20 rounds, time.total: 12.12, time.warmup: 3.04, time.bench: 9.08, round: 0.61 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_intersect      : 15/20 rounds, time.total: 12.12, time.warmup: 3.04, time.bench: 9.08, round: 0.61 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_andnot         : 15/20 rounds, time.total: 11.53, time.warmup: 2.90, time.bench: 8.63, round: 0.58 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_union          : 15/20 rounds, time.total: 12.12, time.warmup: 3.04, time.bench: 9.08, round: 0.61 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
# 1.6.0_18, Java HotSpot(TM) 64-Bit Server VM, 16.0-b13, Sun Microsystems Inc.,
Benchmark_BitUtil_popNtzJRE.test_pop_array        : 15/20 rounds, time.total: 5.57, time.warmup: 1.42, time.bench: 4.15, round: 0.28 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_xor          : 15/20 rounds, time.total: 9.43, time.warmup: 2.36, time.bench: 7.07, round: 0.47 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_intersect    : 15/20 rounds, time.total: 9.43, time.warmup: 2.36, time.bench: 7.07, round: 0.47 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_andnot       : 15/20 rounds, time.total: 8.64, time.warmup: 2.18, time.bench: 6.46, round: 0.43 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_union        : 15/20 rounds, time.total: 9.42, time.warmup: 2.36, time.bench: 7.07, round: 0.47 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_int : 5/7 rounds, time.total: 34.36, time.warmup: 9.70, time.bench: 24.66, round: 4.93 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_long: 5/7 rounds, time.total: 43.89, time.warmup: 13.01, time.bench: 30.88, round: 6.18 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
# 1.6.0_18, Java HotSpot(TM) 64-Bit Server VM, 16.0-b13, Sun Microsystems Inc.,
Benchmark_BitUtil_popNtzJRE_simple.test_pop_array : 15/20 rounds, time.total: 3.58, time.warmup: 0.90, time.bench: 2.69, round: 0.18 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_xor   : 15/20 rounds, time.total: 5.56, time.warmup: 1.39, time.bench: 4.16, round: 0.28 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_intersect: 15/20 rounds, time.total: 5.56, time.warmup: 1.39, time.bench: 4.16, round: 0.28 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_andnot: 15/20 rounds, time.total: 5.89, time.warmup: 1.48, time.bench: 4.42, round: 0.29 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_union : 15/20 rounds, time.total: 5.56, time.warmup: 1.39, time.bench: 4.16, round: 0.28 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
{noformat}


*JDK 1.6.0_18 {{-server -XX:-UsePopCountInstruction}}*

{noformat}
# 1.6.0_18, Java HotSpot(TM) 64-Bit Server VM, 16.0-b13, Sun Microsystems Inc.,
Benchmark_BitUtil_trunk.test_pop_array            : 15/20 rounds, time.total: 7.67, time.warmup: 1.94, time.bench: 5.74, round: 0.38 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_xor              : 15/20 rounds, time.total: 11.02, time.warmup: 2.77, time.bench: 8.25, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_intersect        : 15/20 rounds, time.total: 11.02, time.warmup: 2.77, time.bench: 8.25, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_andnot           : 15/20 rounds, time.total: 10.49, time.warmup: 2.64, time.bench: 7.85, round: 0.52 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_pop_union            : 15/20 rounds, time.total: 11.02, time.warmup: 2.77, time.bench: 8.25, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_ntz_iterator_int     : 5/7 rounds, time.total: 42.61, time.warmup: 12.11, time.bench: 30.49, round: 6.10 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_trunk.test_ntz_iterator_long    : 5/7 rounds, time.total: 52.69, time.warmup: 15.27, time.bench: 37.42, round: 7.48 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
# 1.6.0_18, Java HotSpot(TM) 64-Bit Server VM, 16.0-b13, Sun Microsystems Inc.,
Benchmark_BitUtil_pop3264.test_pop_array          : 15/20 rounds, time.total: 7.69, time.warmup: 1.95, time.bench: 5.74, round: 0.38 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_xor            : 15/20 rounds, time.total: 11.02, time.warmup: 2.76, time.bench: 8.25, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_intersect      : 15/20 rounds, time.total: 11.02, time.warmup: 2.77, time.bench: 8.25, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_andnot         : 15/20 rounds, time.total: 10.48, time.warmup: 2.64, time.bench: 7.84, round: 0.52 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_pop3264.test_pop_union          : 15/20 rounds, time.total: 11.02, time.warmup: 2.76, time.bench: 8.25, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
# 1.6.0_18, Java HotSpot(TM) 64-Bit Server VM, 16.0-b13, Sun Microsystems Inc.,
Benchmark_BitUtil_popNtzJRE.test_pop_array        : 15/20 rounds, time.total: 7.67, time.warmup: 1.94, time.bench: 5.74, round: 0.38 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_xor          : 15/20 rounds, time.total: 11.02, time.warmup: 2.76, time.bench: 8.25, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_intersect    : 15/20 rounds, time.total: 11.02, time.warmup: 2.77, time.bench: 8.25, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_andnot       : 15/20 rounds, time.total: 10.48, time.warmup: 2.64, time.bench: 7.84, round: 0.52 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_pop_union        : 15/20 rounds, time.total: 11.02, time.warmup: 2.77, time.bench: 8.25, round: 0.55 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_int : 5/7 rounds, time.total: 31.27, time.warmup: 8.83, time.bench: 22.44, round: 4.49 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE.test_ntz_iterator_long: 5/7 rounds, time.total: 39.90, time.warmup: 11.84, time.bench: 28.06, round: 5.61 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
# 1.6.0_18, Java HotSpot(TM) 64-Bit Server VM, 16.0-b13, Sun Microsystems Inc.,
Benchmark_BitUtil_popNtzJRE_simple.test_pop_array : 15/20 rounds, time.total: 18.03, time.warmup: 4.51, time.bench: 13.52, round: 0.90 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_xor   : 15/20 rounds, time.total: 21.25, time.warmup: 5.01, time.bench: 16.24, round: 1.08 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_intersect: 15/20 rounds, time.total: 21.67, time.warmup: 5.43, time.bench: 16.25, round: 1.08 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_andnot: 15/20 rounds, time.total: 22.43, time.warmup: 5.62, time.bench: 16.81, round: 1.12 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
Benchmark_BitUtil_popNtzJRE_simple.test_pop_union : 15/20 rounds, time.total: 21.65, time.warmup: 5.42, time.bench: 16.23, round: 1.08 [+- 0.00], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
{noformat}

","19/Nov/12 11:58;jpountz;Now that we have dropped support for Java 5, maybe it would make sense to make Lucene use the JDK impl of ntz? According to the release notes, the numberOfTrailingZeros method was made an intrinsic in Java 6u18[1] which is nearly 3 years old now, so this sounds like a safe bet?

 [1] http://www.oracle.com/technetwork/java/javase/6u18-142093.html","19/Nov/12 12:46;dweiss;I think it makes sense to simplify the existing code. It's going to be faster and simpler on modern JVMs/ hardware where these are inlined as intrinsics.","19/Nov/12 13:34;yseeley@gmail.com;bq. Now that we have dropped support for Java 5, maybe it would make sense to make Lucene use the JDK impl of ntz?

+1","19/Nov/12 14:56;uschindler;+1, we could have done this already for 4.0! I think there was a proposal to do this, but some people said it might be too early (see above).","20/Nov/12 10:22;jpountz;Patch:
 - replace BitUtil.ntz with Long.numberOfTrailingZeros
 - replace BitUtil.pop with Long.bitCount
 - replace the pop_* methods with simple loops using Long.bitCount

All lucene & solr tests passed except some zk tests whose failure seem unrelated.","20/Nov/12 16:03;commit-tag-bot;[trunk commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1411712

LUCENE-2221: Use JVM intrinsics instead of BitUtil.{ntz,pop}.


","20/Nov/12 16:07;dweiss;Thanks Adrien.","20/Nov/12 16:12;commit-tag-bot;[branch_4x commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1411719

LUCENE-2221: Use JVM intrinsics instead of BitUtil.{ntz,pop} (merged from r1411712).


","20/Nov/12 16:16;jpountz;bq. Thanks Adrien.

Thanks Dawid for having done the hard work!

I'll give a look at Mike's benchs tonight to make sure this change didn't make anything worse...","22/Mar/13 16:15;commit-tag-bot;[branch_4x commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1411719

LUCENE-2221: Use JVM intrinsics instead of BitUtil.{ntz,pop} (merged from r1411712).

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CombinedNGramTokenFilter,LUCENE-1306,12398209,,New Feature,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,karl.wettin,karl.wettin,karl.wettin,13/Jun/08 04:49,10/Mar/13 13:32,30/Sep/19 08:38,10/Mar/13 13:32,,,,,,,,,,,,,,,modules/analysis,,,0,,,,"Alternative NGram filter that produce tokens with composite prefix and suffix markers.

{code:java}
ts = new WhitespaceTokenizer(new StringReader(""hello""));
ts = new CombinedNGramTokenFilter(ts, 2, 2);
assertNext(ts, ""^h"");
assertNext(ts, ""he"");
assertNext(ts, ""el"");
assertNext(ts, ""ll"");
assertNext(ts, ""lo"");
assertNext(ts, ""o$"");
assertNull(ts.next());
{code}",,,,,,,,,,,,,,,,"21/Jun/08 21:34;karl.wettin;LUCENE-1306.txt;https://issues.apache.org/jira/secure/attachment/12384440/LUCENE-1306.txt","13/Jun/08 04:52;karl.wettin;LUCENE-1306.txt;https://issues.apache.org/jira/secure/attachment/12383946/LUCENE-1306.txt",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-06-13 05:57:18.447,,,false,,,,,,,,,,,,,,,12442,,,Sun Mar 10 13:32:57 UTC 2013,New,Patch Available,,,,,,"0|i04xsn:",26740,,,,,,,,,"13/Jun/08 05:57;kawai;I'm sorry I could not see what it means that, ""combined"" + ""ngram"" from the code above. :( Can I ask you to let me know the intension?
","13/Jun/08 07:25;karl.wettin;The current NGram analysis in trunk is split in two, on for edge-grams and one for inner grams. 

This patch combines them both in a single filter that uses ^prefix and suffix$ tokens if they are some sort of edge gram, or both around the complete token if n is great enough. There is also method to extend if you want to add a payload (more boost to edge grams or something) or do something to the gram tokens depending on what part of the original token they contain.","15/Jun/08 03:22;otis;Should there be a way for the client of this class to specify the prefix and suffix char?

Is having, for example, ""^h"" as the first bi-gram token really the right thing to do?  Would ""^he"" make more sense?  I know that makes it 3 characters long, but it's 2 chars from the input string.  Not sure, so I'm asking.

Is this primarily to distinguish between the edge and inner n-grams?  If so, would it make more sense to just make use of Token type variable instead?
","17/Jun/08 02:56;kawai;After thinking for a week, I think this idea is nice.

IMHO, this might be renamed to NGramTokenizer simply. A general n-gram tokenizer accepts a sequence that has no gap in it. By the concept, TokenFilter accepts a tokien stream (gapped sequence), and current NGramTokenFilter does not work well in that sense. CombinedNGramTokenFilter filles the gap with prefix(^) and suffix($), and the token stream becomes a simple stream again virtually, n-gram works nice agian.

Comments:
1. prefix and suffix chars should be configurable. Because user must choose a char that is not used in the terms.
2. prefix and suffix might be a white space. Because most of the users are not interested in whitespace itself.
3. If you want to do a phrase query (for example, ""This is""), we have to generate $^ token in the gap to make the positions valid.
4. n-gram algorithm should be rewritten to make the positions valid. Please see LUCENE-1225

I think ""^h"" is OK, because prefix and suffix are the chars that was introduced as a workaround.
","17/Jun/08 20:08;gsingers;Note, also, that one could use the ""flags"" to indicate what the token is.  I know that's a little up in the air just yet, but it does exist.  This would mean that no stripping of special chars is required.","17/Jun/08 20:58;karl.wettin;I'll refine and document this patch soon. Terrible busy though. Hasty responses:

bq. Should there be a way for the client of this class to specify the prefix and suffix char? 
bq. 1. prefix and suffix chars should be configurable. Because user must choose a char that is not used in the terms.

There are getters and setters, but nothing in the constructor.

bq. Is having, for example, ""^h"" as the first bi-gram token really the right thing to do? Would ""^he"" make more sense? I know that makes it 3 characters long, but it's 2 chars from the input string. Not sure, so I'm asking.

I always considered 'start of word' and 'end of word' as a single character and a part of n. I might be wrong though. I'll have to take a look at what other people did. It would not be a very hard thing to include a setting for that.

bq. Is this primarily to distinguish between the edge and inner n-grams? If so, would it make more sense to just make use of Token type variable instead?
bq. one could use the ""flags"" to indicate what the token is. 

I might be missing something in your line of questioning. Don't understand what it would help to have the flag or token type as they are not stored in the index.

I don't want separate fields for the prefix, inner and suffix grams, I want to use the same single filter at query time. I typically pass down the gram boost in the payload, evaluated on gram size, how far away it is from the prefix and suffix, et c. 

bq. 3. If you want to do a phrase query (for example, ""This is""), we have to generate $^ token in the gap to make the positions valid.

If you are creating ngrams over multiple words, say a sentence, then I state that there should only be a prefix in the start of the senstance and a suffix in the end of the sentance and that grams will contain whitespace. I never did phrase queries using grams but I'd probably want prefix and suffix around each token. This is another good reason to keep them in the same field with prefix and suffix markers in the token, or?","18/Jun/08 05:06;kawai;First of all, my comment No.3 was not wrong, sorry. We don't have to insert $^ token in the ngram stream.

{quote}
I don't want separate fields for the prefix, inner and suffix grams, I want to use the same single filter at query time. 
{quote}

I agree with that. :)

Then, let's consider about the phrase query.
1. At store time, we want to store a sentence ""This is a pen""
2. At query time, we want to query with ""This is""

At store time, with WhitespaceTokenizer+CombinedNGramTokenFilter(2,2), we get:
^T Th hi is s$ ^i is s$ ^a a$ ^p pe en n$

At query time, with WhitespaceTokenizer+CombinedNGramTokenFilter(2,2), we get:
^T Th hi is s$ ^i is s$

We can find that the stored sequence because it contains the query sequence.

{quote}
If you are creating ngrams over multiple words, say a sentence, then I state that there should only be a prefix in the start of the senstance and a suffix in the end of the sentance and that grams will contain whitespace.
{quote}

If so, at query time, with WhitespaceTokenizer+CombinedNGramTokenFilter(2,2), we get:
""^T"",""Th"",""hi"",""is"",""s "","" i"",""is"",""s$""

We can't find the stored sequence because it does not contain the query sequence. n-gram query is always phrase query in the micro scope. 

+1 for prefix and suffix markers in the token.

{quote}
Note, also, that one could use the ""flags"" to indicate what the token is. I know that's a little up in the air just yet, but it does exist. 
{quote}

Yes, there is a flags. Of cource, we can use it. But I can't find the way to use them efficiently in THIS CASE, right now.

{quote}
This would mean that no stripping of special chars is required.
{quote}

Unfortunately, stripping is done outside of the ngram filter by WhitespaceTokenizer.","21/Jun/08 21:34;karl.wettin;New in this patch:
 * offsets as in NGramTokenFilter
 * token type ""^gram"", ""gram$"", ""^gram$"" and ""gram""
 * a bit of javadocs

There is also a todo I'll have to look in to some other day.

{code:java}
//  todo
//  /**
//   * if true, prefix and suffix does not count as a part of the ngram size.
//   * E.g. '^he' has as n of 2 if true and 3 if false
//   */
//  private boolean usingBoundaryCharsPartOfN = true;
{code}

This was not quite as simple to add as I hoped it would be and will try to find some time to fix that before I commit it.
","04/Aug/08 11:16;kawai;The files looks good for me.","12/Nov/08 17:41;otis;Could/should this not be folded into the existing Ngram code in contrib?
","10/Mar/13 13:32;erickerickson;SPRING_CLEANING_2013 We can reopen if necessary. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unncessary creation of object in org.apache.lucene.analysis.WordlistLoader.getWordSet(),LUCENE-1248,12392486,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,ddillard,ddillard,27/Mar/08 19:40,10/Mar/13 13:31,30/Sep/19 08:38,10/Mar/13 13:31,2.3.1,2.3.2,,,,,,,,,,,,,modules/analysis,,,0,,,,"Here's the function:

  public static HashSet getWordSet(File wordfile) throws IOException {
    HashSet result = new HashSet();
    FileReader reader = null;
    try {
      reader = new FileReader(wordfile);
      result = getWordSet(reader);
    }
    finally {
      if (reader != null)
        reader.close();
    }
    return result;
  }

The creation of the new HashSet object in the declaration of ""result"" is unnecessary.  Either ""result"" will be unconditionally set by the call to getWordSet() or an exception will occur.

This was detected by FindBugs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-10 13:31:42.899,,,false,,,,,,,,,,,,,,,12500,,,Sun Mar 10 13:31:42 UTC 2013,New,,,,,,,"0|i04y5j:",26798,,,,,,,,,"10/Mar/13 13:31;erickerickson;SPRING_CLEANING_2013 We can reopen if necessary. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow overriding a Document,LUCENE-778,12360660,,New Feature,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,hibou,hibou,17/Jan/07 09:08,10/Mar/13 13:19,30/Sep/19 08:38,10/Mar/13 13:19,2.0.0,,,,,,,,,,,,,,core/index,,,0,,,,"In our application, we have some kind of generic API that is handling how we are using Lucene. The different other applications are using this API with different semantics, and are using the Lucene fields quite differently. We wrote some usefull functions to do this mapping. Today, as the Document class cannot be overriden, we are obliged to make a document wrapper by application, ie some MyAppDocument and MyOtherAppDocument which have a property holding a real Lucene Document. Then, when MyApp or MyOtherApp want to use our generic lucene API, we have to ""get out"" the Lucene document, ie do some genericLuceneAPI.writeDoc(myAppDoc.getLuceneDocument()). This work fine, but it becomes quite tricky to use the other function of our generic API which is genericLuceneAPI.writeDocs(Collection<Document> docs).

I don't know the rational behind making final Document, but removing it will allow more object-oriented code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-01-17 10:03:29.64,,,false,,,,,,,,,,,,,,,12975,,,Sun Mar 10 13:19:30 UTC 2013,New,,,,,,,"0|i0512f:",27270,,,,,,,,,"17/Jan/07 10:03;mikemccand;One reason is because ""Lucene controls the construction of Documents from Hits"":

    http://www.gossamer-threads.com/lists/lucene/java-dev/31447

This issue periodically comes up, eg:

    http://www.gossamer-threads.com/lists/lucene/java-dev/32733

And there was at least this [closed, unresolved] issue from the past:

    http://issues.apache.org/jira/browse/LUCENE-358","17/Jan/07 10:21;hibou;Just after committing the jira issue, I just figured out that I haven't searched for topics about it. Sorry.
BTW, thanks for the pointers.

But what my request here is not making Lucene providing customized documents, like in LUCENE-662, it is about allowing passing to writer.addDocument() a document that is extending Document, nothing more.

For instance, I would like to do something like that :

public RDFDocument extends Document {
  public RDFDocument(String uri) {
    add(new Field(""uri"", uri); 
  }
  
  public void addStatement(String prop, String value) {
    add(new Field(prop, value));
  }
}

Should we move this discussion to lucene-dev ?
","17/Jan/07 17:24;cutting;If we decide to make Document non-final, then we must document very clearly the ways in which it may be subclassed, in particular that instances returned from IndexReader.document() will not be of the subclass.

> Should we move this discussion to lucene-dev ?

Since Jira comments are sent to lucene-dev, I see no reason.  The mailing list may be more appropriate for discussing general, process-related issues, but for specific technical issues, Jira is usually better.
","21/Jan/07 19:50;karl.wettin;Doug Cutting [17/Jan/07 09:24 AM]
> If we decide to make Document non-final, then we must document
> very clearly the ways in which it may be subclassed, in particular
> that instances returned from IndexReader.document() will not be 
>of the subclass. 

+1 for a more polymorphic Document. 

In fact, I would like to introduce interfaces for all ""low level"" classes. reader, writer, modifier, term, document, et c. And  I did that that I did in Lucene-550. I've had to definalized a lot of these classes and their methods to make Lucene compliant with a number of formula 1-A patterns.","23/Feb/07 22:44;hossman;From email...

http://www.nabble.com/-jira--Created%3A-%28LUCENE-778%29-Allow-overriding-a-Document-tf3026011.html

: A simple solution might be a 'classname' setup for the Document
: creation - like the default Directory implementation uses. As long as
: the subclass has a no-arg ctor it is trivial.

a differnet tack on the topic: there is really no good reason why the
""Document"" class used for indexing data should be the same as the
""Document"" classs ued for returning results ... using the same class in
this way results in all sort of confusio abotu which methods can be called
in which context, and frequently leads people to assume they can do safe
""round trips"" of their Documents ... doing a search, modifying a field
value, and then re-inexing it -- not considering what happens to
non-STOREd fields or field/document boosts.

any work done to change the Document API to make it easier to subclass
should probably start with a seperation of these too completley different
concepts.

One approach off the top of my head: make an IndexableDocument interface
for clients to pass to IndexWriter and a ""ReturnableDocument"" class for
IndexReader/IndexSearcher to return ... the existing Document class can
subclass ReturnableDocument and impliment IndexableDocument, the existing
methods with Document in their sig would be deprecated and replaced with
methods using one of these new class names

...some followup comments can be found in the thread archive.
","04/Mar/07 11:12;hibou;Marker interface is a nice idea, but I think this will make Document handling more painfull. In my use case this will not be optimal.
In our application, we have a kind of workflow of Document. We have a big/master index which is holding every data on the system, and then we have specialized index which a part of the big one. The big one is for making big global queries on the whole data. The specialized are specialized for the end application. So the workflow is :
* from the raw data, make them as Document and index it in the master index
* for each specialized index :
** do the specific query on the master index
** from the retreived document, redecorate it with specialized indexed field
** index the decorated documents in the specialized index

Here I just have to decorate the Document retrieved form the master index. With incompatible interfaces, this won't be possible anymore. I will have to reinstanciate a Document each time and repopulate it.
So why not keep IndexWriter#addDocument(Document), and just change IndexReader#doc(int) to make it return a kind of DocumentWithOnlyStoredData, with DocumentWithOnlyStoredData extends Document. (the proposed named is horrible, I know !)
","05/Mar/07 00:03;hossman;1) from a design standpoint, as long as IndexReader/IndexSearcher deal with documents using a different interface name i nthe signature then IndexWriter, it doens't really matter what name is used -- what i suggested tackles a second point: keeping the existing interface backwards compatible for the forseeable future while migrating to this modified API.

2) if you are already decorating the docs you get back from your ""master index"" before adding them to your specialized indexes, then nothing i described would prevent that from continuing to work ... the bulk of your code could be example the same, you would just need a simple DecoratedDocument which impliments IndexableDocument and wraps a ReturnableDocument.

something like that might even be a common enough use case to inlcude in the core ... anyone with a legitimate use cases for doing round trips could use it as well.","05/Mar/07 10:02;hibou;Note that I was talking about the future API, with some deprecated functions removed. So the API will look like :
class IndexReader {
  ReturnableDocument doc(int n);
}
class IndexWriter {
  void addDocument(IndexableDocument doc);
}

1) it does matter from the user point of view : we cannot do anymore writer.addDocument(reader.doc(10)).

2) Effectively I can implement a DecoratedDocument. But I cannot make Lucene instanciating my own document, the reader will still return some ReturnableDocument. Unless you want to allow the user to customize in Lucene the instanciation of Document by providing a factory of Document ?","05/Mar/07 21:49;hossman;
> 1) it does matter from the user point of view : we cannot do anymore
> writer.addDocument(reader.doc(10)).

...and i argue that is precisely what we want ... it's not generally safe for people do do that now, but becuase it is so easy, they think they can.  If we change the API so that doc(int) doesn't return the same class that addDocument takes we make go out of their way a little bit and need to write something like...

   writer.addDocument(new ReIndexedDocument(reader.doc(10)));

...where the RedIndexedDocument class can have some good javadocs explaining what it is, when to use it, and most importantly: when it doesn't work.

> 2) Effectively I can implement a DecoratedDocument. But I cannot make Lucene
> instanciating my own document, the 

...i'm still not clear why you feel you need lucene to instantiate your specific class ... why wouldn't a decorator class work for the use case you describe?

public class YourDocumentWraper implements IndexableDocument {
   public YourDocumentWraper(ReturnableDocument r, Object specialStuff) {
     ...
   }
   public Fieldable getFieldable(String f) {
     // ...check if you want special stuff, if not...
     return r.getFieldable(f);
  }
}



","05/Mar/07 23:18;gsingers;I feel like I'm missing something in this discussion, but couldn't we just make Document non-final and add:

public void document(int n, Document doc)

or even mark it as expert and call it populateDocument(int n, Document doc) or something like that

The semantics of which are to add the fields for document n to the Document object doc.  

From the looks of the code, most of the Readers first thing in the document call is: Document result = new Document()  so it is not like we are doing some complicated construction that is optimized for the different types of readers.



","06/Mar/07 09:04;hibou;Hoss> OK I got it.
In fact my concern was about ""semantic"". I agree with you that the API make the Lucene user think that every index data are retrieved while doing reader.doc(int i) (as I thought the first days using Lucene). But here you are proposing to completely separate the indexing from the retrieving, saying that they are not compatible with each other. I think this is wrong because basically you can retrieve a document and repushed the same in the index, even if it has no sense. But this was pure semantic concerns.
Now looking at the implementation you are proposing of a ""YourDocumentWraper"", we can make it work correctly without any performance issue. So I won't make a war is a such design is implemented ;)

Grant>In fact the discussion derived from the original issue. BTW, this would be nice !","06/Mar/07 19:36;hossman;Grant: I have no serious objection to making Document non-final ... i was just pointing out that as long as we are mucking with the Document related APIs, this may be a good time to solve the perception issue people have with what a ""Document"" is ... that single container class gets used in two very different ways, and clarifying what those ways are with seperate Interfaces would be useful.  from a more parcticle standpoint, if i want to subclass Document and add custom behavior, what methods impact the way a document gets indexed?  what methods impact the behavior of returned data? ... the answers to thouse questions aren't obvious to most novice users as is -- but we tend to punt on them since no one can ""replace"" a Document instance at the moment ... we'd need to be more clear about that if subclassing is allowed and DocumentFactories or passing empty Beans to IndexReader.document start being allowed.

regarding your specific question about...
   public void document(int n, Document doc) 

...i'm leary of that approach just because of all the places people might want it ... with FieldSelector or without; from a Searcher, or an IndexReader, or a Hits instance, etc...

I'd be more in favor of adding a ""setDocumentFactory"" method on IndexReader or something like that if we went that route ... but as i said, if Document (or whatever interface/abstract class we make IndexReader.document return) isn't final, then it's not clera to me why a Decorator model wouldn't be good enough.","10/Mar/07 17:46;hibou;Rethinking about the function ""public void document(int n, Document doc)"", in fact it will completely break the work I have done for LUCENE-662.

And finally, I agree with you Hoss. Two different interfaces, and let the user implement the document he wants. As a first step, the user will decorate his document. And in a second time, Lucene could provide the user the possibility to have his own DocumentFactory.
","02/Nov/07 09:57;michaelbusch;Maybe we should deprecate the document() methods and add
getStoredFields() methods that return a Map<String, Field>?
Then we could also definalize Document. Thoughts?","02/Nov/07 12:55;yseeley@gmail.com;> getStoredFields() methods that return a Map<String, Field>

Or a Map<String,Field[]> or Map<String,Object> where Object could be Field or Field[]
Unless it was a LinkedHashMap, that would also lose the ordering of different fields.

Document is lightweight now...pretty much just a List<Fieldable> that the user can get directly at.
If anything, a factory for Fieldable (which for Field, isn't as lightweight) would be more useful than a Document factory.
","10/Mar/13 13:19;erickerickson;SPRING_CLEANING_2013 We can reopen if necessary. Think this code has been extensively re-worked anyway.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default field in query syntax documentation has confusing error,LUCENE-4718,12629251,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,haydenmuhl,haydenmuhl,25/Jan/13 10:06,05/Feb/13 23:16,30/Sep/19 08:38,05/Feb/13 23:11,4.0,,,,,,,,,,4.2,,,,core/queryparser,,,0,documentation,,,"The explanation of default search fields uses two different queries that are supposed to be semantically the same, but the query text changes between the two examples.",,300,300,,0%,300,300,,,,,,,,,"25/Jan/13 10:08;haydenmuhl;SOLR-4357.patch;https://issues.apache.org/jira/secure/attachment/12566493/SOLR-4357.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-02-05 23:06:12.298,,,false,,,,,,,,,,,,,,,309243,,,Tue Feb 05 23:16:11 UTC 2013,New,Patch Available,,,,,,"0|i1e9sf:",291515,,,,,,,,,"25/Jan/13 10:08;haydenmuhl;Small fix for documentation.","05/Feb/13 23:06;commit-tag-bot;[trunk commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1442786

LUCENE-4718: Fix documentation of oal.queryparser.classic.

","05/Feb/13 23:11;jpountz;Committed! Thanks Hayden!","05/Feb/13 23:16;commit-tag-bot;[branch_4x commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1442790

LUCENE-4718: Fix documentation of oal.queryparser.classic (merged from r1442786).

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make PackedInts encoding from/to byte byte-aligned,LUCENE-4726,12629456,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,jpountz,jpountz,jpountz,27/Jan/13 22:30,29/Jan/13 15:36,30/Sep/19 08:38,29/Jan/13 15:30,,,,,,,,,,,,,,,,,,0,,,,"I had to do it for a patch for LUCENE-4609, but even if faceting doesn't end up using PackedInts, I think it would be better if encoding and decoding were byte-aligned instead of long-aligned (although encoding from/to long still needs to be long-aligned).",,,,,,,,,,,,,,,,"27/Jan/13 23:04;jpountz;LUCENE-4726.patch;https://issues.apache.org/jira/secure/attachment/12566694/LUCENE-4726.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-29 13:24:12.569,,,false,,,,,,,,,,,,,,,309952,,,Tue Jan 29 15:36:11 UTC 2013,New,,,,,,,"0|i1hhpz:",310296,,,,,,,,,"27/Jan/13 23:04;jpountz;Patch. This doesn't seem to hurt Lucene41PF performance, it's maybe even a little better:

{noformat}
TaskQPS Lucene41      StdDevQPS byte-aligned      StdDev                Pct diff
                PKLookup      328.35      (2.8%)      327.69      (2.6%)   -0.2% (  -5% -    5%)
                 Prefix3      681.89      (2.7%)      682.33      (3.3%)    0.1% (  -5% -    6%)
              AndHighLow     2837.83      (5.1%)     2843.82      (5.2%)    0.2% (  -9% -   11%)
             AndHighHigh      201.72      (1.1%)      202.19      (1.3%)    0.2% (  -2% -    2%)
              OrHighHigh       94.25      (7.0%)       94.53      (7.5%)    0.3% ( -13% -   15%)
              AndHighMed      678.40      (2.3%)      680.57      (2.6%)    0.3% (  -4% -    5%)
         MedSloppyPhrase      249.77      (1.7%)      250.67      (1.9%)    0.4% (  -3% -    4%)
               OrHighLow      424.97      (6.4%)      426.56      (6.9%)    0.4% ( -12% -   14%)
                Wildcard      258.81      (2.4%)      259.78      (3.1%)    0.4% (  -5% -    6%)
               OrHighMed      393.80      (6.1%)      395.47      (6.8%)    0.4% ( -11% -   14%)
             LowSpanNear      138.76      (3.6%)      139.46      (3.3%)    0.5% (  -6% -    7%)
            HighSpanNear      106.41      (3.3%)      107.05      (2.9%)    0.6% (  -5% -    7%)
                  Fuzzy1      125.72      (2.5%)      126.52      (2.6%)    0.6% (  -4% -    5%)
         LowSloppyPhrase      402.33      (1.7%)      404.95      (2.1%)    0.7% (  -3% -    4%)
                  IntNRQ       98.61      (7.9%)       99.31     (10.6%)    0.7% ( -16% -   20%)
             MedSpanNear      174.14      (3.7%)      175.53      (3.4%)    0.8% (  -6% -    8%)
        HighSloppyPhrase       36.23      (3.7%)       36.52      (4.7%)    0.8% (  -7% -    9%)
                 LowTerm     3629.10      (3.7%)     3659.26      (4.5%)    0.8% (  -7% -    9%)
                 Respell      137.12      (2.4%)      138.36      (2.9%)    0.9% (  -4% -    6%)
                 MedTerm     1301.50      (4.2%)     1314.41      (8.3%)    1.0% ( -11% -   14%)
               MedPhrase      273.01      (3.9%)      275.83      (3.4%)    1.0% (  -6% -    8%)
               LowPhrase      150.72      (5.6%)      152.36      (4.7%)    1.1% (  -8% -   12%)
                  Fuzzy2       49.40      (2.9%)       49.94      (3.4%)    1.1% (  -4% -    7%)
              HighPhrase      134.89      (6.1%)      136.73      (5.7%)    1.4% (  -9% -   13%)
                HighTerm      576.10      (4.4%)      586.79     (10.2%)    1.9% ( -12% -   17%)
{noformat}","29/Jan/13 13:24;commit-tag-bot;[trunk commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1439886

LUCENE-4726: Make PackedInts encoding from/to byte byte-aligned.

","29/Jan/13 15:31;jpountz;I'll have a look at the nightly benchs tomorrow to make sure I didn't break anything.","29/Jan/13 15:36;commit-tag-bot;[branch_4x commit] Adrien Grand
http://svn.apache.org/viewvc?view=revision&revision=1439938

LUCENE-4726: Make PackedInts encoding from/to byte byte-aligned (merged from r1439886).

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Adding ""d"" character to default ElisionFilter",LUCENE-3931,12548453,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,dadoonet,dadoonet,28/Mar/12 08:47,14/Jan/13 15:04,30/Sep/19 08:38,14/Jan/13 14:59,,,,,,,,,,,,,,,core/index,,,0,,,,"As described in Wikipedia (http://fr.wikipedia.org/wiki/%C3%89lision), the d character is used in french as an elision character.
E.g.: déclaration d'espèce
So, it would be useful to have it as a default elision token.

{code:title=ElisionFilter.java|borderStyle=solid}
  private static final CharArraySet DEFAULT_ARTICLES = CharArraySet.unmodifiableSet(
      new CharArraySet(Version.LUCENE_CURRENT, Arrays.asList(
          ""l"", ""m"", ""t"", ""qu"", ""n"", ""s"", ""j"", ""d""), true));
{code}

HTH
David.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-01-14 14:23:30.083,,,false,,,,,,,,,,,,,,,233550,,,Mon Jan 14 15:04:12 UTC 2013,New,,,,,,,"0|i04hfr:",24090,,,,,,,,,"14/Jan/13 14:23;martijn.v.groningen;This makes sense to me.","14/Jan/13 14:38;teofili;that's true for Italian as well.","14/Jan/13 14:41;sarowe;Because ElisionFilter use is used by more than just French, the set of contractions was moved out of ElisionFilter (LUCENE-3884).

The issue of missing French contractions has already been addressed, in LUCENE-4662.

I didn't notice this issue - I would have resolved it when I resolved LUCENE-4662.

So Martijn, unless there is some other reason to keep this issue open, I think it can be resolved as a duplicate.","14/Jan/13 14:45;sarowe;bq. that's true for Italian as well.

[ItalianAnalyzer|http://svn.apache.org/viewvc/lucene/dev/tags/lucene_solr_4_0_0/lucene/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java?revision=1396952&view=markup#l53] includes ""d"" in the list of contractions it gives to ElisionFilter.
","14/Jan/13 14:46;teofili;ok, thanks for clarifying Steve.","14/Jan/13 14:59;martijn.v.groningen;I see. I'll close it.","14/Jan/13 15:04;dadoonet;Thanks all!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
das,LUCENE-4507,12613638,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Incomplete,,tyh73bac,tyh73bac,26/Oct/12 11:47,26/Oct/12 13:33,30/Sep/19 08:38,26/Oct/12 13:33,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-10-26 13:33:19.867,,,false,,,,,,,,,,,,,,,251234,,,Fri Oct 26 13:33:19 UTC 2012,,,,,,,,"0|i0b65b:",63108,,,,,,,,,"26/Oct/12 13:33;erickerickson;Nothing here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DisjunctionSumScorer and ScorerDocQueue javadocs and one method name out of date after move from skipTo() to advance(),LUCENE-2337,12459790,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,paul.elschot@xs4all.nl,paul.elschot@xs4all.nl,21/Mar/10 09:52,19/Aug/12 09:42,30/Sep/19 08:38,19/Aug/12 09:41,,,,,,,,,,,4.1,,,,core/search,general/javadocs,,0,,,,,,,,,,,,,,,,,,,,"21/Mar/10 10:12;paul.elschot@xs4all.nl;LUCENE-2337.patch;https://issues.apache.org/jira/secure/attachment/12439403/LUCENE-2337.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-21 10:09:02.702,,,false,,,,,,,,,,,,,,,11458,,,Sun Aug 19 09:41:44 UTC 2012,New,,,,,,,"0|i04rev:",25706,,,,,,,,,"21/Mar/10 10:09;uschindler;Also Javadocs of lots of Scorers or DocIdSetIterators mention a return value of -1 for nextDoc() and advance() instead of NO_MORE_DOCS.","21/Mar/10 10:12;paul.elschot@xs4all.nl;The patch updates the javadocs for the move from skipTo() to advance().

It also renames the public method ScorerDocQueue.topSkipToAndAdjustElsePop to topAdvanceAndAdjustElsePop.
Although this is a public API change, I don't think it is worthwhile to use deprecation for this, because the method is really Lucene internal.

There are also some older things changed: the remark that skipTo() is used is removed (all scorers meanwhile have implemented skipTo() and have moved to advance()), and some commented code lines have are removed.","21/Mar/10 10:16;paul.elschot@xs4all.nl;Is there an easy way to determine all places where the javadocs mention a  -1 that should be NO_MORE_DOCS ?","21/Mar/10 11:22;shaie;Note that -1 is a valid return value in case doc() is called before nextDoc(). However it is not valid for nextDoc() and advance().","19/Aug/12 09:41;paul.elschot@xs4all.nl;Later patches have meanwhile inlined the scorer queue and removed the offending javadocs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check what's Jenkins pattern for e-mailing log fragments (so that it includes failures).,LUCENE-4092,12558661,,Task,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,sarowe,dweiss,dweiss,30/May/12 19:51,30/Jul/12 21:55,30/Sep/19 08:38,30/Jul/12 21:55,,,,,,,,,,,,,,,general/test,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-30 20:16:25.243,,,false,,,,,,,,,,,,,,,243873,,,Mon Jul 30 21:55:51 UTC 2012,New,,,,,,,"0|i04gfj:",23927,,,,,,,,,"30/May/12 20:16;sarowe;*Editable Email Notification*/*Default Content* configuration for the {{Lucene-Solr-tests-only-trunk}} job:

{noformat}
Build: ${BUILD_URL}

${FAILED_TESTS}

Build Log (for compile errors):
${BUILD_LOG_REGEX,regex=""\\[javac\\]\\s+\\d+ error(s*)\\b"",linesBefore=100,linesAfter=0}
{noformat}","30/May/12 20:22;dweiss;Yeah, but what is ${FAILED_TESTS}? I mean -- can we edit it to detect ""<<< FAILURES!"" and report 100 lines before this string?","30/May/12 20:47;sarowe;bq. Yeah, but what is $\{FAILED_TESTS}? I mean -- can we edit it to detect ""<<< FAILURES!"" and report 100 lines before this string?


{panel:title=Jenkins's *Content Token Reference*}
All arguments are optional. Arguments may be given for each token in the form _name=""value""_ for strings and in the form _name=value_ for booleans and numbers.  The \{'s and \}'s may be omitted if there are no arguments.

Examples: $TOKEN, $\{TOKEN}, $\{TOKEN, count=100}, $\{ENV, var=""PATH""}

*Available Tokens*

* *$\{DEFAULT_SUBJECT}* - This is the default email subject that is configured in Jenkins's system configuration page. 
* *$\{DEFAULT_CONTENT}* -  This is the default email content that is configured in Jenkins's system configuration page. 
* *$\{PROJECT_DEFAULT_SUBJECT}* -  This is the default email subject for this project.  The result of using this token in the advanced configuration is what is in the Default Subject field above. WARNING: Do not use this token in the Default Subject or Content fields.  Doing this has an undefined result. 
* *$\{PROJECT_DEFAULT_CONTENT}* -  This is the default email content for this project.  The result of using this token in the advanced configuration is what is in the Default Content field above. WARNING: Do not use this token in the Default Subject or Content fields.  Doing this has an undefined result. 
* *$\{BUILD_LOG, _maxLines_, _escapeHtml_}* - Displays the end of the build log.
** _maxLines_ - display at most this many lines of the log.\\
Defaults to 250.
** _escapeHtml_ - If true, HTML is escaped.\\
Defaults to false.
* *$\{BUILD_LOG_REGEX, _regex_, _linesBefore_, _linesAfter_, _maxMatches_, _showTruncatedLines_, _substText_, _escapeHtml_, _matchedLineHtmlStyle_}* - Displays lines from the build log that match the regular expression.
** _regex_ - Lines that match this regular expression are included. See also _java.util.regex.Pattern_\\
Defaults to ""(?i)\b(error|exception|fatal|fail(ed|ure)|un(defined|resolved))\b"".
** _linesBefore_ - The number of lines to include before the matching line. Lines that overlap with another match or _linesAfter_ are only included once.\\
Defaults to 0.
** _linesAfter_ - The number of lines to include after the matching line. Lines that overlap with another match or _linesBefore_ are only included once.\\
Defaults to 0.
** _maxMatches_ - The maximum number of matches to include. If 0, all matches will be included.\\
Defaults to 0.
** _showTruncatedLines_ - If _true_, include {{[...truncated ### lines...]}} lines.\\
Defaults to true.
** _substText_ - If non-null, insert this text into the email rather than the entire line.\\
Defaults to null.
** _escapeHtml_ - If true, escape HTML.\\
Defaults to false.
** _matchedLineHtmlStyle_ - If non-null, output HTML. matched lines will become {{<b style=""your-style-value"">html escaped matched line</b>}}.\\
Defaults to null.
* *$\{BUILD_LOG_EXCERPT, _start_, _end_}* - Displays an excerpt from the build log.
** _start_ - Regular expression to match the excerpt starting line to be included (exluded). See [_java.util.regex.Pattern_|http://download.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html]
** _end_ - Regular expression to match the excerpt ending line to be included (exluded)
* *$\{BUILD_NUMBER}* - Displays the number of the current build.
* *$\{BUILD_STATUS}* - Displays the status of the current build. (failing, success, etc...)
* *$\{BUILD_URL}* - Displays the URL to the current build.
* *$\{CHANGES, _showPaths_, _showDependencies_, _format_, _pathFormat_}* - Displays the changes since the last build.
** _showDependencies_ - if true, changes to projects this build depends on are shown.\\
Defaults to false.
** _showPaths_ - if true, the paths modified by a commit are shown.\\
Defaults to false.
** _format_ - for each commit listed, a string containing %X, where %X is one of %a for author, %d for date, %m for message, %p for paths, or %r for revision.  Not all revision systems support %d and %r.  If specified, _showPaths_ is ignored.\\
Defaults to ""[%a] %m\n"".
** _pathFormat_ - a string containing %p to indicate how to print paths.\\
Defaults to ""\t%p\n"".
* *$\{CHANGES_SINCE_LAST_SUCCESS, _reverse_, _format_, _showPaths_, _changesFormat_, _pathFormat_}* - Displays the changes since the last successful build.
<ul>
** _reverse_ - indicates that most recent builds should be at the top.\\
Defaults to false.
** _format_ - for each build listed, a string containing %X, where %X is one of %c for changes, or %n for build number.\\
Defaults to ""Changes for Build #%n\n%c\n"".
** _showPaths_, _changesFormat_, _pathFormat_ - defined as _showPaths_, _format_, and _pathFormat_ from $\{CHANGES}, respectively.
* *$\{CHANGES_SINCE_LAST_UNSTABLE, _reverse_, _format_, _showPaths_, _changesFormat_, _pathFormat_}* - Displays the changes since the last unstable or successful build.
** _reverse_ - indicates that most recent builds should be at the top.\\
Defaults to false.
** _format_ - for each build listed, a string containing %X, where %X is one of %c for changes, or %n for build number.\\
Defaults to ""Changes for Build #%n\n%c\n"".
** _showPaths_, _changesFormat_, _pathFormat_ - defined as _showPaths_, _format_, and _pathFormat_ from $\{CHANGES}, respectively.
* *$\{ENV, _var_}* - Displays an environment variable.
** _var_ - the name of the environment variable to display.  If """", show all.\\
Defaults to """".
* *$\{FAILED_TESTS, _showStack_, _maxTests_}* - Displays failing unit test information, if any tests have failed.
** _showStack_ - indicates that most recent builds should be at the top.\\
Defaults to true.
** _maxTests_ - display at most this many failing tests.\\
No limit is set by default.
* *$\{JENKINS_URL}* - Displays the URL to the Jenkins server. (You can change this on the system configuration page.)
* *$\{HUDSON_URL}* - _*deprecated, please use $JENKINS_URL*_
* *$\{PROJECT_NAME}* - Displays the project's name.
* *$\{PROJECT_URL}* - Displays a URL to the project's page.
* *$\{SVN_REVISION}* - Displays the subversion revision number.
* *$\{CAUSE}* - Displays the cause of the build.
* *$\{JELLY_SCRIPT, _template_}* - Custom message content generated from a Jelly script template. There are two templates provided: ""html"" and ""text"". Custom Jelly templates should be placed in $JENKINS_HOME/email-templates. When using custom templates, the template filename without "".jelly"" should be used for the ""template"" argument.
** _template_ - the template name.\\
Defaults to ""html"".
* *$\{FILE, _path_}* - Includes the content of a specified file.
** _path_ - The path to the file. Relative to the workspace root.
* *$\{TEST_COUNTS, _var_}* - Displays the number of tests.
** _var_ - Defaults to ""total"".
*** total - the number of all tests. 
*** fail - the number of failed tests.
*** skip - the number of skipped tests. 
* *$\{SCRIPT, _script_, _template_, _init_}* - Custom message content generated from a script using JSR 223. Custom scripts should be placed in $JENKINS_HOME/email-templates. When using custom scripts, the script filename WITH .py/.rb/etc should be used for the ""script"" argument.\\
templates and other items may be loaded using the host.readFile(String fileName) function\\
the function will look in the resources for the email-ext plugin first, and then in the $JENKINS_HOME/email-templates directory. No other directories will be searched.
** _script_ - the script name.\\
Defaults to ""email-ext.groovy"".
** _template_ - the template filename.\\
Defaults to ""groovy-html.template""
** _init_ - true to run the language's init script.\\
Defaults to true
** Available Script Engines
*** _ECMAScript_ - 1.8 (js)
*** _Groovy_ - 1.8.5 (groovy)
{panel}","30/May/12 20:59;dweiss;Ok, so BUILD_LOG_REGEX should do! I don't know Jenkins but any failed test (suite) will have the ""<<< FAILURES!"" marker attached -- feel free to experiment ;)","30/May/12 21:13;sarowe;I'll switch it to the following - hopefully it will capture everything (any length multiline) between the suite header and ""<<< FAILURES!""

{noformat}
Build: ${BUILD_URL}

${FAILED_TESTS}

Build Log (for compile errors):
${BUILD_LOG_REGEX,regex=""(?s:\\[java4\\]\\s*Suite:.*?<<<\\s*FAILURES!)""}
{noformat}","30/May/12 21:19;sarowe;bq. regex=""(?s:\\[java4\\]\\s*Suite:.*?<<<\\s*FAILURES!)""

Hmm, that won't work - it'll grab everything from the first ""Suite:"" to ""<<< FAILURES!"", including any number of (non-failing) Suite mentions inbetween.

I guess ""<<< FAILURES!"" and 100 previous lines will work, but I'd rather get the exact region.  I'll work on it.
","30/May/12 21:28;dweiss;This will be a killer regexp, I can feel it ;)","30/May/12 21:33;rcmuir;Is there also a way we could improve the output of other checks (e.g. the javadocs warnings task, two javadocs checkers in javadocs-lint, and the rat-checker)
so that if it causes a build failure its included as well, rather than just ""No tests ran"" or ""All tests passed"" or whatever it does today?","30/May/12 21:34;rcmuir;By the way: I'm not suggesting to make the regex even more hairy, i'm just wondering if we can modify these tasks
so that when they fail, they can include certain symbols/stuff to ensure they are included in the summary...
","30/May/12 21:54;sarowe;This one seems to work (from Perl against a recent Jenkins log with a failure):

{noformat}
[^\r\n]*\[junit4\]\s*Suite:.*[\r\n]+[^\r\n]*\[junit4\]\s*(?!Completed)(?!IGNOR)\S(?s:.*?)<<<\s*FAILURES!
{noformat}

I'll change the {{Lucene-Solr-tests-only-trunk}} job configuration to use it (after escaping backslashes).","30/May/12 22:04;sarowe;{quote}
Is there also a way we could improve the output of other checks (e.g. the javadocs warnings task, two javadocs checkers in javadocs-lint, and the rat-checker)
so that if it causes a build failure its included as well, rather than just ""No tests ran"" or ""All tests passed"" or whatever it does today?
{quote}

It may be that alternates can be added to the BUILD_LOG_REGEX - I'll look at the output and see if any changes need to be made to enable that.","31/May/12 17:00;sarowe;I plan on adding the following (as suggested by Robert) as alternations to the BUILD_LOG_REGEX for all non-Maven Jenkins jobs (some of these things don't run under the Maven jobs, and Maven's output is different enough that it'll require separate treatment):

bq. the javadocs warnings task

{noformat}
(?:[^\r\n]*\[javadoc\].*\r?\n)*[^\r\n]*\[javadoc\]\s*[1-9]\d*\s+warnings.*\r?\n
{noformat}

bq. two javadocs checkers in javadocs-lint

Output from javadocs-lint seems to show up only when there's a problem, so any output from it will always be extracted by the following regex:

{noformat}
[^\r\n]*javadocs-lint:.*\r?\n(?:[^\r\n]*\[echo\].*\r?\n)*
{noformat}

bq. and the rat-checker

{noformat}
[^\r\n]*rat-sources:\s+\[echo\].*(?:\r?\n[^\r\n]*\[echo\].*)*\s*[1-9]\d*\s+Unknown\s+Licenses.*\r?\n(?:[^\r\n]*\[echo\].*\r?\n)*
{noformat}

Along with two others:

# Compilation failures:
{noformat}
(?:[^\r\n]*\[javac\].*\r?\n)*[^\r\n]*\[javac\]\s*[1-9]\d*\s*error.*\r?\n
{noformat}
# Jenkins FATAL errors:
{noformat}
[^\r\n]*FATAL:(?s:.*)
{noformat}
","31/May/12 17:18;sarowe;bq. I plan on adding the following (as suggested by Robert) as alternations to the BUILD_LOG_REGEX for all non-Maven Jenkins jobs

Done.  Here's the full *Editable Email Notification*/*Default Content* configuration:

{noformat}
Build: ${BUILD_URL}

${FAILED_TESTS}

Build Log:
${BUILD_LOG_REGEX,regex=""(?x:
# Compilation failures
(?:[^\\r\\n]*\\[javac\\].*\\r?\\n)*[^\\r\\n]*\\[javac\\]\\s*[1-9]\\d*\\s*error.*\\r?\\n
# Test failures
|[^\\r\\n]*\\[junit4\\]\\s*Suite:.*[\\r\\n]+[^\\r\\n]*\\[junit4\\]\\s*(?!Completed)(?!IGNOR)\\S(?s:.*?)<<<\\s*FAILURES!
# License problems
|[^\\r\\n]*rat-sources:\\s+\\[echo\\].*(?:\\r?\\n[^\\r\\n]*\\[echo\\].*)*\\s*[1-9]\\d*\\s+Unknown\\s+Licenses.*\\r?\\n(?:[^\\r\\n]*\\[echo\\].*\\r?\\n)*
# Javadocs warnings
|(?:[^\\r\\n]*\\[javadoc\\].*\\r?\\n)*[^\\r\\n]*\\[javadoc\\]\\s*[1-9]\\d*\\s+warnings.*\\r?\\n
# Other javadocs problems (broken links and missing javadocs)
|[^\\r\\n]*javadocs-lint:.*\\r?\\n(?:[^\\r\\n]*\\[echo\\].*\\r?\\n)*
# Jenkins problems
|[^\\r\\n]*FATAL:(?s:.*)
)""}
{noformat}","31/May/12 17:44;sarowe;I'm going to add one more to the regex:

{noformat}
# Third-party dependency license/notice problems
|[^\\r\\n]*validate:.*\\r?\\n[^\\r\\n]*\\[echo\\].*\\r?\\n(?:[^\\r\\n]*\\[licenses\\].*\\r?\\n)*[^\\r\\n]*\\[licenses\\].*[1-9]\\d*\\s+error.*\\r?\\n
{noformat}","31/May/12 17:48;sarowe;bq. I'm going to add one more to the regex

Done - added to the configuration on all non-Maven Jenkins jobs","31/May/12 18:15;rcmuir;awesome! thank you!","31/May/12 20:09;dweiss;Thanks for working on this, Steve. It'll really be useful.","31/May/12 22:08;sarowe;Two problems:

# Spreading the BUILD_LOG_REGEX regex value over multiple lines is not supported by Jenkins's email templating functionality, which is provided by the Jenkins Email Extension Plugin (email-ext) [https://wiki.jenkins-ci.org/display/JENKINS/Email-ext+plugin].  See [the configuration token parsing regexes in ContentBuilder.Tokenizer|https://github.com/jenkinsci/email-ext-plugin/blob/master/src/main/java/hudson/plugins/emailext/plugins/ContentBuilder.java#L134], in particular the comment over the {{stringRegex}} field:{code:java}// Sequence of (1) not \ "" CR LF and (2) \ followed by non line terminator
private static final String stringRegex = ""\""([^\\\\\""\\r\\n]|(\\\\.))*\"""";{code}
This could be fixed by allowing line terminators to be escaped:{code:java}// Sequence of (1) not \ "" CR LF and (2) \ followed by any non-CR/LF character or (CR)LF
private static final String stringRegex = ""\""([^\\\\\""\\r\\n]|(\\\\(?:.|\r?\n)))*\"""";{code}
I submitted a Jenkins JIRA issue for this: [https://issues.jenkins-ci.org/browse/JENKINS-13976].
# [BuildLogRegexContent, the content parser for BUILD_LOG_REGEX, matches line-by-line|https://github.com/jenkinsci/email-ext-plugin/blob/master/src/main/java/hudson/plugins/emailext/plugins/content/BuildLogRegexContent.java#L213], so regexes targeting multiple lines will fail.  I can see two possible routes to address this:
## The BUILD_LOG_EXCERPT token allows specification of begin/end line regexes, and includes everything inbetween matches.  I'm doubtful this will enable capture of the stuff we want, though.
## I'll try to add an argument to BUILD_LOG_REGEX to enable multi-line content matching, and make a pull request/JIRA issue to get it included in the next release of the plugin.

In the mean time, I'll switch the configuration in our Jenkins jobs to the following:

{noformat}
Build: ${BUILD_URL}

${FAILED_TESTS}

Build Log:
${BUILD_LOG_REGEX,regex=""[ \\t]*(?:\\[javac\\]\\s+[1-9]\\d*\\s+error|\\[junit4\\].*<<<\\s+FAILURES!|\\[javadoc\\]\\s+[1-9]\\d*\\s+warning).*"",linesBefore=100}
${BUILD_LOG_REGEX,regex=""[ \\t]*\\[echo\\].*)*\\s*[1-9]\\d*\\s+Unknown\\s+Licenses.*"",linesBefore=17,linesAfter=20}
${BUILD_LOG_REGEX,regex=""[ \\t]*javadocs-lint:.*"",linesBefore=0,linesAfter=75}
${BUILD_LOG_REGEX,regex="".*FATAL:.*"",linesBefore=0,linesAfter=100}
{noformat}","31/May/12 23:03;mikemccand;Steve you are a regexp God.","01/Jun/12 02:02;sarowe;bq. I'll switch the configuration in our Jenkins jobs to the following ... 

Done.","01/Jun/12 19:50;sarowe;This one had syntax problems (a recent test failure notification email complained about it):

{noformat}
${BUILD_LOG_REGEX,regex=""[ \\t]*\\[echo\\].*)*\\s*[1-9]\\d*\\s+Unknown\\s+Licenses.*"",linesBefore=17,linesAfter=20}
{noformat}

I switched it to the following on all non-Maven Jenkins job configuratinos:

{noformat}
${BUILD_LOG_REGEX,regex=""[ \\t]*\\[echo\\]\\s+[1-9]\\d*\\s+Unknown\\s+Licenses.*"",linesBefore=17,linesAfter=20}
{noformat}","01/Jun/12 19:54;sarowe;{quote}
Spreading the BUILD_LOG_REGEX regex value over multiple lines is not supported by Jenkins's email templating functionality 
[...]
This could be fixed by allowing line terminators to be escaped:
[...]
I submitted a Jenkins JIRA issue for this: [https://issues.jenkins-ci.org/browse/JENKINS-13976].
{quote}

I forked the email-ext project on github and made a pull request, which has now been incorporated into the upstream project.
","05/Jun/12 17:56;sarowe;{quote}
BuildLogRegexContent, the content parser for BUILD_LOG_REGEX, matches line-by-line, so regexes targeting multiple lines will fail. [...] I'll try to add an argument to BUILD_LOG_REGEX to enable multi-line content matching, and make a pull request/JIRA issue to get it included in the next release of the plugin.
{quote}

I made a new content token named BUILD_LOG_MULTILINE_REGEX in my fork of the email-ext-plugin - see [JENKINS-14000|https://issues.jenkins-ci.org/browse/JENKINS-14000].  My pull request ([#40|https://github.com/jenkinsci/email-ext-plugin/pull/40]) was merged into the upstream project a few days ago, so the next release will include this change.  (Looks like this plugin releases once a month or so, so the wait before we can use it shouldn't be too long, assuming the plugins on the ASF Jenkins instance are kept up-to-date.)
","07/Jun/12 12:47;dweiss;Assigning to you, Steven, since you do the heavy lifting here anyway. Thanks!","16/Jun/12 15:39;sarowe;v2.22 of the Jenkins Email-ext plugin was released today, incorporating the new BUILD_LOG_MULTILINE_REGEX content token functionality.  Olivier Lamy kindly upgraded the plugin in ASF Jenkins and restarted it.

I'm going to re-try using [the full-log multiline regex I posted in comment-13286757|https://issues.apache.org/jira/browse/LUCENE-4092?focusedCommentId=13286757&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13286757], with modification I've since added to Lucene jobs' configurations (using multiple BUILD_LOG_REGEX content tokens, one per context window a.k.a. linesBefore/linesAfter).  

Hopefully now we'll be able to get exactly the right snippet from the logs for each failure case.","16/Jun/12 16:58;sarowe;Here is the *Editable Email Notification*/*Default Content* configuration I applied to all non-Maven ASF Jenkins jobs (modulo whitespace in front of the trailing line-continuation backslashes):

*edit*: BUILD_LOG_REGEX -> BUILD_LOG_MULTILINE_REGEX

{noformat}
Build: ${BUILD_URL}

${FAILED_TESTS}

Build Log:
${BUILD_LOG_MULTILINE_REGEX,regex=""(?x:                                                                                      \
# Compilation failures                                                                                             \
(?:.*\\[javac\\].*\\r?\\n)*.*\\[javac\\]\\s+[1-9]\\d*\\s+error.*\\r?\\n                                            \
# Test failures                                                                                                    \
|.*\\[junit4\\]\\s*Suite:.*[\\r\\n]+.*\\[junit4\\]\\s*(?!Completed)(?!IGNOR)\\S(?s:.*?)<<<\\s*FAILURES!            \
# Source file license problems                                                                                     \
|.*rat-sources:.*(?:\\r?\\n.*\\[echo\\].*)*\\s+[1-9]\\d*\\s+Unknown\\s+Licenses.*\\r?\\n(?:.*\\[echo\\].*\\r?\\n)* \
# Third-party dependency license problems - include 2 preceding lines and 1 following line                         \
|(?:.*\\r?\\n){2}.*\\[licenses\\]\\s+MISSING\\s+sha1(?:.*\\r?\\n){2}                                               \
# Javadoc warnings                                                                                                 \
|(?:.*\\[javadoc\\].*\\r?\\n)*.*\\[javadoc\\]\\s*[1-9]\\d*\\s+warnings.*\\r?\\n                                    \
# Other javadocs problems: broken links and missing javadocs                                                       \
|.*javadocs-lint:.*\\r?\\n(?:.*\\[echo\\].*\\r?\\n)*                                                               \
# Thread dumps - include 1 preceding line and the remainder of the log                                             \
|.*\\r?\\n.*Full thread dump(?s:.*)                                                                                \
# Jenkins problems - include the remainder of the log                                                              \
|.*(?:FATAL|ERROR):(?s:.*)                                                                                         \
# Include the Ant call stack - include the remainder of the log                                                    \
|.*BUILD\\s+FAILED(?s:.*)                                                                                          \
)""}
{noformat}
","16/Jun/12 17:14;sarowe;bq. {{${BUILD_LOG_REGEX,regex=""(?x:}} [...]

Ack!  I forgot to switch the previous version to use the new {{BUILD_LOG_*MULTILINE*_REGEX}} content token!

Fixed now...","30/Jul/12 12:34;sarowe;A bug effectively prevented use of the new BUILD_LOG_MULTILINE_REGEX config: [JENKINS-14132|https://issues.jenkins-ci.org/browse/JENKINS-14132].  The fix on that issue has been incorporated in v2.24.1 of the Email-ext plugin, and Olivier Lamy has kindly installed this version on ASF Jenkins.  Uwe helped work out the kinks, and has been running the patched version for a while on his Jenkins instance - upgraded to the release version now.

I will close as fixed later today once it's clear the ASF configuration is functional.
","30/Jul/12 12:41;sarowe;Here's the current configuration regex (edited so that the trailing backslashes are lined up):

{noformat}
Build Log:
${BUILD_LOG_MULTILINE_REGEX,regex=""(?x:                                                                         \
                                                                                                                \
(?:.*\\[javac\\]\\s++(?![1-9]\\d*\\s+error).*\\r?\\n)*+.*\\[javac\\]\\s+[1-9]\\d*\\s+error.*\\r?\\n             \
                                                                                                                \
|.*\\[(?:junit4:)?junit4\\]\\s+Suite:.*+\\s++.*\\[(?:junit4:)?junit4\\]\\s++(?!Completed)(?!IGNOR).*\\r?\\n     \
 (?:.*\\[(?:junit4:)?junit4\\]\\s++(?!Completed\\s+.*<<<\\s*FAILURES!).*\\r?\\n)*+                              \
 .*\\[(?:junit4:)?junit4\\]\\s++Completed\\s+.*<<<\\s*FAILURES!\\r?\\n                                          \
                                                                                                                \
|.*rat-sources:.*\\r?\\n                                                                                        \
 (?:\\s*+\\[echo\\]\\s*\\r?\\n|\\s*+\\[echo\\]\\s++(?![1-9]\\d*\\s+Unknown\\s+License)\\S.*\\r?\\n)*+           \
 \\s*+\\[echo\\]\\s+[1-9]\\d*\\s+Unknown\\s+License.*\\r?\\n                                                    \
 (?:\\s*+\\[echo\\].*\\r?\\n)*+                                                                                 \
                                                                                                                \
|(?:.*\\r?\\n){2}.*\\[licenses\\]\\s+MISSING\\s+sha1(?:.*\\r?\\n){2}                                            \
                                                                                                                \
|.*check-licenses:.*\\r?\\n\\s*\\[echo\\].*\\r?\\n\\s*\\[licenses\\]\\s+MISSING\\s+LICENSE.*\\r?\\n             \
 (?:\\s*+\\[licenses\\].*\\r?\\n)++                                                                             \
                                                                                                                \
|(?:.*\\[javadoc\\]\\s++(?![1-9]\\d*\\s+warning).+\\r?\\n)*+.*\\[javadoc\\]\\s+[1-9]\\d*\\s+warning.*\\r?\\n    \
                                                                                                                \
|.*javadocs-lint:.*\\r?\\n(?:.*\\[exec\\].*\\r?\\n)*+                                                           \
                                                                                                                \
|.*check-forbidden-apis:.*\\r?\\n                                                                               \
 (?:\\s*+\\[forbidden-apis\\]\\s*\\r?\\n                                                                        \
  |\\s*+\\[forbidden-apis\\]\\s++(?!Scanned\\s+\\d+\\s+class\\s+file\\(s\\))\\S.*\\r?\\n)*+                     \
 \\s*+\\[forbidden-apis\\]\\s++Scanned\\s+\\d+\\s+class\\s+file\\(s\\).*[1-9]\\d*\\s+error\\(s\\)\\.\\r?\\n     \
                                                                                                                \
|.*\\r?\\n.*Full\\s+thread\\s+dump(?s:.*+)                                                                      \
                                                                                                                \
|.*(?:FATAL|ERROR):(?s:.*+)                                                                                     \
                                                                                                                \
|.*BUILD\\s+FAILED(?s:.*+)                                                                                      \
)""}
{noformat}","30/Jul/12 21:55;sarowe;ASF Jenkins build log excerpts in notification emails are looking good.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Some test statistics (mean, stddev).",LUCENE-4174,12596214,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Not A Problem,dweiss,dweiss,dweiss,28/Jun/12 06:55,29/Jun/12 08:53,30/Sep/19 08:38,28/Jun/12 07:30,,,,,,,,,,,,,,,general/test,,,0,,,,,,,,,,,,,,,,LUCENE-4181,,,,"28/Jun/12 07:28;dweiss;Selection_001.png;https://issues.apache.org/jira/secure/attachment/12533785/Selection_001.png",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,243791,,,Thu Jun 28 07:30:59 UTC 2012,New,,,,,,,"0|i04fxb:",23845,,,,,,,,,"28/Jun/12 07:18;dweiss;I calculated simple stats from a dozen test runs on my build machine (trunk at Jun 28, 2012).

h3. The slowest tests (mean time, stddev)

{noformat}
name                                       | mean    | stddev | min    | max
BasicDistributedZkTest                     | 168155  | 12697  | 136591 | 192507
FullSolrCloudTest                          | 95912   | 5867   | 76449  | 102628
OverseerTest                               | 95801   | 9025   | 82868  | 108840
RecoveryZkTest                             | 82924   | 13524  | 63800  | 115504
TestJoinUtil                               | 69328   | 74451  | 7358   | 268199
SpellCheckComponentTest                    | 65744   | 14222  | 38798  | 85575
FullSolrCloudDistribCmdsTest               | 49543   | 2837   | 45376  | 53916
TestReplicationHandler                     | 45352   | 1863   | 41645  | 48253
LeaderElectionIntegrationTest              | 42893   | 5400   | 33565  | 50718
TestLBHttpSolrServer                       | 35746   | 704    | 34841  | 37133
LeaderElectionTest                         | 33766   | 6039   | 23574  | 45419
DistributedSpellCheckComponentTest         | 30688   | 3500   | 23605  | 36914
TestRealTimeGet                            | 30642   | 3960   | 24503  | 39594
TestJapaneseTokenizer                      | 27920   | 5575   | 22090  | 48021
TestDuelingAnalyzers                       | 26972   | 4330   | 18522  | 33446
ZkControllerTest                           | 26740   | 8703   | 16446  | 45590
TestSynonymMapFilter                       | 24929   | 3784   | 19259  | 31461
NGramTokenizerTest                         | 22886   | 8533   | 13972  | 49923
TestIndexWriterWithThreads                 | 22195   | 7698   | 14150  | 46912
TestExtendedMode                           | 19029   | 5525   | 12693  | 38139
TestDistributedGrouping                    | 18384   | 3128   | 12726  | 24952
TestRandomFaceting                         | 17223   | 5578   | 7514   | 27142
SamplingAccumulatorTest                    | 16966   | 7418   | 8858   | 30789
CloudStateUpdateTest                       | 16611   | 2728   | 10203  | 21394
TestDistributedSearch                      | 15850   | 2450   | 12915  | 22035
TestPackedInts                             | 15469   | 3433   | 11274  | 25217
SamplingWrapperTest                        | 14782   | 6307   | 7890   | 28589
AdaptiveAccumulatorTest                    | 14672   | 4603   | 8838   | 26736
SpellCheckCollatorTest                     | 13402   | 3775   | 5068   | 18996
SolrExampleStreamingBinaryTest             | 12692   | 1110   | 11701  | 15060
TestJapaneseAnalyzer                       | 12538   | 3785   | 7372   | 24728
SolrExampleStreamingTest                   | 12441   | 882    | 11635  | 14698
UIMAUpdateRequestProcessorTest             | 12053   | 346    | 11135  | 12807
TestNorms                                  | 11652   | 8949   | 1861   | 35629
TestFSTs                                   | 11421   | 3263   | 6679   | 16884
SoftAutoCommitTest                         | 11157   | 794    | 9906   | 12756
TestExtendedDismaxParser                   | 11149   | 1469   | 8410   | 13435
AutoCommitTest                             | 10827   | 752    | 9228   | 11822
TestPhoneticFilterFactory                  | 10273   | 2269   | 8453   | 17129
BasicZkTest                                | 10001   | 1108   | 7167   | 11637
TestFaceting                               | 9927    | 3660   | 6748   | 19713
TestPerfTasksLogic                         | 9842    | 768    | 8519   | 11465
TestGraphTokenizers                        | 9379    | 2526   | 5502   | 13915
TestAnalyzers                              | 9378    | 4605   | 5109   | 22722
TestHashPartitioner                        | 9351    | 3664   | 6645   | 22686
ZkSolrClientTest                           | 9290    | 824    | 7845   | 10847
UIMATypeAwareAnnotationsTokenizerFactoryTe | 8883    | 240    | 8540   | 9393
UIMAAnnotationsTokenizerFactoryTest        | 8748    | 393    | 7883   | 9308
TestGrouping                               | 8656    | 1154   | 6909   | 11361
TestSmartChineseAnalyzer                   | 8594    | 2904   | 6694   | 18657
WordBreakSolrSpellCheckerTest              | 8501    | 2056   | 4204   | 12071
TestIndexWriterReader                      | 8303    | 3168   | 4899   | 14695
TestRecovery                               | 8247    | 3132   | 5854   | 17090
DistributedTermsComponentTest              | 8132    | 797    | 6736   | 9315
TestLookaheadTokenFilter                   | 7896    | 3085   | 3992   | 14377
StatsComponentTest                         | 7767    | 1015   | 6230   | 9364
TestJoin                                   | 7764    | 2381   | 5615   | 14003
{noformat}

h3. Largest test execution variance 

{noformat}
name                          | mean   | stddev | min   | max
TestJoinUtil                  | 69328  | 74451  | 7358  | 268199
SpellCheckComponentTest       | 65744  | 14222  | 38798 | 85575
RecoveryZkTest                | 82924  | 13524  | 63800 | 115504
BasicDistributedZkTest        | 168155 | 12697  | 136591| 192507
OverseerTest                  | 95801  | 9025   | 82868 | 108840
TestNorms                     | 11652  | 8949   | 1861  | 35629
ZkControllerTest              | 26740  | 8703   | 16446 | 45590
TestTypePromotion             | 6467   | 8554   | 761   | 26213
NGramTokenizerTest            | 22886  | 8533   | 13972 | 49923           
{noformat}","28/Jun/12 07:28;dweiss;Logarithmic scale test time distribution (by mean, desc).","28/Jun/12 07:30;dweiss;This is just informational. It'd be nice to have those slowest tests run a bit faster though. Or maybe move them to @Nightly since they're so long to execute?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Reqs page should be release specific,LUCENE-1154,12387151,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,gsingers,gsingers,25/Jan/08 13:35,24/May/12 14:42,30/Sep/19 08:38,16/Nov/09 08:32,,,,,,,,,,,3.0,,,,general/website,,,0,,,,"The System Requirements page, currently under the Main->Resources section of the website should be part of a given version's documentation, since it will be changing for a given release.  

I will ""deprecate"" the existing one, but leave it in place(with a message) to cover the existing releases that don't have this, but will also add it to the release docs for future releases.",,,,,,,,,,,,,,,,"15/Nov/09 17:38;uschindler;LUCENE-1154-site.patch;https://issues.apache.org/jira/secure/attachment/12425010/LUCENE-1154-site.patch","15/Nov/09 17:38;uschindler;LUCENE-1154-trunk.patch;https://issues.apache.org/jira/secure/attachment/12425009/LUCENE-1154-trunk.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-11-14 22:25:24.175,,,false,,,,,,,,,,,,,,,12590,,,Mon Nov 16 08:32:42 UTC 2009,New,,,,,,,"0|i04yqn:",26893,,,,,,,,,"29/Jan/08 13:37;gsingers;We can keep the existing one until 3.0 is released.","14/Nov/09 22:25;uschindler;I am not so familar with forrest, what is to do, to add a new page? Just create and link a new XML file?","14/Nov/09 23:18;simonw;see http://wiki.apache.org/jakarta-lucene/HowToUpdateTheWebsite

simon","14/Nov/09 23:24;uschindler;On this page is nothing new, I already know how to change the web sites. I think, I have to read the forrest docs to find out how to change the menu and add *new* pages.","15/Nov/09 17:38;uschindler;Here a patch for version specific sysreqs. One patch apllies to trunk, the other to the site.

Please tell me, if this is ok, I would commit it before packaging the RC.","16/Nov/09 08:32;uschindler;Committed trunk changes revision: 880660
Committed site changes revision: 880665",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ivy/maven config to pull from sonatype releases,LUCENE-3987,12550986,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Duplicate,dweiss,dweiss,dweiss,15/Apr/12 13:17,27/Apr/12 08:06,30/Sep/19 08:38,27/Apr/12 08:06,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,"18/Apr/12 15:51;dweiss;LUCENE-3987.patch;https://issues.apache.org/jira/secure/attachment/12523207/LUCENE-3987.patch","15/Apr/12 13:17;dweiss;ivy-sonatype.patch;https://issues.apache.org/jira/secure/attachment/12522699/ivy-sonatype.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-15 13:21:28.351,,,false,,,,,,,,,,,,,,,235850,,,Fri Apr 27 08:06:04 UTC 2012,New,,,,,,,"0|i04h3b:",24034,,,,,,,,,"15/Apr/12 13:17;dweiss;A patch that modifies ivy resolver chain and adds a repository to maven pom. Works for me.","15/Apr/12 13:21;rcmuir;How will this work in releases, where dev-tools doesnt exist (and where lucene/ is the root of its source package).
","15/Apr/12 13:29;dweiss;I don't want to merge this in (note no fix version). I just filed it for reference in case somebody needs it.","15/Apr/12 13:29;dweiss;This is for reference only.","15/Apr/12 13:31;rcmuir;Well i wasn't saying we couldnt commit it (though, i dont understand what it does)...
i was just questioning the file paths :)

","18/Apr/12 15:48;dweiss;After some deliberation I would like to add ivysettings.xml to test-framework module which would allow (this module) to fetch dependencies from an additional repository (sonatype releases). I will also add this to corresponding maven descriptor so these would be in sync.

Maintenance-wise this is not an issue -- sonatype is mirroring to central so effectively they're the same but there is no lag between releases and syncs.","18/Apr/12 15:51;dweiss;Patch with module-specific ivy settings and maven settings.","26/Apr/12 11:54;rcmuir;Is it possible to also add the chinese mirror in this way?
http://docs.codehaus.org/display/MAVENUSER/Mirrors+Repositories
","26/Apr/12 11:56;rcmuir;Or, is the commit here actually just breaking chinese users (redirectly directly)
and somehow losing maven's mirroring capabilities? 

Currently chinese users cant download lucene.","26/Apr/12 15:31;dweiss;This patch shouldn't make any difference. If they could download it before, they should be (even more) able to download it now.

What's the symptoms of not being able to download lucene? Can they access repo1.maven.org directly without errors?","26/Apr/12 15:38;rcmuir;You are right, this should only change test-framework?

I think this was the wrong issue to bring this up (sorry!)... and i only have hearsay of the problem
(have not been testing directly)...

lets mark it resolved... but I have concerns, in general are these maven repos accessible 
from China? Does anyone know?","27/Apr/12 08:06;dweiss;Fixed at the global level in LUCENE-3892",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove remaining @author references,LUCENE-1378,12403879,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,otis,otis,otis,08/Sep/08 04:49,06/Apr/12 22:20,30/Sep/19 08:38,02/Jun/09 15:16,,,,,,,,,,,2.9,,,,,,,0,,,,"$ find . -name \*.java | xargs grep '@author' | cut -d':' -f1 | xargs perl -pi -e 's/ \@author.*//'
",,,,,,,,,,,,,,,,"10/Sep/08 03:02;markrmiller@gmail.com;LUCENE-1378.patch;https://issues.apache.org/jira/secure/attachment/12389802/LUCENE-1378.patch","08/Sep/08 04:50;otis;LUCENE-1378.patch;https://issues.apache.org/jira/secure/attachment/12389654/LUCENE-1378.patch","17/Nov/08 20:28;paul.elschot@xs4all.nl;LUCENE-1378b.patch;https://issues.apache.org/jira/secure/attachment/12394097/LUCENE-1378b.patch","17/Nov/08 21:02;paul.elschot@xs4all.nl;LUCENE-1378c.patch;https://issues.apache.org/jira/secure/attachment/12394100/LUCENE-1378c.patch",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2008-09-08 06:58:12.467,,,false,,,,,,,,,,,,,,,12373,,,Tue Jun 02 16:51:01 UTC 2009,New,Patch Available,,,,,,"0|i04xdb:",26671,,,,,,,,,"08/Sep/08 06:58;paul.elschot@xs4all.nl;The patch of 20080907 has some commented code added in SweetSpotSimilarityTest, probably unwanted.
Also, author lines are replaced by emty comment lines, perhaps it's better remove these lines completely. I didn't see any place where that could go wrong by changing the perl substitute command to do so, and the compiler would find such possible comment errors anyway.
","09/Sep/08 09:35;mikemccand;Otis can you finish & commit this?","09/Sep/08 22:01;otis;Eh, rusty perl

$ find . -name \*.java | xargs grep '@author' | cut -d':' -f1 | xargs perl -pi -e 's/\* @author.*//'

Doesn't work -- that \* in front of @author doesn't cut it.","10/Sep/08 03:02;markrmiller@gmail.com;Here is a clean patch off trunk if you want to avoid the perl  (have a hard time remember my perl myself).

Ah...thought that patch was older, perl was no blocker <g>

Take the second anyway, it removes the 36 empty blocks (see below) created and was done with java <g>

 /**


  *
  */","15/Sep/08 15:44;otis;Thanks Mark.

Committed revision 695514.

","16/Nov/08 11:23;paul.elschot@xs4all.nl;Under *nix this command:

find src -name '*.html' | xargs grep Author

still shows some html author tags.
","17/Nov/08 20:28;paul.elschot@xs4all.nl;1378b.patch removes html author tags.","17/Nov/08 21:02;paul.elschot@xs4all.nl;Just try and imagine a life without grep. The 1378c.patch removes my name.","12/Dec/08 18:09;paul.elschot@xs4all.nl;Reopened, so fix 2.9 instead of 2.4.
Or should I rather open a new issue?","02/Jun/09 15:16;otis;Done.  Thank you Paul.

Sending        src/java/org/apache/lucene/analysis/package.html
Sending        src/java/org/apache/lucene/analysis/standard/package.html
Sending        src/java/org/apache/lucene/index/package.html
Sending        src/java/org/apache/lucene/queryParser/package.html
Sending        src/java/org/apache/lucene/search/package.html
Sending        src/java/org/apache/lucene/store/package.html
Sending        src/java/org/apache/lucene/util/package.html
Sending        src/test/org/apache/lucene/search/TestBooleanOr.java
Transmitting file data ........
Committed revision 781055.
","02/Jun/09 16:37;paul.elschot@xs4all.nl;I thought I had caught them all, but under unix in the trunk dir:

grep -r -i author src | grep -v '\.svn' | wc

still gives some lines mentioning authors in the src/site and src/jsp directories.
I don't know whether that's ok.

There is also some code in src/test using a query with an author: field, but that's no problem.","02/Jun/09 16:51;otis;I think a bunch of that xdocs stuff under site should/will really be removed with time, as some of it is out of date (e.g. benchmarks, contrib) and harder to maintain than Wiki pages.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Author tags from code,LUCENE-974,12375553,,Wish,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,gsingers,gsingers,gsingers,07/Aug/07 21:47,06/Apr/12 22:20,30/Sep/19 08:38,09/Aug/07 15:22,,,,,,,,,,,,,,,,,,0,,,,Remove all author tags from the code.,,,,,,,,,,,,,,,,"07/Aug/07 22:49;gsingers;LUCENE-974.patch;https://issues.apache.org/jira/secure/attachment/12363370/LUCENE-974.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12769,,,Thu Aug 09 15:22:11 UTC 2007,,,,,,,,"0|i04zun:",27073,,,,,,,,,"07/Aug/07 22:49;gsingers;Remove all @author tags","09/Aug/07 14:02;gsingers;I'm going to commit this today.","09/Aug/07 15:22;gsingers;Committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cloneable classes can use their class in the clone() function,LUCENE-3933,12548543,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Duplicate,ryantxu,ryantxu,ryantxu,28/Mar/12 20:20,28/Mar/12 21:15,30/Sep/19 08:38,28/Mar/12 21:15,,,,,,,,,,,,,,,,,,0,,,,"Since Java5, we are allowed to use an explicit class when returning clone()

It is nicer to use:
{code:java}
OpenBitSet copy = original.clone();
{code}
then
{code:java}
OpenBitSet copy = (OpenBitSet)original.clone();
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-03-28 20:27:37.517,,,false,,,,,,,,,,,,,,,233640,,,Wed Mar 28 20:27:37 UTC 2012,,,,,,,,"0|i04hfb:",24088,,,,,,,,,"28/Mar/12 20:27;rcmuir;This is a duplicate of LUCENE-2000",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken link in docs,LUCENE-3845,12545074,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,chengas123,chengas123,04/Mar/12 04:17,07/Mar/12 01:59,30/Sep/19 08:38,07/Mar/12 01:59,,,,,,,,,,,,,,,general/javadocs,general/website,,0,,,,"The link for ""Nightly Build Documentation"" is broken on http://lucene.apache.org/core/developer.html

(btw, any update on the status of a 4.0 alpha/beta release?)",,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-03-07 01:59:51.803,,,false,,,,,,,,,,,,,,,230260,,,Wed Mar 07 01:59:51 UTC 2012,New,,,,,,,"0|i04hy7:",24173,,,,,,,,,"07/Mar/12 01:59;hossman;thanks for pointing out the broken link .. no idea what that sub-section was suppose to be about since there is a ""nightly javadocs"" section right below it, so i just removed it",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FVH: uncontrollable color tags,LUCENE-3019,12503939,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,koji,koji,koji,11/Apr/11 14:39,24/Jan/12 14:19,30/Sep/19 08:38,15/Sep/11 01:00,2.9.4,3.0.3,3.1,4.0-ALPHA,,,,,,,3.5,4.0-ALPHA,,,modules/highlighter,,,0,,,,"The multi-colored tags is a feature of FVH. But it is uncontrollable (or more precisely, unexpected by users) that which color is used for each terms.",,,,,,,,,,,,,,,,"11/Apr/11 14:52;koji;LUCENE-3019.patch;https://issues.apache.org/jira/secure/attachment/12476005/LUCENE-3019.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-03 16:40:39.806,,,false,,,,,,,,,,,,,,,10878,,,Tue Jan 24 14:19:11 UTC 2012,New,,,,,,,"0|i04mzz:",24991,,,,,,,,,"11/Apr/11 14:52;koji;The patch. It fixes the problem when usePhraseHighlighter=true.

When the flag is false and FVH works on N-gram field, not a few terms may be created in tree, then it causes uncontrollable.

But I think the case of using usePhraseHighlighter=false with N-gram field is rare, the attached patch will be enough.","03/Jun/11 16:40;rcmuir;bulk move 3.2 -> 3.3","14/Sep/11 23:53;koji;I'll commit soon.","15/Sep/11 01:00;koji;trunk: Committed revision 1170908.
3x: Committed revision 1170913.","27/Nov/11 12:29;uschindler;Bulk close after release of 3.5","23/Jan/12 15:07;imotov;The change from HashSet to ArrayList for flatQueries resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0. Our queries sometime contain tens of thousands of terms. As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls.","23/Jan/12 22:25;koji;Uh, that is not good news. Please don't hesitate to open a ticket. Patches would be welcome as I don't have time to look into it for a few weeks...","24/Jan/12 14:19;imotov;Created LUCENE-3719 with a patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
optimizer for n-gram PhraseQuery,LUCENE-3426,12522647,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,koji,koji,koji,10/Sep/11 14:31,21/Jan/12 23:57,30/Sep/19 08:38,14/Sep/11 13:07,2.9.4,3.0.3,3.1,3.2,3.3,3.4,4.0-ALPHA,,,,3.5,4.0-ALPHA,,,core/search,,,0,,,,"If 2-gram is used and the length of query string is 4, for example q=""ABCD"", QueryParser generates (when autoGeneratePhraseQueries is true) PhraseQuery(""AB BC CD"") with slop 0. But it can be optimized PhraseQuery(""AB CD"") with appropriate positions.

The idea came from the Japanese paper ""N.M-gram: Implementation of Inverted Index Using N-gram with Hash Values"" by Mikio Hirabayashi, et al. (The main theme of the paper is different from the idea that I'm using here, though)",,,,,,,,,,,,,,,,"13/Sep/11 23:11;koji;LUCENE-3426.patch;https://issues.apache.org/jira/secure/attachment/12494342/LUCENE-3426.patch","13/Sep/11 16:38;koji;LUCENE-3426.patch;https://issues.apache.org/jira/secure/attachment/12494258/LUCENE-3426.patch","12/Sep/11 01:34;koji;LUCENE-3426.patch;https://issues.apache.org/jira/secure/attachment/12493980/LUCENE-3426.patch","12/Sep/11 00:47;koji;LUCENE-3426.patch;https://issues.apache.org/jira/secure/attachment/12493977/LUCENE-3426.patch","11/Sep/11 01:19;koji;LUCENE-3426.patch;https://issues.apache.org/jira/secure/attachment/12493934/LUCENE-3426.patch","10/Sep/11 14:38;koji;LUCENE-3426.patch;https://issues.apache.org/jira/secure/attachment/12493910/LUCENE-3426.patch","12/Sep/11 01:48;koji;PerfTest.java;https://issues.apache.org/jira/secure/attachment/12493981/PerfTest.java","11/Sep/11 02:47;koji;PerfTest.java;https://issues.apache.org/jira/secure/attachment/12493940/PerfTest.java",,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2011-09-11 12:58:43.567,,,false,,,,,,,,,,,,,,,2690,,,Sat Jan 21 23:57:13 UTC 2012,New,,,,,,,"0|i04kif:",24588,,,,,,,,,"10/Sep/11 14:38;koji;First draft. The patch doesn't include test yet.

I added optimizeForNgram() methods in the patch that will remove ""redundant"" terms and positions.

Optimizing PhraseQuery will change score.","11/Sep/11 01:19;koji;I added test code.","11/Sep/11 02:46;koji;The result of speed up is:

||n-gram/query length||normal(ms)||optimized(ms)||speed up||
|bi-gram/4|21,766|16,641|30.8%|
|bi-gram/6|29,865|21,518|38.8%|
|tri-gram/5|8,188|6,140|33.4%|
|tri-gram/6|9,001|5,925|51.9%|","11/Sep/11 12:58;rcmuir;Hi Koji, I wonder if instead it would be cleaner as a subclass of PhraseQuery (NGramPhraseQuery or similar),
that rewrites to the (possibly optimized) PhraseQuery in rewrite(). For example, it would build an optimized 
PhraseQuery when slop = 0, and there are enough terms to optimize, otherwise it would build a ""normal"" phrasequery.

Then the optimization would be easy to apply, the user just uses NGramPhraseQuery instead of PhraseQuery.
for example, from QueryParser:
{noformat}
  @Override
  protected PhraseQuery newPhraseQuery() {
    return new NGramPhraseQuery();
  }
{noformat}
","12/Sep/11 00:47;koji;I like the idea of introducing the newly created class! Here is the new patch.","12/Sep/11 01:02;rcmuir;I think I like it better too... though I wonder if its possible to keep the original NGramPhraseQuery unmodified?
this way its not changed by Query.rewrite(), and if a user reuses the query (which we document they can do), they could then call add() again and everything works.

Also, somewhat related to the issue might be SOLR-2660. We don't have to commit that patch, but we could separate
out the queryparser refactoring to make it easier for such an optimization to be ""automatic"" in solr, because it allows
SolrQueryParser to delegate creation of Phrase/MultiPhraseQuery to the FieldType.

","12/Sep/11 01:34;koji;{quote}
I think I like it better too... though I wonder if its possible to keep the original NGramPhraseQuery unmodified?
this way its not changed by Query.rewrite(), and if a user reuses the query (which we document they can do), they could then call add() again and everything works.
{quote}

I wonder it that too. Here is the new patch. This time I added assertSame()/NotSame() to check the rewritten Query to test code.","12/Sep/11 01:47;koji;For automatic in Solr, I wonder if we could move the feature to n-gram tokenizers, and we could have something like:

{code}
<fieldType name=""text_cjk"" class=""solr.TextField"" positionIncrementGap=""100""
           autoGeneratePhraseQueries=""true"">
  <analyzer type=""index"">
    <tokenizer class=""solr.CJKTokenizerFactory""/>
  </analyzer>
  <analyzer type=""query"">
    <tokenizer class=""solr.CJKTokenizerFactory"" optimizePhraseQuery=""true""/>
  </analyzer>
</fieldType>
{code}
","12/Sep/11 02:00;rcmuir;Well if we apply the refactoring part of SOLR-2660 (we can split out into a separate issue), we could add such a thing as an attribute to the fieldType?

I like the way your patch looks now! A couple more questions:
* doesn't the optimization also apply to MultiPhraseQuery? If so, NGramPhraseQuery could extend MultiPhraseQuery and just rewrite to the correct one (MultiPhrase or Phrase depending upon the situation after optimization)
* what about hashCode/equals? Although the same results will be returned, scoring will differ, maybe it NGramPhraseQuery should implement these?
","12/Sep/11 02:10;koji;I'm not sure it could apply MutiPhraseQuery. Let me take more time.

Considering hashCode/equals is good point. I'll see.
","13/Sep/11 16:38;koji;New patch. I added equals/hashCode in the patch.

I think it is too complex to apply optimization to MultiPhraseQuery, so I'd like to stick with PhraseQuery in the patch.","13/Sep/11 16:55;rcmuir;I think I agree Koji, the patch looks good.

Though we should be able to keep PhraseQuery's internal members 'private' since NGramPhraseQuery now uses getter methods to access positions, terms, slop ?","13/Sep/11 17:16;mikemccand;Patch looks great!  What a nice opto :)","13/Sep/11 23:11;koji;Thank you for your continuous review the patch, Robert! Here is the new patch. Now I don't touch the existing PhraseQuery as I use getter methods.

I think this is ready to commit.","14/Sep/11 13:07;koji;trunk: Committed revision 1170586.
3x: Committed revision 1170593.","27/Nov/11 12:29;uschindler;Bulk close after release of 3.5","21/Jan/12 23:03;billnbell;Is this automatic in SOLR? Or do we need to add a feature to support his in SOLR?","21/Jan/12 23:57;koji;bq. Is this automatic in SOLR?

No. I've opened SOLR-3055.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"TestFSTs.testRandomWords throws AIOBE when ""verbose""=true",LUCENE-3327,12514459,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,jdyer,jdyer,18/Jul/11 15:41,27/Nov/11 12:31,30/Sep/19 08:38,20/Jul/11 23:09,4.0-ALPHA,,,,,,,,,,3.4,4.0-ALPHA,,,core/FSTs,,,0,,,,"Seems like invalid utf-8 sometimes gets passed to Bytesref.utf8ToString() in the verbose ""println""s.",,,,,,,,,,,,,,,,"20/Jul/11 22:52;jdyer;LUCENE-3327.patch;https://issues.apache.org/jira/secure/attachment/12487230/LUCENE-3327.patch","18/Jul/11 15:43;jdyer;LUCENE-3327.patch;https://issues.apache.org/jira/secure/attachment/12486882/LUCENE-3327.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-18 17:06:56.78,,,false,,,,,,,,,,,,,,,2751,,,Wed Jul 20 23:09:41 UTC 2011,New,Patch Available,,,,,,"0|i04l4f:",24687,,,,,,,,,"18/Jul/11 15:43;jdyer;this just calls UnicodeUtil.newString(..) instead of BytesRef.utf8ToString() in all cases.","18/Jul/11 17:06;mikemccand;Hmm, I don't think this is quite right: in the BYTE1 case, these are the bytes from the term, and we shouldn't pretend they are unicode code points (which is what UnicodeUtil.newString is given).

Ie, we really do need the inputMode to be passed to inputToString.

Really, this test pretends a term is always a utf8 byte sequence, which in general is not the case (terms are arbitrary byte[]), it's just that this test only ever operates on terms that are in fact utf8 byte sequences (I think?).

Indeed I'm also hitting AIOOBE (ant test-core -Dtestcase=TestFSTs -Dtestmethod=testRandomWords -Dtests.seed=-3451527662631579719:-3355372777860187201):

{noformat}
There was 1 failure:
1) testRandomWords(org.apache.lucene.util.fst.TestFSTs)
java.lang.ArrayIndexOutOfBoundsException: 44
	at org.apache.lucene.util.UnicodeUtil.UTF8toUTF16(UnicodeUtil.java:586)
	at org.apache.lucene.util.BytesRef.utf8ToString(BytesRef.java:203)
	at org.apache.lucene.util.fst.TestFSTs.inputToString(TestFSTs.java:989)
	at org.apache.lucene.util.fst.TestFSTs.access$000(TestFSTs.java:53)
	at org.apache.lucene.util.fst.TestFSTs$FSTTester.verifyPruned(TestFSTs.java:833)
	at org.apache.lucene.util.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:507)
	at org.apache.lucene.util.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:366)
	at org.apache.lucene.util.fst.TestFSTs.doTest(TestFSTs.java:214)
	at org.apache.lucene.util.fst.TestFSTs.testRandomWords(TestFSTs.java:963)
	at org.apache.lucene.util.fst.TestFSTs.testRandomWords(TestFSTs.java:938)
{noformat}

Spooky because this test supposedly creates random valid unicode strings (_TestUtil.randomRealisticUnicodeString)... hmmm.","20/Jul/11 20:02;jdyer;{quote}
Spooky because this test supposedly creates random valid unicode strings (_TestUtil.randomRealisticUnicodeString)... hmmm.
{quote}

but then it breaks them down into prefixes and those aren't always valid utf-8...","20/Jul/11 21:04;mikemccand;Ahhh that's what makes the invalid UTF8.  OK.  Can we just change that one place (that cuts a potentially invalid UTF8 prefix) to just use BytesRef.toString?","20/Jul/11 22:52;jdyer;Wherever the test is printing out a term _prefix_, it just calls IntsRef.toString() rather than try to convert this to something more human-readable.","20/Jul/11 23:07;mikemccand;Looks great James, thanks!  I confirmed this fixes the above exc when I run w/ verbose.  I'll commit shortly -- I just changed the new arg's name to ""isValidUnicode"", and fixed up the whitespace.","20/Jul/11 23:09;mikemccand;Thanks James!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leftover legacy enum in IndexReader,LUCENE-3302,12513452,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,uschindler,uschindler,10/Jul/11 08:57,27/Nov/11 12:31,30/Sep/19 08:38,10/Jul/11 09:11,3.0,3.1,3.2,3.3,,,,,,,3.4,4.0-ALPHA,,,core/index,,,0,,,,"In IndexReader we still have some leftover ""handmade"" enum from pre-Java5 times. Unfortunately the Generics/Java5 Policeman did not notice it.

This patch is just code cleanup, no baclkwards breaks, as code using this enum would not see any difference (because only superclass changes).

I will commit this asap.",,300,300,,0%,300,300,,,,,,,,,"10/Jul/11 08:58;uschindler;LUCENE-3302.patch;https://issues.apache.org/jira/secure/attachment/12485990/LUCENE-3302.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,2763,,,Sun Jul 10 09:11:26 UTC 2011,New,Patch Available,,,,,,"0|i04l9z:",24712,,,,,,,,,"10/Jul/11 09:11;uschindler;Committed trunk revision: 1144792
Committed 3.x revision: 1144793",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable passing a config into PKIndexSplitter,LUCENE-3296,12513319,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,simonw,jasonrutherglen,jasonrutherglen,09/Jul/11 02:33,27/Nov/11 12:31,30/Sep/19 08:38,12/Jul/11 08:17,3.3,4.0-ALPHA,,,,,,,,,3.4,4.0-ALPHA,,,core/other,,,0,,,,I need to be able to pass the IndexWriterConfig into the IW used by PKIndexSplitter.,,,,,,,,,,HBASE-3529,,,,,,"11/Jul/11 10:23;simonw;LUCENE-3296.patch;https://issues.apache.org/jira/secure/attachment/12486040/LUCENE-3296.patch","11/Jul/11 09:52;simonw;LUCENE-3296.patch;https://issues.apache.org/jira/secure/attachment/12486034/LUCENE-3296.patch","10/Jul/11 22:21;jasonrutherglen;LUCENE-3296.patch;https://issues.apache.org/jira/secure/attachment/12486011/LUCENE-3296.patch","09/Jul/11 02:57;jasonrutherglen;LUCENE-3296.patch;https://issues.apache.org/jira/secure/attachment/12485805/LUCENE-3296.patch",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-07-09 06:47:13.565,,,false,,,,,,,,,,,,,,,2765,,,Tue Jul 12 08:17:44 UTC 2011,New,Patch Available,,,,,,"0|i04lbb:",24718,,,,,,,,,"09/Jul/11 02:57;jasonrutherglen;Patch, all tests pass.","09/Jul/11 06:47;simonw;looks good.. do you have to use Version.LUCENE_CURRENT in the ctors or can you pass in a version. using Version.LUCENE_CURRENT is discouraged for several reasons.","09/Jul/11 09:06;jasonrutherglen;That was in there previously.  Lets change it.","10/Jul/11 22:21;jasonrutherglen;This patch uses LUCENE_40.  All tests pass.","10/Jul/11 23:11;uschindler;Simon: The Version.LUCENE_CURRENT is not important here, for easier porting, the version should be LUCENE_CURRENT (and it was before Jason's patch). Else we will have to always upgrade it with every new release. The same applies to the IndexUpdater class in core, it also uses LUCENE_CURRENT when you not pass in anything (as the version is completely useless for simple merge operations - like here).

I would take this issue, but with LUCENE_CURRENT.","10/Jul/11 23:11;uschindler;btw.: The LUCENE_CURRENT was inserted by myself in the orginal code :-)","11/Jul/11 06:14;jasonrutherglen;Uwe, the first patch [1] is implemented with CURRENT.

1. https://issues.apache.org/jira/secure/attachment/12485805/LUCENE-3296.patch","11/Jul/11 09:52;simonw;here is a new patch. I added a second IWC since we can not reuse IWC instances across IW due to SetOnce restrictions. I also moved out the VERSION_CURRENT and made it a ctor argument. We should not randomly use the VERSION_CURRENT but rather be consistent when we use version.

bq. Simon: The Version.LUCENE_CURRENT is not important here, for easier porting, the version should be LUCENE_CURRENT (and it was before Jason's patch). Else we will have to always upgrade it with every new release. The same applies to the IndexUpdater class in core, it also uses LUCENE_CURRENT when you not pass in anything (as the version is completely useless for simple merge operations - like here).

not entirely true, we use the index splitter in 3.x and if you upgrade from 3.1 to 3.2 you get a new mergepolicy by default which doesn't merge in order. I think its a problem that this version is not in 3.x yet so let fix it properly and backport.

Simon","11/Jul/11 10:10;uschindler;bq. not entirely true, we use the index splitter in 3.x and if you upgrade from 3.1 to 3.2 you get a new mergepolicy by default which doesn't merge in order. I think its a problem that this version is not in 3.x yet so let fix it properly and backport.

PKIndexSplitter is new in 3.3, so you would never used it with older versions...","11/Jul/11 10:14;uschindler;bq. We should not randomly use the VERSION_CURRENT but rather be consistent when we use version.

I agree, but when you backport to 3.4, you have to keep backwards compatibility, so only deprecate the ctor there.

IndexUpgrader only uses LUCENE_CURRENT when you invoke from command line, in all other cases its required arg, so we are consistent here.

We should also look at the other IndexSplitters in this package!","11/Jul/11 10:23;simonw;this patch includes added version to MultipassIndexSplitter ctor.

I am going to commit this and backport to 3.x","12/Jul/11 08:17;simonw;Committed to trunk and backported to 3.x. I marked parts in 3.x as deprecated. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Detect the test thread by reference, not by name.",LUCENE-3437,12523184,12522682,Sub-task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,15/Sep/11 12:40,27/Nov/11 12:29,30/Sep/19 08:38,15/Sep/11 12:53,,,,,,,,,,,3.5,4.0-ALPHA,,,,,,0,,,,"Get rid of this:
{code}
      if (doFail && (Thread.currentThread().getName().equals(""main"") 
          || Thread.currentThread().getName().equals(""Main Thread""))) {
{code}",,,,,,,,,,,,,,,,"15/Sep/11 12:42;dweiss;LUCENE-3437.patch;https://issues.apache.org/jira/secure/attachment/12494608/LUCENE-3437.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-15 13:06:37.907,,,false,,,,,,,,,,,,,,,2684,,,Sun Nov 27 12:29:34 UTC 2011,New,,,,,,,"0|i04kfz:",24577,,,,,,,,,"15/Sep/11 12:42;dweiss;I intend to commit shortly if there are no objections and tests pass locally.","15/Sep/11 13:06;rcmuir;nice cleanup!","27/Nov/11 12:29;uschindler;Bulk close after release of 3.5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaxonomyReader/Writer and their Lucene* implementation,LUCENE-3552,12529818,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,shaie,shaie,shaie,02/Nov/11 07:17,27/Nov/11 12:29,30/Sep/19 08:38,03/Nov/11 06:29,,,,,,,,,,,3.5,4.0-ALPHA,,,modules/facet,,,0,,,,"The facet module contains two interfaces TaxonomyWriter and TaxonomyReader, with two implementations Lucene*. We've never actually implemented two TaxonomyWriters/Readers, so I'm not sure if these interfaces are useful anymore. Therefore I'd like to propose that we do either of the following:

# Remove the interfaces and remove the Lucene part from the implementation classes (to end up with TW/TR impls). Or,
# Keep the interfaces, but rename the Lucene* impls to Directory*.

Whatever we do, I'd like to make the impls/interfaces impl also TwoPhaseCommit.

Any preferences?",,,,,,,,,,,,,,,,"02/Nov/11 14:24;shaie;LUCENE-3552.patch;https://issues.apache.org/jira/secure/attachment/12501964/LUCENE-3552.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-02 09:34:04.979,,,false,,,,,,,,,,,,,,,215677,,,Sun Nov 27 12:29:34 UTC 2011,New,Patch Available,,,,,,"0|i04jqn:",24463,,,,,,,,,"02/Nov/11 09:34;dafna;Hi Shai,
I would vote for (2). We may want to still maintain the option to implement both in one Lucene index, in order to avoid the burden of synch-ing them. 
Regards, Dafna.","02/Nov/11 09:47;shaie;What do you mean? If we'll combine the search and taxonomy index, I don't believe we'll call it TaxonomyWriter. The purpose of the interface is for allowing a different implementation of the taxonomy, i.e. something that is not based on a Lucene Directory (e.g. a DB or something).

Perhaps I misunderstand you?","02/Nov/11 12:18;dafna;Hi Shai,
Yes, the old (already forgotten) DB implementation of taxonomy index, and the subsequent Lucene implementation gave rise to the definition of TaxonomyReader/Writer interface. Currently, indeed, and we are left with Lucene (Directory) implementation only. But I thought that as the interface is already there, and we have some thoughts of yet another implementation of taxonomy index (and hence TaxonomyReader/Writer), as part of the Lucene search index, which may help to ease the burden of synch between these two indices, why not let the interface live a bit longer before we remove it.   Regards, Dafna.","02/Nov/11 12:57;shaie;Ok, let's keep the interface. It's harmful and I can certainly think of alternate taxonomy implementations. Perhaps one day someone would want to implement the taxonomy over a serialized FST or some other data structure ...

So in this case, I'll:
* Keep the interfaces
* Make TaxomomyWriter extend TwoPhaseCommit
* Rename Lucene TaxoWriter/Reader to DirectoryTaxoWriterReader.

Any suggestions about an alternative names to the implementations?","02/Nov/11 14:24;shaie;Patch renames LTW/R to DirectoryTW/TR. Also, I renamed LTW's openLuceneIndex/closeLuceneIndex to open/closeIndexWriter.

I've also made TW extend TwoPhaseCommit.

I think that it's ready to commit. I'll port the changes to trunk afterwards. I'll wait until tomorrow before I commit (the changes are trivial).","02/Nov/11 14:49;doronc;Shai Patch looks good!

Applies cleanly (after svn mv'ing the two renamed classes) and all facet tests run.

Only userguide.html still mentions the old classes:

{code}
TaxonomyWriter taxo = new LuceneTaxonomyWriter(taxoDir, OpenMode.CREATE);
..
TaxonomyReader taxo = new LuceneTaxonomyReader(taxoDir);
{code}

Other than that good to go.","03/Nov/11 06:29;shaie;Thanks Doron, good catch !

I also renamed o.a.l.facet.taxonomy.lucene to *.directory.

Committed revision 1196963 (trunk).
Committed revision 1196965 (3x).","27/Nov/11 12:29;uschindler;Bulk close after release of 3.5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NumericUtils.floatToSortableInt/doubleToSortableLong does not sort certain NaN ranges correctly and NumericRangeQuery produces wrong results for NaNs with half-open ranges,LUCENE-3582,12531994,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,dweiss,dweiss,18/Nov/11 20:49,27/Nov/11 12:29,30/Sep/19 08:38,19/Nov/11 12:08,,,,,,,,,,,3.5,4.0-ALPHA,,,,,,0,,,,"The current implementation of floatToSortableInt does not account for different NaN ranges which may result in NaNs sorted before -Infinity and after +Infinity. The default Java ordering is: all NaNs after Infinity.

A possible fix is to make all NaNs canonic ""quiet NaN"" as in:
{code}
// Canonicalize NaN ranges. I assume this check will be faster here than 
// (v == v) == false on the FPU? We don't distinguish between different
// flavors of NaNs here (see http://en.wikipedia.org/wiki/NaN). I guess
// in Java this doesn't matter much anyway.
if ((v & 0x7fffffff) > 0x7f800000) {
  // Apply the logic below to a canonical ""quiet NaN""
  return 0x7fc00000 ^ 0x80000000;
}
{code}

I don't commit because I don't know how much of the existing stuff relies on this (nobody should be keeping different NaNs  in their indexes, but who knows...).",,,,,,,,,,,,,,,,"19/Nov/11 11:14;uschindler;LUCENE-3582.patch;https://issues.apache.org/jira/secure/attachment/12504353/LUCENE-3582.patch","19/Nov/11 00:39;uschindler;LUCENE-3582.patch;https://issues.apache.org/jira/secure/attachment/12504308/LUCENE-3582.patch","18/Nov/11 20:58;dweiss;LUCENE-3582.patch;https://issues.apache.org/jira/secure/attachment/12504266/LUCENE-3582.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-11-18 21:25:06.485,,,false,,,,,,,,,,,,,,,217730,,,Sun Nov 27 12:29:32 UTC 2011,New,,,,,,,"0|i04jjz:",24433,,,,,,,,,"18/Nov/11 21:25;uschindler;Why not simply use floatToIntBits without the raw? This one normalized NaN.
We used the raw methods for speed reasons as we assumed that NaN sorting makes no sense and NumericRangeQuery cannot usefully select those values. E.g. half open ranges will select NaN values incorrectly, so an index containing NaN values is not useable with NRQ where the upper bound is null.

The same applies to Doubles, you patch is missing them.","18/Nov/11 21:52;dweiss;bq. Why not simply use floatToIntBits without the raw?

There is no specific reason -- I wrote a similar routine using floatToIntBits, actually, but Robert pointed out that using raw bits may be faster and that NumericUtils already has a method for doing float sorting using fixed precision arithmetic. I checked the source (it's not suitable for my needs because I need unsigned order) but noticed the code is incorrect. That's pretty much it ;)

I admit I thought floatToIntBits normalizes the representation (mantissa) but it doesn't -- in fact, the implementation in OpenJDK is indeed identical to what I suggested.

Feel free to commit and correct for doubles.","18/Nov/11 22:10;uschindler;So you agree if we exchange with flotToIntBits the bug is fixed. I would prefer to use the native JDK implementation, maybe it will get replaced by an intrisic :-)

For NumericRangeQuery any change in this method has no real effect, as its broken/not working for NaN at all. To fix this, NumericRangeQurey.newFloatRange(.., f, null,...) should be explicitely use Float.POSITIVE_INFINITY instead of null, otherwise some NaNs could be selected, too (with your normalization all NaNs would be inside that range).

What should we do here?
","18/Nov/11 22:31;dweiss;bq. So you agree if we exchange with flotToIntBits the bug is fixed.

Yes, as far as I can see the implementation of floatToIntBits is exactly floatToRawIntBits + normalization of NaN. I doubt there'll be intrinsics for that -- the code is short and simple enough that it will probably inline and jit into a few assembly instructions anyway.

I don't quite understand the other part of your question... the code in my patch is virtually the same as floatToIntBits:


        int result = floatToRawIntBits(value);
        // Check for NaN based on values of bit fields, maximum
        // exponent and nonzero significand.
        if ( ((result & FloatConsts.EXP_BIT_MASK) ==
              FloatConsts.EXP_BIT_MASK) &&
             (result & FloatConsts.SIGNIF_BIT_MASK) != 0)
            result = 0x7fc00000;
        return result;

The only difference is that this doesn't normalize ""significant"" NaNs (failing), but I don't know if they're even used in jvm code anywhere.","18/Nov/11 22:43;yseeley@gmail.com;We don't really support NaN as a value in Lucene in general I think.  I know that our sorting (priority queue) methods don't support NaN, and this is why FunctionQuery has the following code:

{code}
      // Current Lucene priority queues can't handle NaN and -Infinity, so
      // map to -Float.MAX_VALUE. This conditional handles both -infinity
      // and NaN since comparisons with NaN are always false.
      return score>Float.NEGATIVE_INFINITY ? score : -Float.MAX_VALUE;
{code}","18/Nov/11 22:47;uschindler;I have no preference to floatToIntBits or floatToRawIntBits. I just copied the code from Yoniks method from Solr, my original NumericRangeQuery code donation back in the past used floatToIntBits. I just said, the behaviour of NaN in NumericRangeQuery is undefined so there was no reason to support NaN with NRQ at all. So I dont care. It does affect NRQ, but to fix NRQ correctly, half open ranges must be modified to end at Positive_infnity, but then NRQ can never match NaN.

In my opinion, NumericUtils is made for NumericRangeQuery and the raw method is an intrinsic, we should use it. I would simply not like to fix this.

If we fix it, i have to add some checks in NRQ's ctor, too. So it supports NaN.","18/Nov/11 22:49;dweiss;Ok, don't fix it then, no problem from me -- like I said, I only found out because I looked inside. It's not a bug, it's a feature ;)

I personally think utility methods should work correctly for all kinds of input -- that utility method in NumericUtils should at least say it doesn't support NaN (or better: assert so).","18/Nov/11 23:00;uschindler;I agree that normalizing Nan would be goof for NRQ, because this way you can search for NaN using:
NumericRangeQuery.newFloatRange(...., Float.NaN, Float.NaN, true, true);

Otherwise the bits produced for the bounds may not be the same bits like indexed and thats the main problem. This  would fix this issue. Another thing to maybe fix would be the half-open ranges to correctly handle infinity. In that case a NRQ would never hit NaN (even when half open) but with the above query you could still search for NaN (as a ""point value""). ","19/Nov/11 00:39;uschindler;Here a patch that fixes NumericRangeQuery to correctly handle NaN. If the upper/lower bounds == null, it will replace that by infinity and will never match NaN (this was a bug before). If you want to hit NaN with NRQ, you can do that only by directly hitting it using NumericRangeQuery.newFloatRange(""float"", Float.NaN, Float.NaN, true, true)

This patch also handles doubles in addition to floats and uses the native Java method without raw. Tests were modified to check for NaN, too.

The use of floatToIntBits instead of floatToRawIntBits has no real performance impact, as this method is only used during indexing. Population of FieldCache is unaffected. It just ensures that indexes are built with normalized NaN values, so NRQ can work correctly.

Stored fields were already stored using the non-raw method, so this is now consistent.","19/Nov/11 08:17;dweiss;Looks good to me. Thanks Uwe.","19/Nov/11 10:54;uschindler;Improved tests for NRQ. Ready to commit.","19/Nov/11 11:20;uschindler;bq. I assume this check will be faster here than (v != v)

This is the in my opinion the funniest thing you can show your students about floats. Everybody will tell you this can never be true :-)

Indeed OpenJDKs isNaN() is implemented exactly like that, it returns (v != v).","19/Nov/11 12:05;dweiss;Isn't that a shocking experience? :)
","19/Nov/11 12:08;uschindler;Committed trunk revision: 1203966
Committed 3.x revision: 1203967","27/Nov/11 12:29;uschindler;Bulk close after release of 3.5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove DocumentBuilder interface from facet module,LUCENE-3549,12529664,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,shaie,shaie,shaie,01/Nov/11 14:23,27/Nov/11 12:29,30/Sep/19 08:38,02/Nov/11 07:07,,,,,,,,,,,3.5,4.0-ALPHA,,,modules/facet,,,0,,,,"The facet module contains an interface called DocumentBuilder, which contains a single method, build(Document) (it's a builder API). We use it in my company to standardize how different modules populate a Document object. I've included it with the facet contribution so that things will compile with as few code changes as possible.

Now it's time to do some cleanup and I'd like to start with this interface. If people think that this interface is useful to reside in 'core', then I don't mind moving it there. But otherwise, let's remove it from the code. It has only one impl in the facet module: CategoryDocumentBuilder, and we can certainly do without the interface.

More so, it's under o.a.l package which is inappropriate IMO. If it's moved to 'core', it should be under o.a.l.document.

If people see any problem with that, please speak up. I will do the changes and post a patch here shortly.",,,,,,,,,,,,,,,,"01/Nov/11 15:03;shaie;LUCENE-3549.patch;https://issues.apache.org/jira/secure/attachment/12501779/LUCENE-3549.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-01 14:29:55.981,,,false,,,,,,,,,,,,,,,215523,,,Sun Nov 27 12:29:25 UTC 2011,New,Patch Available,,,,,,"0|i04jrb:",24466,,,,,,,,,"01/Nov/11 14:29;mikemccand;I think we should just remove it?

This is really an app-level thing, and I think overkill for Lucene's one usage of it.","01/Nov/11 14:35;uschindler;+1 to remove. Alltogether it looks too much XML DOM tree like :-) [it's only missing DocumentBuilderFactory]","01/Nov/11 15:03;shaie;Patch against 3x (but easy to apply on trunk as well).

I will commit this soon.","02/Nov/11 07:07;shaie;Committed revision 1196471 (3x).
Committed revision 1196474 (trunk).","27/Nov/11 12:29;uschindler;Bulk close after release of 3.5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make DirectoryTaxonomyWriter's indexWriter member private,LUCENE-3556,12530002,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,shaie,shaie,shaie,03/Nov/11 06:49,27/Nov/11 12:29,30/Sep/19 08:38,03/Nov/11 07:42,,,,,,,,,,,3.5,4.0-ALPHA,,,modules/facet,,,0,,,,"DirectoryTaxonomyWriter has a protected indexWriter member. As far as I can tell, for two reasons:

# protected openIndexWriter method which lets you open your own IW (e.g. with a custom IndexWriterConfig).
# protected closeIndexWriter which is a hook for letting you close the IW you opened in the previous one.

The fixes are trivial IMO:
# Modify the method to return IW, and have the calling code set DTW's indexWriter member
# Eliminate closeIW. DTW already has a protected closeResources() which lets you clean other resources you've allocated, so I think that's enough.

I'll post a patch shortly.",,,,,,,,,,,,,,,,"03/Nov/11 07:02;shaie;LUCENE-3556.patch;https://issues.apache.org/jira/secure/attachment/12502110/LUCENE-3556.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-27 12:29:24.264,,,false,,,,,,,,,,,,,,,215861,,,Sun Nov 27 12:29:24 UTC 2011,New,Patch Available,,,,,,"0|i04jpr:",24459,,,,,,,,,"03/Nov/11 07:02;shaie;Trivial patch against trunk. I'd like to commit this shortly.","03/Nov/11 07:42;shaie;Committed revision 1196982 (trunk).
Committed revision 1196983 (3x).","27/Nov/11 12:29;uschindler;Bulk close after release of 3.5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add DataInput/DataOutput subclasses that delegate to an InputStream/OutputStream.,LUCENE-3202,12510293,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,dweiss,dweiss,dweiss,14/Jun/11 10:09,02/Jul/11 02:40,30/Sep/19 08:38,14/Jun/11 12:12,,,,,,,,,,,3.3,4.0-ALPHA,,,core/other,,,0,,,,Such classes would be handy for FST serialization/deserialization.,,,,,,,,,,,,,,,,"14/Jun/11 12:12;dweiss;LUCENE-3202.patch;https://issues.apache.org/jira/secure/attachment/12482548/LUCENE-3202.patch","14/Jun/11 10:24;dweiss;LUCENE-3202.patch;https://issues.apache.org/jira/secure/attachment/12482540/LUCENE-3202.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-14 11:47:56.316,,,false,,,,,,,,,,,,,,,2092,,,Sat Jul 02 02:40:13 UTC 2011,New,,,,,,,"0|i04lwf:",24813,,,,,,,,,"14/Jun/11 10:24;dweiss;A patch moving these stream delegation classes to org.apache.lucene.store. A potential bugfix is piggybacked (potential partial read(byte[]) was not handled correctly).","14/Jun/11 11:47;shaie;Patch looks good. Two comments:
# The files are missing the Apache License header.
# In some places you use this.is / this.out and others just is/out. Can you consolidate on one (I prefer w/o this.)?","14/Jun/11 11:52;rcmuir;I agree with moving these to .store package, sorry I forgot about this in the suggest refactoring.

I've had to write similar classes myself before since they were not there.
","14/Jun/11 12:04;dweiss;Thanks Shai. I'll add the headers, clean up this where applicable and commit in.","14/Jun/11 12:12;dweiss;Updated patch, applied.","02/Jul/11 02:40;rcmuir;bulk close for 3.3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Easier way to run benchmark,LUCENE-2963,12501037,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,doronc,doronc,doronc,10/Mar/11 17:02,03/Jun/11 16:37,30/Sep/19 08:38,20/Mar/11 20:25,,,,,,,,,,,3.2,4.0-ALPHA,,,modules/benchmark,,,0,,,,"Move Benchmark.main to a more easily accessible method exec() that can be easily invoked by external programs.
",,,,,,,,,,,,,,,,"10/Mar/11 17:04;doronc;LUCENE-2963.patch;https://issues.apache.org/jira/secure/attachment/12473289/LUCENE-2963.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-10 18:09:26.839,,,false,,,,,,,,,,,,,,,10915,,,Fri Jun 03 16:37:19 UTC 2011,New,,,,,,,"0|i04ncf:",25047,,,,,,,,,"10/Mar/11 17:04;doronc;Trivial 3x patch","10/Mar/11 18:09;shaie;Looks perfect !

+1 to commit.","20/Mar/11 20:25;doronc;Committed:
r1083534 - 3x
r1083557 - trunk","03/Jun/11 16:37;rcmuir;Bulk closing for 3.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""ant eclipse"" should create an Eclipse project",LUCENE-3128,12507909,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,shaie,dserodio,dserodio,20/May/11 18:20,03/Jun/11 16:37,30/Sep/19 08:38,24/May/11 06:05,3.1,,,,,,,,,,3.2,4.0-ALPHA,,,general/build,,,0,newbie,,,"The ""eclipse"" Ant target creates a .classpath file, but not a .project file, so the user has to create an Eclipse project in a separate step. Creating a .project file (if it doesn't exist yet) would make it easier for Eclipse users to build Lucene.",,600,600,,0%,600,600,,,,,,,,,"20/May/11 18:21;dserodio;LUCENE-3128.patch;https://issues.apache.org/jira/secure/attachment/12479936/LUCENE-3128.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-23 03:31:36.136,,,false,,,,,,,,,,,,,,,2129,,,Fri Jun 03 16:37:15 UTC 2011,New,Patch Available,,,,,,"0|i04mcf:",24885,,,,,,,,,"23/May/11 03:31;shaie;Patch looks good. I see that you copy .project w/ overwrite=false, which is good.

If there are no objections, I will commit it tomorrow.","24/May/11 06:05;shaie;Committed revision 1126883 (3x).
Committed revision 1126884 (trunk).

Thanks Daniel !","03/Jun/11 16:37;rcmuir;Bulk closing for 3.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate Directory.touchFile,LUCENE-2027,12439834,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,04/Nov/09 10:09,03/Jun/11 16:37,30/Sep/19 08:38,18/May/11 23:22,,,,,,,,,,,3.2,4.0-ALPHA,,,core/store,,,0,,,,"Lucene doesn't use this method, and, FindBugs reports that FSDirectory's impl shouldn't swallow the returned result from File.setLastModified.",,,,,,,,,,,,,,,,"16/May/11 17:11;mikemccand;LUCENE-2027.patch;https://issues.apache.org/jira/secure/attachment/12479346/LUCENE-2027.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-11-04 16:15:21.84,,,false,,,,,,,,,,,,,,,11749,,,Fri Jun 03 16:37:14 UTC 2011,New,,,,,,,"0|i04tbr:",26016,,,,,,,,,"04/Nov/09 16:15;simonw;We talked about this yesterday night @ApacheCon. It would probably be the right thing to deprecate and introduce a new method returning a boolean. For now we where talking about throwing an IOException if setLastModified returns false. The Directory interface already has an IOException in the function signature so that would not break back compat but would yield the correct behaviour.

simon","04/Nov/09 16:20;mikemccand;Why not simply deprecate and then remove the method?  Nothing in Lucene uses it.","04/Nov/09 16:21;uschindler;Missed this issue, from java-user:

We discussed about this method yesterday in the evening. The abstract base
class defines the method as throwing an IOException. So the correct
behaviour would be to throw an IOException if setLastModified returns false
(which happens according to the docs, if the date cannot be changed because
of an IO/FS prob).
","16/May/11 17:11;mikemccand;Patch, removing Dir.touchFile from trunk.

For 3.x I'll deprecate.","03/Jun/11 16:37;rcmuir;Bulk closing for 3.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve ArrayUtil/CollectionUtil.*Sort() methods to early-reaturn on empty or one-element lists/arrays,LUCENE-2990,12502301,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,uschindler,uschindler,24/Mar/11 15:58,03/Jun/11 16:37,30/Sep/19 08:38,26/Mar/11 11:11,,,,,,,,,,,3.2,4.0-ALPHA,,,,,,0,,,,"It might be a good idea to make CollectionUtil or ArrayUtil return early if the passed-in list or array's length <= 1 because sorting is unneeded then. This improves maybe automaton or other places, as for empty or one-element lists no SorterTermplate is created.",,,,,,,,,,,,LUCENE-2989,,,,"24/Mar/11 16:31;uschindler;LUCENE-2990.patch;https://issues.apache.org/jira/secure/attachment/12474526/LUCENE-2990.patch","24/Mar/11 16:20;uschindler;LUCENE-2990.patch;https://issues.apache.org/jira/secure/attachment/12474522/LUCENE-2990.patch","24/Mar/11 16:10;uschindler;LUCENE-2990.patch;https://issues.apache.org/jira/secure/attachment/12474521/LUCENE-2990.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-03-24 18:26:52.364,,,false,,,,,,,,,,,,,,,2177,,,Fri Jun 03 16:37:10 UTC 2011,New,Patch Available,,,,,,"0|i04n6f:",25020,,,,,,,,,"24/Mar/11 16:20;uschindler;Simplier patch (no duplicate checks, missed to see delegation).","24/Mar/11 16:31;uschindler;Add corresponding tests","24/Mar/11 18:26;simonw;Uwe patch looks good. 

I wonder if we could change the variable name from l to length or maybe size.
I had to look twice here 
{code}
 if (l <= 1) return;
{code}

to realize that you don't compare one <= one :)","24/Mar/11 18:28;uschindler;Will do. Courier New is an ugly font :-)","26/Mar/11 11:11;uschindler;Renamed local variable from ""l"" to ""size"".

Committed trunk revision: 1085689
Committed 3.x revision: 1085691","03/Jun/11 16:37;rcmuir;Bulk closing for 3.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.lucene.analysis.cn.ChineseTokenizer missing offset decrement,LUCENE-324,12314474,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,saturnism@gmail.com,saturnism@gmail.com,14/Dec/04 18:09,02/Jun/11 22:04,30/Sep/19 08:38,05/Dec/05 08:09,,,,,,,,,,,1.9,,,,modules/analysis,,,0,,,,"Apparently, in ChineseTokenizer, offset should be decremented like bufferIndex
when Character is OTHER_LETTER.  This directly affects startOffset and endOffset
values.

This is critical to have Highlighter working correctly because Highlighter marks
matching text based on these offset values.","Operating System: All
Platform: All",,,,,,,,,,,,,,,"15/Dec/04 12:01;saturnism@gmail.com;ASF.LICENSE.NOT.GRANTED--ChineseTokenizerTest.java;https://issues.apache.org/jira/secure/attachment/12312463/ASF.LICENSE.NOT.GRANTED--ChineseTokenizerTest.java","14/Dec/04 18:10;saturnism@gmail.com;ASF.LICENSE.NOT.GRANTED--chinese_tokenizer-missing_offset.patch;https://issues.apache.org/jira/secure/attachment/12312462/ASF.LICENSE.NOT.GRANTED--chinese_tokenizer-missing_offset.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,32687.0,,,2004-12-15 08:13:20.0,,,false,,,,,,,,,,,,,,,13425,,,Mon Dec 05 08:09:01 UTC 2005,,,,,,,,"0|i053vj:",27725,,,,,,,,,"14/Dec/04 18:10;saturnism@gmail.com;Created an attachment (id=13749)
Patch for ChineseTokenizer to correctly count offsets
","15/Dec/04 08:13;otis@apache.org;Ray: is there a simple way you can show that this is indeed a needed fix?  Maybe
a short class that shows that offsets are wrong.


Lucene developers: can anyone confirm whether this is really needed it?  I don't
use ChineseTokenizer enough to know for sure if this is a good fix, or something
that will break the code.","15/Dec/04 12:01;saturnism@gmail.com;Created an attachment (id=13758)
Testcase that tests ChineseTokenizer and OTHER_LETTER offsets

The problem arises when OTHER_LETTER characters and the rest of the characters
are mixed together.  When given a string ""a&#22825;b"", tokens and corresponding
offsets should be the following:
a : (0, 1)
&#22825; : (1, 2)
b : (2, 3)","15/Dec/04 12:16;saturnism@gmail.com;I haven't done a formal trace of the code yet, but I think it would make sense
that the offset should only be incremented if the character is pushed into the
buffer.  Current code, howerver, increments offset by default, regardless
whether the character is pushed into the buffer.

If that's the case, then there are more places that needs to be fixed.","05/Dec/05 08:09;ehatcher;Ray - ???  (let's see if JIRA can handle Chinese :)  Sorry for the delay in applying this patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FIXME in src/test/org/apache/lucene/IndexTest.java,LUCENE-354,12314504,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,java-dev@lucene.apache.org,hdiwan,hdiwan,03/Mar/05 06:55,02/Jun/11 22:04,30/Sep/19 08:38,27/May/06 01:38,,,,,,,,,,,,,,,modules/examples,,,0,,,,"Index: src/test/org/apache/lucene/IndexTest.java
===============================================================
====
--- src/test/org/apache/lucene/IndexTest.java   (revision 155945)
+++ src/test/org/apache/lucene/IndexTest.java   (working copy)
@@ -27,8 +27,7 @@   
public static void main(String[] args) {
     try {
       Date start = new Date();
-      // FIXME: OG: what's with this hard-coded dirs??
-      IndexWriter writer = new IndexWriter(""F:\\test"", new SimpleAnalyzer(),
+      IndexWriter writer = new IndexWriter(File.createTempFile(""luceneTest"",""idx""), new 
SimpleAnalyzer(),
                                           true);
        writer.setMergeFactor(20);","Operating System: Mac OS X 10.3
Platform: Macintosh",,,,,,,,,,,,,,,"03/Mar/05 06:56;hdiwan;ASF.LICENSE.NOT.GRANTED--test.pat;https://issues.apache.org/jira/secure/attachment/12312533/ASF.LICENSE.NOT.GRANTED--test.pat",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,33820.0,,,2005-03-12 12:31:10.0,,,false,,,,,,,,,,,,,,,13395,,,Sat Mar 12 12:31:10 UTC 2005,,,,,,,,"0|i053ov:",27695,,,,,,,,,"03/Mar/05 06:56;hdiwan;Created an attachment (id=14390)
patch to fix issue 

This is a duplicate of what's included in the description","12/Mar/05 12:31;otis@apache.org;Thanks, applied.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Make ""ant -projecthelp"" show the javadocs and docs targets as well",LUCENE-447,12317716,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,lvl,lvl,07/Oct/05 19:00,02/Jun/11 22:03,30/Sep/19 08:38,08/Oct/05 20:04,,,,,,,,,,,1.9,,,,core/other,,,0,,,,"Added a description to the targets ""javadocs"" and ""docs"".
This makes ant show them when the executes ""ant -projecthelp""
",diff with revision 307059 (2005-10-07 11:50),,,,,,,,,,,,,,,"07/Oct/05 19:02;lvl;build.diff;https://issues.apache.org/jira/secure/attachment/12314759/build.diff",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2005-10-08 20:04:04.0,,,false,,,,,,,,,,,,,,,13302,,,Sat Oct 08 20:04:04 UTC 2005,,,,,,,,"0|i05347:",27602,,,,,,,,,"07/Oct/05 19:02;lvl;attached diff.","08/Oct/05 20:04;lucenebugs@danielnaber.de;Thanks, the patch has juts been committed.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use FileFilter instead of FileNameFilter in FSDirectory.listAll,LUCENE-3132,12507998,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Invalid,shaie,shaie,shaie,22/May/11 07:54,22/May/11 07:59,30/Sep/19 08:38,22/May/11 07:59,,,,,,,,,,,,,,,,,,0,,,,"FSDirectory.listAll() uses FileNameFilter, but all it does is check whether the File + name given denotes a directory. For that, it does new File(dir, file).isDirectory(). If we use FileFilter, new File() won't be necessary. This is a trivial thing, I'll post a patch soon.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,10808,,,Sun May 22 07:59:59 UTC 2011,New,,,,,,,"0|i04mbr:",24882,,,,,,,,,"22/May/11 07:59;shaie;Sorry, hit the 'create' button instead getting out the ""Issue create process"". FilenameFilter is required because we return the file names (we call File.list() and not File.listFiles()).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
no need for LowerCaseFilter from ArabicAnalyzer,LUCENE-2786,12491735,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Not A Problem,,ibrahim,ibrahim,01/Dec/10 05:29,16/May/11 18:16,30/Sep/19 08:38,01/Dec/10 12:02,3.0.2,,,,,,,,,,,,,,modules/analysis,,,0,,,,"No need for this line 171:
result = new LowerCaseFilter(result);

in ArabicAnalyzer

simply because there is no lower case or upper case in Arabic language. it is totally not related to each other.",All,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-12-01 05:49:41.788,,,false,,,,,,,,,,,,,,,11068,,,Wed Dec 01 12:02:37 UTC 2010,New,,,,,,,"0|i04ofr:",25224,,,,,,,,,"01/Dec/10 05:49;dmsmith555;I bet it is there for mixed language texts.","01/Dec/10 06:48;uschindler;Hi Ibrahim,

the LowerCaseFilter is there for english text embedded into arabic (like western names). All Analyzers in Lucene work that way.","01/Dec/10 07:04;ibrahim;O.K i think i messed this

Thanks
","01/Dec/10 12:02;rcmuir;lowercasefilter is there for non-arabic text, for consistency with other analyzers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
document LengthFilter wrt Unicode 4.0,LUCENE-2070,12440831,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,rcmuir,rcmuir,16/Nov/09 17:41,16/May/11 18:15,30/Sep/19 08:38,24/Sep/10 00:48,,,,,,,,,,,3.1,4.0-ALPHA,,,modules/analysis,,,0,,,,"LengthFilter calculates its min/max length from TermAttribute.termLength()
This is not characters, but instead UTF-16 code units.

In my opinion this should not be changed, merely documented.
If we changed it, it would have an adverse performance impact because we would have to actually calculate Character.codePointCount() on the text.

If you feel strongly otherwise, fixing it to count codepoints would be a trivial patch, but I'd rather not hurt performance.
I admit I don't fully understand all the use cases for this filter.
",,,,,,,,,,,,,,,,"16/Nov/09 17:42;rcmuir;LUCENE-2070.patch;https://issues.apache.org/jira/secure/attachment/12425099/LUCENE-2070.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-30 15:50:18.859,,,false,,,,,,,,,,,,,,,11707,,,Wed Mar 30 15:50:18 UTC 2011,New,Patch Available,,,,,,"0|i04t27:",25973,,,,,,,,,"24/Sep/10 00:48;rcmuir;Committed revision 1000675, 1000678 (3x)","30/Mar/11 15:50;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
some valid email address characters not correctly recognized,LUCENE-1556,12416543,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,paulnilsson,paulnilsson,10/Mar/09 15:15,16/May/11 18:15,30/Sep/19 08:38,29/Sep/10 05:50,2.4.1,,,,,,,,,,3.1,4.0-ALPHA,,,modules/analysis,,,0,,,,"the EMAIL expression in StandardTokenizerImpl.jflex misses some unusual but valid characters in the left-hand-side of the email address. This causes an address to be broken into several tokens, for example:

somename+site@gmail.com gets broken into ""somename"" and ""site@gmail.com""
husband&wife@talktalk.net gets broken into ""husband"" and ""wife@talktalk.net""

These seem to be occurring more often. The first seems to be because of an anti-spam trick you can use with google (see: http://labnol.blogspot.com/2007/08/gmail-plus-smart-trick-to-find-block.html). I see the second in several domains but a disproportionate amount are from talktalk.net, so I expect it's a signup suggestion from the service.

Perhaps a fix would be to change line 102 of StandardTokenizerImpl.jflex from:
EMAIL      =  {ALPHANUM} (("".""|""-""|""_"") {ALPHANUM})* ""@"" {ALPHANUM} (("".""|""-"") {ALPHANUM})+

to 

EMAIL      =  {ALPHANUM} (("".""|""-""|""_""|""+""|""&"") {ALPHANUM})* ""@"" {ALPHANUM} (("".""|""-"") {ALPHANUM})+

I'm aware that the StandardTokenizer is meant to be more of a basic implementation rather than an implementation the full standard, but it is quite useful in places and hopefully this would improve it slightly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-09-29 05:50:14.801,,,false,,,,,,,,,,,,,,,12198,,,Wed Mar 30 15:49:52 UTC 2011,New,,,,,,,"0|i04w9b:",26491,,,,,,,,,"29/Sep/10 05:50;rcmuir;fixed in LUCENE-2167","30/Mar/11 15:49;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChineseFilter is inefficient,LUCENE-1943,12437160,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,rcmuir,rcmuir,02/Oct/09 21:29,16/May/11 18:15,30/Sep/19 08:38,03/Oct/09 13:55,,,,,,,,,,,3.0,,,,modules/analysis,,,0,,,,"trivial patch to use CharArraySet, so it can use termBuffer() instead of term()
",,,,,,,,,,,,,,,,"02/Oct/09 21:30;rcmuir;LUCENE-1943.patch;https://issues.apache.org/jira/secure/attachment/12421157/LUCENE-1943.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,11827,,,Sat Oct 03 13:55:21 UTC 2009,New,Patch Available,,,,,,"0|i04tuf:",26100,,,,,,,,,"03/Oct/09 13:55;rcmuir;Committed revision 821322.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
French elision filter should use CharArraySet,LUCENE-2021,12439483,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,rcmuir,rcmuir,30/Oct/09 03:52,16/May/11 18:15,30/Sep/19 08:38,30/Oct/09 11:25,,,,,,,,,,,3.0,,,,modules/analysis,,,0,,,,"French elision filter creates new strings, lowercases them, etc just to check against a Set<String>.
trivial patch to use chararrayset instead.",,,,,,,,,,,,,,,,"30/Oct/09 05:05;rcmuir;LUCENE-2021.patch;https://issues.apache.org/jira/secure/attachment/12423662/LUCENE-2021.patch","30/Oct/09 03:53;rcmuir;LUCENE-2021.patch;https://issues.apache.org/jira/secure/attachment/12423655/LUCENE-2021.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-10-30 04:53:46.588,,,false,,,,,,,,,,,,,,,11753,,,Fri Oct 30 11:25:46 UTC 2009,New,Patch Available,,,,,,"0|i04td3:",26022,,,,,,,,,"30/Oct/09 04:53;uschindler;The setArticles method could check with instanceof, if the supplied set is a chararrayset and use it directly (see StopFilter). Otherwise looks good.","30/Oct/09 05:00;rcmuir;bq. The setArticles method could check with instanceof, if the supplied set is a chararrayset and use it directly (see StopFilter). Otherwise looks good.

good idea, I'll fix this.","30/Oct/09 05:05;rcmuir;do not create a new chararrayset in setArticles if the supplied set is already a chararrayset.","30/Oct/09 11:25;rcmuir;Committed revision 831268.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Arabic Analyzer: Stopwords list needs enhancement,LUCENE-1966,12437657,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,narmok,narmok,08/Oct/09 22:52,16/May/11 18:15,30/Sep/19 08:38,14/Oct/09 12:24,2.9,,,,,,,,,,3.0,,,,modules/analysis,,,0,,,,"The provided Arabic stopwords list needs some enhancements (e.g. it contains a lot of words that not stopwords, and some cleanup) . patch will be provided with this issue.",,,,,,,,,,,,,,,,"11/Oct/09 09:36;narmok;LUCENE-1966.patch;https://issues.apache.org/jira/secure/attachment/12421819/LUCENE-1966.patch","08/Oct/09 23:07;narmok;LUCENE-1966.patch;https://issues.apache.org/jira/secure/attachment/12421683/LUCENE-1966.patch","08/Oct/09 23:07;narmok;arabic-stopwords-comments.txt;https://issues.apache.org/jira/secure/attachment/12421682/arabic-stopwords-comments.txt",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-10-09 01:04:09.561,,,false,,,,,,,,,,,,,,,11805,,,Wed Oct 14 12:24:48 UTC 2009,New,Patch Available,,,,,,"0|i04tpb:",26077,,,,,,,,,"08/Oct/09 23:07;narmok;Please see the arabic-stopwords-comments.txt to see my comments on the list, and why/what did I change.

The patch provides an updated Arabic stopwords file, and modifies ArabicAnalyzer to filter stopwords after the normalization, as the provided list is a normalized Arabic stop words.

Best,","09/Oct/09 01:04;rcmuir;Basem, thanks for the patch, and the comments.

One thing I noticed: if I apply the patch, على (the stopword) will not be filtered as a stopword. This is because it will be normalized to علي (the name).

So, if we are going to normalize before stopfilter, I think we need to make sure the stopwords do not contain yeh without dots, or else these will not work. This is one example of why I was scared to apply normalization before stopwords, because by doing so, we cause على and علي to conflate.

Let me know what you think about this.
","11/Oct/09 09:36;narmok;Robert, you are correct, to solve the problem we have two options: 
1- to remove words like علي and وفي
2- to use unnormalized stiowirds list, before the normalization filter.

I think the best is the second option, so this patch only modifies the list (unnormalized), please try it.","11/Oct/09 13:34;rcmuir;Basem, thanks. I like the new list.

I have one very minor question: in the list we have أيضا / ايضا twice.

I wanted to check with you, is this by accident or did you have some other spellings in mind?

If it is by accident, let me know, I can just remove the duplicates before committing.","11/Oct/09 14:53;narmok;Hi Robert,

Regarding ايضا / أيضا ...

No, not by accident, I included both formats (normalized,unnormalized). Arabic users tend to use both on the internet (different spellings), another example is words like أي / اي","11/Oct/09 15:01;rcmuir;Basem, I meant: there are two entries for أيضا , and two entries for ايضا (total of four)

edit: here are the relevant line numbers from the new stopwords.txt:

Lines 72 and 73:
{noformat}
ايضا
أيضا
{noformat}

Lines 123 and 124:
{noformat}
ايضا
أيضا
{noformat}","11/Oct/09 15:32;rcmuir;Basem I can simply remove 123 & 124 if this is the case, but I did not want to do this without checking first.

The reason is, I wonder if perhaps you intended for these two to be أيضاً and ايضاً (with fathatan)","11/Oct/09 17:21;narmok;Oh, my mistake, sorry, yes please remove the last two on 123 & 124.

no, they are just duplicate of the ones on line 72 & 73

","11/Oct/09 17:25;rcmuir;Basem, ok! Thanks a lot for your help here. I will commit soon.","11/Oct/09 18:25;rcmuir;before I commit this, I want to solicit any comments/concerns about backwards compat, assuming the following notice:

{noformat}
Changes in runtime behavior

 * LUCENE-1966: Modified and cleaned the default Arabic stopwords list used
   by ArabicAnalyzer. You'll need to fully re-index any previously created 
   indexes.  (Basem Narmok via Robert Muir)
{noformat}

i know contrib has no bw compat guarantee, but just want to double-check. 
Perhaps in the future someone might help fix the Persian stopwords file also so this may happen again :)
","11/Oct/09 22:22;narmok;Seems good.

BTW with FAST ESP we never used stopwords, as hits from stopwords get low relevancy (keywords with high number of hits = low value, low importance, so less relevant), so such hits will never get into the top results. Also, using stopwords will affect phrase search, most of the search engines avoid removing them. But, at the end it depends on the client's application, and what she really wants, as enterprise search could have very specific and different needs than Internet search.

Anyways, still I am testing the Arabic Analyzer, and I will provide you with more comments soon. but for the stopwords they are good for now :)","11/Oct/09 22:52;rcmuir;Basem, yes I think the improvements are good.

My question is really: is it OK to commit this for 3.0 or should we wait for 3.1?
","14/Oct/09 12:24;rcmuir;Committed revision 825110.

Thanks Basem!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lazy Atomic Loading Stopwords in SmartCN ,LUCENE-1965,12437632,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,simonw,simonw,simonw,08/Oct/09 18:32,16/May/11 18:15,30/Sep/19 08:38,08/Oct/09 19:33,2.9,,,,,,,,,,3.0,,,,modules/analysis,,,0,,,,"The default constructor in SmartChineseAnalyzer loads the default (jar embedded) stopwords each time the constructor is invoked. 
This should be atomically loaded only once in an unmodifiable set.

",,,,,,,,,,,,,,,,"08/Oct/09 19:08;simonw;LUCENE-1965.patch;https://issues.apache.org/jira/secure/attachment/12421650/LUCENE-1965.patch","08/Oct/09 18:33;simonw;LUCENE-1965.patch;https://issues.apache.org/jira/secure/attachment/12421646/LUCENE-1965.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-10-08 18:42:34.716,,,false,,,,,,,,,,,,,,,11806,,,Thu Oct 08 19:33:54 UTC 2009,New,Patch Available,,,,,,"0|i04tpj:",26078,,,,,,,,,"08/Oct/09 18:33;simonw;attached patch","08/Oct/09 18:42;rcmuir;Simon, everything is ok, but i have one comment:

the new test: testChineseStopWordsNull, I think this is a duplicate of the one above. here is the context:
{code}
  /*
   * Punctuation is handled in a strange way if you disable stopwords
   * In this example the IDEOGRAPHIC FULL STOP is converted into a comma.
   * if you don't supply (true) to the constructor, or use a different stopwords list,
   * then punctuation is indexed.
   */
  public void testChineseStopWordsOff() throws Exception {  
    Analyzer ca = new SmartChineseAnalyzer(false); /* doesnt load stopwords */
    String sentence = ""我购买了道具和服装。"";
    String result[] = { ""我"", ""购买"", ""了"", ""道具"", ""和"", ""服装"", "","" };
    assertAnalyzesTo(ca, sentence, result);
    
    
  }
  
  public void testChineseStopWordsNull() throws IOException{
    Analyzer ca = new SmartChineseAnalyzer(false); /* sets stopwords to empty set */
    String sentence = ""我购买了道具和服装。"";
    String result[] = { ""我"", ""购买"", ""了"", ""道具"", ""和"", ""服装"", "","" };
    assertAnalyzesTo(ca, sentence, result);
    assertAnalyzesToReuse(ca, sentence, result);
  }
{code}","08/Oct/09 19:08;simonw;Thanks robert, good catch! I was adding one test with null in the constructor but I missed to finish it apparently. 
I merged it into testChineseStopWordsOff().

Patch attached.
","08/Oct/09 19:13;rcmuir;Simon, cool. I like it now, think its a good improvement, same as with Persian and Arabic, thanks :)","08/Oct/09 19:33;simonw;commited in r823285

thx robert for reviewing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArabicAnalyzer: Lowercase before Stopfilter,LUCENE-1963,12437611,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,rcmuir,rcmuir,08/Oct/09 16:08,16/May/11 18:15,30/Sep/19 08:38,14/Oct/09 23:01,2.9,,,,,,,,,,2.9.1,3.0,,,modules/analysis,,,0,,,,"ArabicAnalyzer lowercases text in case you have some non-Arabic text around.
It also allows you to set a custom stopword list (you might augment the Arabic list with some English ones, for example).

In this case its helpful for these non-Arabic stopwords, to lowercase before stopfilter.
",,,,,,,,,,,,,,,,"08/Oct/09 16:33;rcmuir;LUCENE-1963.patch;https://issues.apache.org/jira/secure/attachment/12421635/LUCENE-1963.patch","08/Oct/09 16:09;rcmuir;LUCENE-1963.patch;https://issues.apache.org/jira/secure/attachment/12421632/LUCENE-1963.patch","14/Oct/09 14:27;rcmuir;LUCENE-1963_branch.patch;https://issues.apache.org/jira/secure/attachment/12422105/LUCENE-1963_branch.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-10-08 16:40:56.15,,,false,,,,,,,,,,,,,,,11808,,,Sat Nov 07 14:57:30 UTC 2009,New,Patch Available,,,,,,"0|i04tpz:",26080,,,,,,,,,"08/Oct/09 16:09;rcmuir;simple patch, but will need to warn in CHANGES.txt that folks should reindex, if they are using non-Arabic stopwords.","08/Oct/09 16:24;rcmuir;if no one objects, I'd like to commit this for 3.0 at the end of the day.","08/Oct/09 16:33;rcmuir;here also update the javadocs to reflect the new order of what is going on in ArabicAnalyzer, to prevent any confusion to users.","08/Oct/09 16:40;dmsmith;can you commit it to 2.9.1 too? (For those stuck on Java 1.4, there is no 3.0).","08/Oct/09 17:07;rcmuir;bq. can you commit it to 2.9.1 too? (For those stuck on Java 1.4, there is no 3.0). 

can someone comment on this one for me. 
I don't think its too much of a stretch to consider this a bug, even if it does not affect Arabic text.
","09/Oct/09 12:59;rcmuir;Committed revision 823534.
(if it is ok to apply this to 2.9 branch as DM requested, we should reopen)
","09/Oct/09 13:16;markrmiller@gmail.com;Your issue - if you can stretch it to bugish territory, I'd +1 it. I'd be wary of getting into porting features to 2.9.1 - but I wouldn't have a problem with this one myself.","09/Oct/09 13:22;rcmuir;Mark, I think the problem is really that I overlooked this use case in LUCENE-1758, because Arabic is not case sensitive.

It won't affect the default usage of the Analyzer (where all the stopwords are in Arabic and lowercase is a no-op).

I am going to also set fix for 2.9.1 and give a day or two for people to comment if they disagree with applying to 2.9 branch.","14/Oct/09 14:27;rcmuir;its been a few days, no one objected to applying this fix to the branch.

but I do not have permissions to commit to the branch... can someone commit this for me? Attached is the patch.","14/Oct/09 23:01;mikemccand;Committed on 2.9.x.  Thanks Robert!","07/Nov/09 14:57;mikemccand;Bulk close all 2.9.1 issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Patch for ShingleFilter.enablePositions (or PositionFilter),LUCENE-1380,12404066,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,gsingers,michaelsembwever,michaelsembwever,10/Sep/08 10:57,16/May/11 18:15,30/Sep/19 08:38,11/Dec/08 14:20,,,,,,,,,,,2.9,,,,modules/analysis,,,14,,,,"Make it possible for *all* words and shingles to be placed at the same position, that is for _all_ shingles (and unigrams if included) to be treated as synonyms of each other.

Today the shingles generated are synonyms only to the first term in the shingle.
For example the query ""abcd efgh ijkl"" results in:
   (""abcd"" ""abcd efgh"" ""abcd efgh ijkl"") (""efgh"" efgh ijkl"") (""ijkl"")

where ""abcd efgh"" and ""abcd efgh ijkl"" are synonyms of ""abcd"", and ""efgh ijkl"" is a synonym of ""efgh"".

There exists no way today to alter which token a particular shingle is a synonym for.
This patch takes the first step in making it possible to make all shingles (and unigrams if included) synonyms of each other.

See http://comments.gmane.org/gmane.comp.jakarta.lucene.user/34746 for mailing list thread.",,,,,,,,,,,,,,,,"24/Sep/08 22:07;sarowe;LUCENE-1380-PositionFilter.patch;https://issues.apache.org/jira/secure/attachment/12390874/LUCENE-1380-PositionFilter.patch","24/Sep/08 18:36;michaelsembwever;LUCENE-1380-PositionFilter.patch;https://issues.apache.org/jira/secure/attachment/12390861/LUCENE-1380-PositionFilter.patch","24/Sep/08 09:40;michaelsembwever;LUCENE-1380-PositionFilter.patch;https://issues.apache.org/jira/secure/attachment/12390821/LUCENE-1380-PositionFilter.patch","17/Sep/08 11:30;michaelsembwever;LUCENE-1380.patch;https://issues.apache.org/jira/secure/attachment/12390261/LUCENE-1380.patch","12/Sep/08 14:09;karl.wettin;LUCENE-1380.patch;https://issues.apache.org/jira/secure/attachment/12390007/LUCENE-1380.patch",,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2008-09-10 15:07:26.525,,,false,,,,,,,,,,,,,,,12371,,,Thu Dec 11 14:20:43 UTC 2008,New,Patch Available,,,,,,"0|i04xcv:",26669,,,,,,,,,"10/Sep/08 11:00;michaelsembwever;Addition to ShingleFilter for property coterminalPositionIncrement.
New corresponding test in ShingleFilterTest.","10/Sep/08 15:07;sarowe;As I said in the thread on java-user that spawned this issue: <http://www.nabble.com/Replacing-FAST-functionality-at-sesam.no---ShingleFilter%2B-exact-matching-td19396291.html> (emphasis added):

{quote}
It works because you've set all of the shingles to be at the same position - probably better to change the one instance of .setPositionIncrement(0) to .setPositionIncrement(1) - that way, MultiPhraseQuery will not be invoked, and the standard disjunction thing should happen.

> [W]ould a patch to ShingleFilter that offers an option
> ""unigramPositionIncrement"" (that defaults to 1) likely be
> accepted into trunk?

The issue is not directly related to whether a unigram is involved, but rather whether or not _*tokens that begin at the same word*_ are given the same position.  The option thus should be named something like ""coterminalPositionIncrement"".  This seems like a reasonable addition, and a patch likely would be accepted, if it included unit tests.
{quote}

You have used the option name I suggested, but have implemented it in a form that doesn't follow the name -- in your implementation, *all* tokens are placed at the same position, not just those that start at the same word -- and I think this form is inappropriate for the general user.

I'm -1 on the patch in its current form.  If rewritten to modify the position increment only for those shingles that begin at the same word, I'd be +1 (assuming it works and is tested appropriately).","10/Sep/08 16:17;michaelsembwever;i suspected such re the option name, but ""coterminal"" is a word i haven't used since high school.

> I'm -1 on the patch in its current form. If rewritten to modify the position increment only for those shingles that begin at the same word, I'd be +1 (assuming it works and is tested appropriately).

As i said in thread your suggestion does not work.
Setting each shingle to have a positionIncrement=1 so to avoid using the MultiPhraseQuery in favour of the plain PhraseQuery makes sense, but does not work. And not phrasing the query doesn't invoke the ShingleFilter properly.

> The ShingleFilter appears to only work, at least for me, on phrases.
> I would think this correct as each shingle is in fact a sub-phrase to the larger original phrase.

If this is the case, ie ShingleFilter works on phrases as a whole entity, and that shingles from each term in the phrase do have a relationship as they all come from the one phrase, then does it not make sense to have the possibility to position them altogether.

For example in the current implementation, in the phrase ""abcd efgh ijkl"" it is the first term ""abcd"" that is responsible for generating the shingles ""abcd efgh ijkl"" and ""abcd efgh"". 
What  says that these shingles couldn't be generated from the ""efgh"" (or ""ijkl"" for the former shingle) term in an alternative implementation?
Why the presumption that it's in the user's interest to force this separation between where this implementation chooses to put its shingles?

If this isn't lost-in-the-bush-logic, have you a suggestion for a more appropriate option name for the current solution?","11/Sep/08 12:51;michaelsembwever;New version with option named enablePositions","12/Sep/08 14:09;karl.wettin;Renamed field to usingPositionIncrement to avoid confusion, and added a bunch of javadocs compiled from the issue comments:

{code:java}
/**
   * If true each original token (unigram) or the first related shingle from it
   * will get a {@link org.apache.lucene.analysis.Token#getPositionIncrement() positionIncrement} of 1,
   * if false all shingle tokens will get a {@link org.apache.lucene.analysis.Token#getPositionIncrement() positionIncrement} of 0.
   * <p>
   * Default value is true.
   * <p>
   * This attribute is typically set false in conjunction with use of the QueryParser that
   * when set true will create a MultiPhraseQuery where at least one word/shingle must be
   * matched from each word/token, not desired in all situations. Setting this to false
   * will instead create a PhraseQuery.
   *
   * @param usingPositionIncrement the coterminal token positionIncrement setting.
   */
  public void setUsingPositionIncrement(boolean usingPositionIncrement){
      this.usingPositionIncrement = usingPositionIncrement;
  }
{code}

Did I get that right?

Steve, are you still -1? I don't see any harm in this patch.","12/Sep/08 22:50;karl.wettin;One could argue that what you should do rather than using this patch is to add a TokenFilter that sets all positionIncrement to 0.","13/Sep/08 12:37;michaelsembwever;> One could argue that what you should do rather than using this patch is to add a TokenFilter that sets all positionIncrement to 0. 

Really? You'll have to excuse me - i am very new to Lucene.
How would i go about that? Such a TokenFilter exists already?

> Setting this to false will instead create a PhraseQuery.

This isn't correct. PhraseQuery is used when every token has a non-zero positionIncrement, ie when severalTokensAtSamePosition == false.
What does happen is that the MultiPhraseQuery that is constructed is limited to one-dimension.
","13/Sep/08 13:57;karl.wettin;>> One could argue that what you should do rather than using this patch is to add a TokenFilter that sets all positionIncrement to 0.
>Really? You'll have to excuse me - i am very new to Lucene.
>How would i go about that? Such a TokenFilter exists already?

{code:java}
new TokenFilter(input) {
  public Token next(Token reusableToken) throws IOException {
    reusableToken = input.next(reusableToken);
    reusableToken.setPositionIncrement(0);
    return reusableToken;
  }
};
{code}","13/Sep/08 14:38;michaelsembwever;Ok. So there's no way to do it through configuration only.
Would a patch with such a TokenFilter be useful for anybody else other than ShingleFilter users? Again i'm a newbie here but i suspect there's no other filter (yet) which works _across_ the tokens (and hence breaks down the importance of positionIncrement) within a query in the way ShingleFilter does. for example from the mailing list from steve:
> On the other hand, I'm not sure how useful position information is for shingles in the general case: they already have relative position info 
> embedded within them.  And how likely is it that one would want to perform a phrase/span query over shingles?  Pretty unlikely, ...","14/Sep/08 12:47;karl.wettin;bq. Ok. So there's no way to do it through configuration only.

In Solr? Well, I don't really do Solr but I'm pretty sure all you have to do is to create the filter as a new class, add it to the class path and add it as a filter to the query analyzer in your configuration.

bq. Would a patch with such a TokenFilter be useful for anybody else other than ShingleFilter users? 

I'd say no, that it only seems to make sense for shingles at query parsing time.

bq. Again i'm a newbie here but i suspect there's no other filter (yet) which works across the tokens (and hence breaks down the importance of positionIncrement) within a query in the way ShingleFilter does.

I don't understand what you say here. All this patch does is to set all position increment of the tokens produced by the ShingleFilter to 0, right? 

I'm going to remove this for 2.4 fix and recommend you to use the filter strategy mentioned. I'll leave the issue open for discussion though.","14/Sep/08 12:57;michaelsembwever;> All this patch does is to set all position increment of the tokens produced by the ShingleFilter to 0, right? 
> I'm going to remove this for 2.4 fix and recommend you to use the filter strategy mentioned. 

The patch to add the new TokenFilter isn't easy-as-abc as lucene needs to have the filter class added to classpath, and Solr needs the TokenFilterFactory added to be able to read it from the configuration files. A lot of work when we're (almost) agreed that removing positional information from all tokens makes sense when using the ShingleFilter.

If it were just the one installation i wouldn't have a problem with adding the custom TokenFilter, but because our use-case is an open sourced and documented system ( read http://sesat.no/howto-solr-query-evaluation.html ) i'd like to make it as easy as possible for third parties.

I would also think that because this is a way to replace commercial and competing technology from FAST that the community would be behind such an enhancement...","15/Sep/08 11:16;michaelsembwever;Updated description to include a more layman's explanation.
Maybe the option should be called ""commonSynonyms"" or the like...","15/Sep/08 11:36;michaelsembwever;typo is editing description.","17/Sep/08 11:30;michaelsembwever;Updated version that ensures first token always has positionIncrement=1

(Karl's changes from his patch are in this patch).","22/Sep/08 15:15;karl.wettin;I'm unassigning myself from this issue as there are so many votes and I consider it a hack to add a change whos soul purpose is to change the behavior of a query parser and I don't think such a thing should be committed. I think the focus should be on the query parser and I understand that is a lot more work than modifying the shingle filter. If you really want to do this change is this layer I suggest that you seperate out this feature to a new filter that modify the position increment.","22/Sep/08 15:43;michaelsembwever;> separate out this feature to a new filter that modify the position increment. 

As Chris explained in the list this approach would clobber all terms into one big synonym group. There may be other terms in the query outside of the quotes which should not be treated as synonyms to the shingles. And it was also mentioned that there were known bugs when the first token had positionIncrement=0 (or all tokens lay at position zero instead of at position one).
i imagine that this rules out such a position increment TokenFilter.","22/Sep/08 15:51;sarowe;{quote}
bq. separate out this feature to a new filter that modify the position increment. 

There may be other terms in the query outside of the quotes which should not be treated as synonyms to the shingles.
{quote}

but they won't be in the same field, right?  Solr has per-field analysis facilities.

bq. And it was also mentioned that there were known bugs when the first token had positionIncrement=0 (or all tokens lay at position zero instead of at position one).

You can tell the filter to set posincr=1 for the first token.

When it receives null from its predecessor in the filter chain, it can reset its ""at the beginning"" flag, and the next time it's used, it'll give posincr=1 for the first token again.","23/Sep/08 14:34;michaelsembwever;> If you really want to do this change is this layer I suggest that you seperate out this feature to a new filter that modify 
> the position increment.

Attaching alternative patch as suggested for PositionFilter and its test.
The first token always maintains its original positionIncrement, but subsequent tokens in the TokenStream has their positionIncrement set to match the value of PositionFilter.positionIncrement

I still fail to understand why Karl and Steve would rather see this logic in the QueryParser. The best explanation so far was from Steve:
> IMO, the correct layer to solve this is in Solr's QParser - 
> I think there should be a way to tell the parser not to parse, but rather to send the whole query to be analyzed.

but i wouldn't be surprised if this goes against the grain of how Solr works.
","23/Sep/08 14:54;sarowe;A couple of comments on the PositionFilter patch:

# The javadocs should be more explicit, e.g. about the fact that positionIncrement defaults to zero
# I think there ought to be a constructor that takes in a positionIncrement, perhaps instead of the setter.
# You don't handle the case where the filter is used for more than one document; there should be an else clause that resets firstTokenPositioned to false after this block:
{code:java}
if(null != reusableToken){
  if(firstTokenPositioned){
    reusableToken.setPositionIncrement(positionIncrement);
  }else{
    firstTokenPositioned = true;
  }
}
{code}
# You should provide a standalone test for the PositionFilter, in addition to the ShingleFilterTest tests.","24/Sep/08 09:40;michaelsembwever;Re-attached the PositionFilter patch addressing Steve's moderation comments. ","24/Sep/08 16:20;sarowe;When I wrote:
bq. 4.  You should provide a standalone test for the PositionFilter, in addition to the ShingleFilterTest tests.

I meant that testing of PositionFilter should be separate from testing its functionality with ShingleFilter.  Your PositionFilter tests looks at offsets, which PositionFilter doesn't affect at all.  It is possible that PositionFilter will be used for other things than ShingleFilter.  Hence, there should be basic test(s) that evaluate PositionFilter without ShingleFilter.

I also think a test to make sure a single instance of PositionFilter will work with multiple documents should be added.

BTW, you don't need to delete JIRA attachments if you want to upload a new version - when you upload a same-named file, the most recent version of the file will be colored black, and older versions will be colored gray.  This is the conventional way Lucene uses JIRA.  It allows people to follow the JIRA comments in the progressive versions of the patch(es).

A typo on line 66 of PositionFilterTest: 
{code:java}
            // end of stream so reset firstTokePositioned
{code}
","24/Sep/08 18:36;michaelsembwever;Re-attached the PositionFilter patch addressing Steve's moderation comments. (2)
Steve,  can you look at the reset versus null token in stream difference. Are both approaches valid to test? (I'd not overridden TokenStream.reset() in the previous patch).","24/Sep/08 22:07;sarowe;Mck, I was wrong about Filter testing over multiple docs - each instance of a Filter is defined only over a single doc, so this doesn't make sense.

However, you are completely on the right track with the reset() operation, since PositionFilter is sensitive to whether it's at the beginning of a stream, and it should respond as you have written it.

So, since I was wrong about PositionFilter needing to handle usage with multiple documents, the else clause that I said should go in (upon receiving null from the input stream) should come back out.  In fact, the proper response from a filter in the analysis chain upon encountering null is to stop processing, since it means end-of-stream, so I've removed your tests with null embedded in this revised patch.

bq. Steve, can you look at the reset versus null token in stream difference. Are both approaches valid to test? (I'd not overridden TokenStream.reset() in the previous patch).

I removed the void-return filterTest(), since it wasn't called from anywhere, and it only used ShingleFilter, and no PositionFilter.  In its place I've added another test named testReset().

I added a test that checks for non-default positionIncrement: testNonZeroPositionIncrement().

I removed PositionFilter.setPositionIncrement(), because using it one could potentially change the position increment in mid-stream, which makes little sense.  The alternate constructor provides a way to set it.

In the patch, I have modified the formatting a little to conform to Lucene convention, which is outlined on the [HowToContribute wiki page|http://wiki.apache.org/lucene-java/HowToContribute#head-59ae13df098fbdcc46abdf980aa8ee76d3ee2e3b]:

{quote}
* Code should be formatted according to [Sun's conventions|http://java.sun.com/docs/codeconv/] with one exception:
** indent two spaces per level, not four.
{quote}

I ran ""svn diff"" under the trunk/ directory, instead of in trunk/contrib/analyzers/ (where you based your patches) - it's simpler for people who look at a lot of these things to have them always be based from trunk/.

Take a look and make sure things are as they should be - the tests pass for me, and I think it's doing what it should do.

If you agree, then hopefully we can get Karl (or another committer, which I'm not) to take a look and see if they think it can be committed.
","25/Sep/08 08:30;michaelsembwever;> Take a look and make sure things are as they should be - the tests pass for me, and I think it's doing what it should do.

Tests run, and code works in my usecase. Thanks Steve.","08/Dec/08 13:34;michaelsembwever;ping. are there any committors willing to commit these changes?","11/Dec/08 13:26;gsingers;Just to be clear, Mck, what changes are you asking about?  The position filter one or the broader Shingle one?

If I'm reading the thread correctly, I think everyone settled on just going w/ the position filter changes, right?","11/Dec/08 13:40;michaelsembwever;Yes we agreed with the PositionFilter approach. 
It works well (and is in production at http://sesam.no) 
and steers clear of having to decide whether ShingleFilter, solely by itself, was intended to be used in such a manner and hence if such positioning functionality should be encapsulated there.
","11/Dec/08 14:20;gsingers;Committed revision 725691.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cosmetic JavaDoc updates,LUCENE-1670,12426797,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,holograph,holograph,01/Jun/09 04:30,13/May/11 18:21,30/Sep/19 08:38,01/Jun/09 10:28,,,,,,,,,,,2.9,,,,general/javadocs,,,0,,,,"I've taken the liberty of making a few cosmetic updates to various JavaDocs:

* MergePolicy (minor cosmetic change)
* LogMergePolicy (minor cosmetic change)
* IndexWriter (major cleanup in class description, changed anchors to JavaDoc links [now works in Eclipse], no content change)

Attached diff from SVN r780545.

I would appreciate if whomever goes over this can let me know if my issue parameter choices were correct (yeah, blame my OCD), and if there's a more practical/convenient way to send these in, please let me know :-)
",Lucene SVN (diff from r780545),,,,,,,,,,,,,,,"01/Jun/09 07:11;holograph;lucene_1670.patch;https://issues.apache.org/jira/secure/attachment/12409540/lucene_1670.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-06-01 05:37:54.5,,,false,,,,,,,,,,,,,,,12088,,,Mon Jun 01 14:30:36 UTC 2009,New,Patch Available,,,,,,"0|i04vjj:",26375,,,,,,,,,"01/Jun/09 04:31;holograph;Patch against r780545","01/Jun/09 05:37;shaie;I have a couple of comments:

# In LogMergePolicy, you add a reference to mergeFactor ({@link #mergeFactor}), which is a private member of LMP. Wouldn't that be an issue when generating the javadocs (i.e., create a link to a non-existent entity, since private members are not generated in the javadocs)?
# In MergePolicy you added a link to SegmentInfo. Same as above - SegmentInfo is package private and therefore I'm not sure its included in the javadocs. If I look here (http://lucene.apache.org/java/2_4_0/api/core/index.html), SegmentInfo does not appear there.
# In IndexWriter, you add references like this: {@link #addDocument(Document) addDocument} - is the 'addDocument' in the end necessary. I've tried it and it doesn't come out nicely in the javadocs. Same goes for {@link #updateDocument(Term, Document) updateDocument}.
# In IndexWriter you replaced *see <a href=""#mergePolicy"">* with *see {@link #mergePolicy below}* - that's wrong for two reasons: (1) mergePolicy is private (see comments above) and (2) the javadocs actually have an element afterwards: *<a name=""mergePolicy""></a>*. The part you replaced references that element, rather than the mergePolicy member (which just happen to have the same name :)).","01/Jun/09 06:06;holograph;Good comments all around, I should definitely have caught all of those. I'll post an update presently.","01/Jun/09 07:09;holograph;Alright, I'll attach a new patch in a couple of minutes that has the following improvements:
* LogMergePolicy: You are correct. I removed the links, as well as tried rewriting the paragraph to be a little clearer.
* MergePolicy: This is not a problem; it does not appear in the API, but it's still linked (hence refactoring tools such as Eclipse will update the link value as well as the class name), and it shows in monospaced font as you'd expect with <code>.
* IndexWriter: Re (3), the extra token is the apparent text for the link (in this case, it ""hides"" the parameters). I merely ""translated"" from the previous anchor tag to links.
* IndexWriter: Fixed a few typoes
* IndexWriter: Reverted the mergePolicy link change
* IndexWriter: Fixed missing IndexSearcher import for link

","01/Jun/09 07:11;holograph;Patch for r780565","01/Jun/09 10:24;shaie;Ok I checked and if you reference an item which is not generated in the Javadocs, it appears in monospace, with no link. So I guess referencing SegmentInfo is fine. Just as a FYI, not everybody use Eclipse, and Eclipse (I think since 3.4) has the ability to update String references to class names you refactor.

This look good now. Unfortunately, I'm not a committer so I cannot take this issue any further :). Hope my review will help the committer that will choose to commit it.","01/Jun/09 10:28;mikemccand;Thanks Tomer!  I just committed this.  There was one warning the patched caused when running ""ant javadocs-core"":

{code}
[javadoc] /lucene/src/clean/src/java/org/apache/lucene/index/IndexWriter.java:215: warning - Tag @link: reference not found: IndexSearcher
{code}

(because IndexSearcher wasn't imported in IndexWrieter.java).  So I just reverted that change (made it fully qualified again).","01/Jun/09 13:01;holograph;That's odd -- one of the changes I made in the second version was to fix that precise warning (added an import directive). The last bullet even mentions it :-/
Anyway glad to be of help!
","01/Jun/09 14:22;mikemccand;Hmm I see that bullet too, but when I look at the patch it doesn't have the added import?  Maybe something got mixed up when you attached the patch?  Are there any other intended changes missing?","01/Jun/09 14:30;holograph;Yeah, the minor typo I fixed. It's not in the diff though. Oh well, I'll get it next time...
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultiFieldQueryParser field boost multiplier,LUCENE-544,12331934,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,karl.wettin,karl.wettin,11/Apr/06 23:13,11/Apr/11 13:06,30/Sep/19 08:38,17/Oct/06 22:26,,,,,,,,,,,,,,,core/queryparser,,,0,,,,"Allows specific boosting per field, e.g. +(name:foo^1 description:foo^0.1).

Went from String[] field to MultiFieldQueryParser.FieldSetting[] field in constructor. ",,,,,,,,,,,,,,,,"14/Apr/06 21:38;karl.wettin;MultiFieldQueryParser.java;https://issues.apache.org/jira/secure/attachment/12325338/MultiFieldQueryParser.java","11/Apr/06 23:15;karl.wettin;MultiFieldQueryParser.java;https://issues.apache.org/jira/secure/attachment/12325204/MultiFieldQueryParser.java","23/Jul/06 13:17;karl.wettin;MultiFieldQueryParser.java.diff;https://issues.apache.org/jira/secure/attachment/12337362/MultiFieldQueryParser.java.diff","22/May/06 06:17;karl.wettin;MultiFieldQueryParser.java.diff;https://issues.apache.org/jira/secure/attachment/12334397/MultiFieldQueryParser.java.diff","17/Oct/06 20:50;switchhook;QueryParserPatch;https://issues.apache.org/jira/secure/attachment/12343102/QueryParserPatch",,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2006-05-18 02:36:17.0,,,false,,,,,,,,,,,,,,,13206,,,Mon Apr 11 13:06:36 UTC 2011,,,,,,,,"0|i052hr:",27501,,,,,,,,,"11/Apr/06 23:15;karl.wettin;The updated code","14/Apr/06 21:38;karl.wettin;now 

* backwards compatible

but also

* serializable (should probably be made externalizable too)
* has hashCode() and equals()

","18/May/06 02:36;otis;Karl - can you submit this as a patch/diff, please?  I could use this myself, so I'd love to commit this.

svn diff -bBt ...  would probably be good.  Also, if you have a patch for MFQP unit test class, please attach those, too.

","18/May/06 19:39;karl.wettin;Otis Gospodnetic commented on LUCENE-544:
> -----------------------------------------
> 
> Karl - can you submit this as a patch/diff, please?

Sure. Can you wait for the weekend?","19/May/06 00:02;otis;But of course.","22/May/06 06:17;karl.wettin;I must have uploaded a bad version last time. Sorry. There was a compilation problem in:

 public static Query parse(String query, String[] fields, Analyzer analyzer)
            throws ParseException {

but that is a depricated method. I simply removed it in the one I've been running.

You are getting no test cases this time. 
I have to do some other things first(tm).

It passes the current though.","23/Jul/06 13:17;karl.wettin;A new patch that does not screw up the formatting and that is up to date with 2.0","17/Oct/06 20:37;switchhook;I have been working on this exact same problem 

Have you created an tests for it?

I am attaching my version. My version requires use of a map to be passed to the constructor


Matt","17/Oct/06 20:39;switchhook;This is my version of the Query Parser that will allow the users to boost some fields","17/Oct/06 20:50;switchhook;My Version of the QueryParser that will allow you to boost your fields

This version used a Map to keep track of what field to boost","17/Oct/06 22:26;otis;I decided to go with Matt's version - smaller change to the class + a unit test.  Thanks Matt!

Karl: if any functionality from your modification is missing after this patch, please feel free to make mods to MFQP in HEAD and attach a patch.
","18/Oct/06 10:03;karl.wettin;Great stuff Matt! Thanks for the contribution! I didn't use this feature for a while now due to my original messy patchwork. And Otis, I totaly understand you didn't commit that. I wouldn't :)","11/Apr/11 13:06;rene.scheibe;There is a problem with parsing the query ""one* two*"".

Using the tests from the patch, I would expect:

(b:one*^5.0 t:one*^10.0) (b:two*^5.0 t:two*^10.0)

But I get:

(b:one* t:one*) (b:two* t:two*)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultSimilarity.queryNorm() should never return Infinity,LUCENE-901,12370660,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,michaelbusch,michaelbusch,01/Jun/07 02:02,30/Mar/11 15:50,30/Sep/19 08:38,25/Jan/11 13:22,,,,,,,,,,,3.1,,,,core/search,,,0,,,,"Currently DefaultSimilarity.queryNorm() returns Infinity if sumOfSquaredWeights=0.
This can result in a score of NaN (e. g. in TermScorer) if boost=0.0f.

A simple fix would be to return 1.0f in case zero is passed in.

See LUCENE-698 for discussions about this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-06-01 05:31:49.808,,,false,,,,,,,,,,,,,,,12841,,,Wed Mar 30 15:50:32 UTC 2011,New,,,,,,,"0|i050av:",27146,,,,,,,,,"01/Jun/07 05:31;hossman;I'm not sure if i agree with this concept. Do we really want the curve of values from queryNorm to have a step drop down from really *huge* values when sumOfSquaredWeights is ""near"" zero to ""1"" when sumOfSquaredWeights becomes so close to zero it can only be represented as 0.0f ?

Float.MAX_VALUE seems like a better choice then 1, but I haven't really thought through wether or not that will still trigger NaN scores.","01/Jun/07 14:22;yseeley@gmail.com;From a scoring perspective, it's a multiplier that's only used at the top level, so the scores produced will be zero anyway.  How else might Similarity.queryNorm() be used?

If the user uses a boost of zero, It doesn't make sense to try and normalize that upwards though.  Returning 0 could also make sense.

","01/Jun/07 16:37;michaelbusch;> Float.MAX_VALUE seems like a better choice then 1, but I haven't really 
> thought through wether or not that will still trigger NaN scores.

Float.MAX_VALUE * 0.0f = 0.0f. It would not result in NaN.","01/Jun/07 17:47;cutting;Float.MAX_VALUE seems like a better choice then 1 [ ... ]

With nested queries, could Float.MAX_VALUE trigger overflow back to Infinity?
","01/Jun/07 17:52;hossman;> From a scoring perspective, it's a multiplier that's only used at the top level, so the scores produced will be zero anyway. 

huh?  queryNorm is passed down by BooleanWeight to all of the sub clauses ... am i miss understanding something? ","01/Jun/07 19:09;yseeley@gmail.com;> huh? queryNorm is passed down by BooleanWeight to all of the sub clauses ... am i miss understanding something?

Sorry, I meant Similarity.queryNorm() is only called at the top level.

> With nested queries, could Float.MAX_VALUE trigger overflow back to Infinity? 

I don't think so... BooleanWeight multiplies the queryNorm by it's boost before calling normalize on the nested weights (hence if the top-level boost is zero, it will call normalize(0.0f) on the subweights).
SpanWeight doesn't seem to do nested normalization at all.
Not sure if all the explain implementations would handle it correctly though.","25/Jan/11 13:22;rcmuir;This one is fixed (there is a Nan/Inf check in queryNorm added fairly recently)
","30/Mar/11 15:50;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some house cleaning in addIndexes*,LUCENE-2455,12464201,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,shaie,shaie,shaie,11/May/10 03:59,30/Mar/11 15:50,30/Sep/19 08:38,27/May/10 15:37,,,,,,,,,,,3.1,4.0-ALPHA,,,core/index,,,0,,,,"Today, the use of addIndexes and addIndexesNoOptimize is confusing - 
especially on when to invoke each. Also, addIndexes calls optimize() in 
the beginning, but only on the target index. It also includes the 
following jdoc statement, which from how I understand the code, is 
wrong: _After this completes, the index is optimized._ -- optimize() is 
called in the beginning and not in the end. 

On the other hand, addIndexesNoOptimize does not call optimize(), and 
relies on the MergeScheduler and MergePolicy to handle the merges. 

After a short discussion about that on the list (Thanks Mike for the 
clarifications!) I understand that there are really two core differences 
between the two: 
* addIndexes supports IndexReader extensions
* addIndexesNoOptimize performs better

This issue proposes the following:
# Clear up the documentation of each, spelling out the pros/cons of 
  calling them clearly in the javadocs.
# Rename addIndexesNoOptimize to addIndexes
# Remove optimize() call from addIndexes(IndexReader...)
# Document that clearly in both, w/ a recommendation to call optimize() 
  before on any of the Directories/Indexes if it's a concern. 

That way, we maintain all the flexibility in the API - 
addIndexes(IndexReader...) allows for using IR extensions, 
addIndexes(Directory...) is considered more efficient, by allowing the 
merges to happen concurrently (depending on MS) and also factors in the 
MP. So unless you have an IR extension, addDirectories is really the one 
you should be using. And you have the freedom to call optimize() before 
each if you care about it, or don't if you don't care. Either way, 
incurring the cost of optimize() is entirely in the user's hands. 

BTW, addIndexes(IndexReader...) does not use neither the MergeScheduler 
nor MergePolicy, but rather call SegmentMerger directly. This might be 
another place for improvement. I'll look into it, and if it's not too 
complicated, I may cover it by this issue as well. If you have any hints 
that can give me a good head start on that, please don't be shy :). ",,,,,,,,,,,,,,,,"25/May/10 16:14;shaie;LUCENE-2455_3x.patch;https://issues.apache.org/jira/secure/attachment/12445469/LUCENE-2455_3x.patch","25/May/10 06:43;shaie;LUCENE-2455_3x.patch;https://issues.apache.org/jira/secure/attachment/12445427/LUCENE-2455_3x.patch","23/May/10 11:16;shaie;LUCENE-2455_3x.patch;https://issues.apache.org/jira/secure/attachment/12445252/LUCENE-2455_3x.patch","22/May/10 06:14;shaie;LUCENE-2455_3x.patch;https://issues.apache.org/jira/secure/attachment/12445232/LUCENE-2455_3x.patch","14/May/10 13:43;shaie;LUCENE-2455_3x.patch;https://issues.apache.org/jira/secure/attachment/12444493/LUCENE-2455_3x.patch","27/May/10 07:45;shaie;LUCENE-2455_trunk.patch;https://issues.apache.org/jira/secure/attachment/12445628/LUCENE-2455_trunk.patch","27/May/10 09:43;uschindler;index.31.cfs.zip;https://issues.apache.org/jira/secure/attachment/12445632/index.31.cfs.zip","27/May/10 09:43;uschindler;index.31.nocfs.zip;https://issues.apache.org/jira/secure/attachment/12445633/index.31.nocfs.zip",,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2010-05-11 10:04:54.98,,,false,,,,,,,,,,,,,,,11364,,,Wed Mar 30 15:50:31 UTC 2011,New,Patch Available,,,,,,"0|i04qhb:",25555,,,,,,,,,"11/May/10 10:04;mikemccand;bq. Remove optimize() call from addIndexes(IndexReader...)

This still makes me nervous.  Yeah it's bad that this method does optimize() now.  But if we remove it, it's bad that this method can attempt to do a ridiculously immense merge, since it [naively] just stuffs everything and and does one merge.  Ie, both at are bad.

Maybe... we could do this: only merge the the incoming IndexReaders, appending a new segment to the end of the index?  Ie do no merging whatsoever of the current segments in the index.

Yes, this can result in ""unbalanced"" segments (ie, a huge segment appears after the long tail of level 0 segments), but, the merge policy can handle this -- it'll work out whatever merges are then necessary to get this segment onto the level that roughly matches its size. 

bq. So unless you have an IR extension, addDirectories is really the one  you should be using.

You mean addIndexes(Directory..)?

{quote}
BTW, addIndexes(IndexReader...) does not use neither the MergeScheduler 
nor MergePolicy, but rather call SegmentMerger directly. This might be 
another place for improvement. I'll look into it, and if it's not too 
complicated, I may cover it by this issue as well. If you have any hints 
that can give me a good head start on that, please don't be shy .
{quote}

This would be best of all :)  But it's tricky, because our MP/MS assume they are working w/ a SegmentInfo.  But, maybe it could somehow be made to work -- eg IR does give us maxDoc, numDocs (so we can know del doc count).  But eg LogByteSizeMergePolicy goes and computes total byte size of the segment (via SegmentInfo) which we cannot do from an IR.

","11/May/10 18:18;shaie;bq. You mean addIndexes(Directory..)?

Yes, copy-paste error.

bq. Maybe... we could do this: only merge the the incoming IndexReaders, appending a new segment to the end of the index?

I like it. IMO, that's what the method should do anyway, for better performance and service to the users. If I'm adding indexes, that doesn't mean I want a whole merge process to kick off. If I want that, I can call maybeMerge or optimize afterwards.

Basically, what I would like to add (and I'm not sure it belongs to this issue) is a ""super fast"" addIndexes method, something like registerIndexes, which doesn't even traverses the posting lists, removes deleted docs etc. - simply registering the new segments in the Directory. If needed - do a bulk copy of the files and update segments*. Simple as that. Maybe it does fit in that issue, as part of the general ""house cleaning""?

I will look more closely into supporting MP + MS w/ addIndexes(readers). Can't promise anything as I learn the code as I go :).","11/May/10 20:01;mikemccand;I agree, addIndexes should be minimal in the work it does...

But bulk copy of the files isn't really possible for addIndexes(IR...) in general, since the readers can be arbitrary (eg FilterIndexReader).","12/May/10 03:35;shaie;Ok. But since addIndexes(IR) is for IR extensions only, I think the number of people tha will be limited by it is very low.

But, why wouldn't they be able to use the Directory... version of the method? Since it's a bulk copy, we don't need IR methods. Maybe just call dir.copyTo or something of that sort? The method will only be asked to copy files (in case they exist elsewhere). I was thinking of introducing just a Directoy version of such method.

Basically, if you use NoMP and call addIndexesNoOptimize today, you get half of what I want, as only resolveExternals will be called. What I want is for the resolveExternals to be even faster, plain and shallow ""resolution"".","12/May/10 09:31;mikemccand;bq. But, why wouldn't they be able to use the Directory... version of the method?

Adding indexes using FilterIndexReader is useful -- eg look @ how the multi-pass index splitter tool works.

bq. What I want is for the resolveExternals to be even faster, plain and shallow ""resolution"".

For addIndexes(Directory), assuming the codecs are identical (the ""write"" codec equals the codec used to write the external segment), and assuming the doc stores of the external segment are private to it, I think we should be able to do a straight file-level copy, but renaming the segment in the process?","12/May/10 10:57;shaie;bq. Adding indexes using FilterIndexReader is useful 

I'm not against that Mike. addIndexes should allow for both IndexReader and Directory. It's the registerIndexes (or whatever name we come up with) which should work with Directory only, and then, even if the app calls addIndexes with its own custom IR, it can still call registerIndexes w/ the Directory only, to do that fast copy/registration. Since no IR method will be involved in the process.

So let's not confuse the two - addIndexes will exist and work as they are today. registerIndexes will be a new one.

bq. assuming the codecs are identical (the ""write"" codec equals the codec used to write the external segment), and assuming the doc stores of the external segment are private to it

Right. Thanks for pointing that out, as it will become an important NOTE in the documentation. This method (registerIndexes) is definitely for advanced users, that have to know *exactly* what's in the foreign indexes. For example, I need this because I'm building several indexes on several nodes and then I want to add them to a central/master one. I know they don't have deletions, and each is already optimized. Therefore traversing the posting lists (as fast as it would be) is completely unnecessary.

bq. but renaming the segment in the process?

Sure! I think we should really 'register' them in the Directory, as if they are the newly flushed segments. I'm sure you have a general idea on how this can be done? Assuming through SegmentInfos or something?","14/May/10 13:22;shaie;While changing addIndexes(reader), I've noticed it first obtains read lock and then calls startTransaction(true). In between it calls flush + optimize, which I've removed (as we no longer want to do that). When I ran the tests, TestIndexWriter.testAddIndexesWithThreads failed on the assert in startTransaction about numDocsInRam != 0. That's expected as I no longer call flush. The failure does not occur always.

In addIndexes(Dir) flush is called before startTransaction. But it makes sense to do it there, as the local segments are also merged. In the new addIndexes(reader) they won't and so I wonder if:
* I shouldn't call startTransaction at all, or
* I should, but also call flush before?","14/May/10 13:43;shaie;Patch handles the changes for 3x:
* addIndexesNoOptimize deprecated - new addIndexes(Directory...) instead
* CHANGES updates
* addIndexes does not do optimize before
* TestAddIndexesNoOptimize renamed to TestAddIndexes
* Changed calls to addIndexesNoOpt to use addIndexes(Dir...) (except for backwards tests)
* Changed textual references to addIndexesNoOpt.

You should ""svn mv lucene/src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java lucene/src/test/org/apache/lucene/index/TestAddIndexes.java"" before you apply the patch.

The patch is much smaller than it looks - it's the rename of TestAddIndexesNoOpt that takes a lot of space.

As I've mentioned before, I'm not sure about addIndexes calling startTransaction w/o flushing. Even though the tests pass, this seems wrong. So I'd appreciate a review.","14/May/10 15:10;mikemccand;I think you should still call flush, and still start/commitTransaction -- addIndexes is ""all or nothing"", which is why we have those transaction methods.  Ie, on exception, the rollbackTransaction puts the original segments back.","14/May/10 15:13;mikemccand;Patch looks good Shai!  Only a small typo in CHANGES (unles -> unless).","14/May/10 16:39;shaie;I see. I understand why it's called in addIndexes(Dir), because the local segments are also touched. But now in the Reader version, they aren't. So it looked odd to me that we flush whatever is in RAM. I think you said once that addIndexes should have done the merge outside, adding the new segment when it's done?

But if you think that flushing the RAM, even though its content is touched, is ok then I'll change it.","14/May/10 17:07;mikemccand;bq.  I understand why it's called in addIndexes(Dir), because the local segments are also touched. But now in the Reader version, they aren't. So it looked odd to me that we flush whatever is in RAM. 

Yeah maybe we should no longer flush (but still call start/commitTransaction).  I think there may've been a reason to flush first (besides that we were also merging local segments)... but I can't remember it.  If you comment out that assert (and the corresponding assert for deletions) do any tests fail?

bq. I think you said once that addIndexes should have done the merge outside, adding the new segment when it's done?

Yes, I would love to fix this -- it'd mean we would not need the start/commit/rollbackTransaction code.

Ie, we play a dangerous game now, where addIndexes is allowed to muck with the in-memory SegmentInfos before it's complete.  It'd be better if all merging happened outside of its SegmentInfos, and only when addIndexes finished, it'd atomically commit to SegmentInfos.

This would then allow commit() to run immediately, not having to wait for any running addIndexes to finish first.  And we would not need to block add/updateDocument nor deleteDocuments while addIndexes is running.

So, actually, I think in addIndexes(IR...) you should not use the transaction logic at all?  Just do the merge externally & commit in the end?  (And try not flushing as well.).","14/May/10 17:25;shaie;Ha :), I knew this will get more complicated and interesting ... So basically, it feels to me that if we'd have registerIndexes, we could in addIndexes merge outside IW and then call register?

So far, tests pass w/ startTransaction. But that test is multi-threaded so it may be a concurrency issue. I'll try to do the addIndexes outside IW and then commit the new segment. If that will be straightforward, then I think I'll understand better how to develop registerIndexes.","14/May/10 17:33;mikemccand;bq. Ha , I knew this will get more complicated and interesting ..

It always does!

bq.  So basically, it feels to me that if we'd have registerIndexes, we could in addIndexes merge outside IW and then call register?

Hmm but register will be an external API for copying over segments in a foreign directory, right?  (And segment must be renamed).

Vs these segments which will be in our directory already, with the right segment name, and just need to be committed to the segmentInfos?

bq. So far, tests pass w/ startTransaction.

You mean w/o the flush?","15/May/10 03:52;shaie;bq. You mean w/o the flush?

Yes. The start/commit transaction looks like that:
{code}
startTransaction(false);

try {
  mergedName = newSegmentName();
  merger = new SegmentMerger(this, mergedName, null);

  for (IndexReader reader : readers)      // add new indexes
    merger.add(reader);
        
  int docCount = merger.merge();                // merge 'em
        
  synchronized(this) {
    info = new SegmentInfo(mergedName, docCount, directory, false, true,
                  -1, null, false, merger.hasProx());
    setDiagnostics(info, ""addIndexes(IndexReader...)"");
    segmentInfos.add(info);
  }
        
  // Notify DocumentsWriter that the flushed count just increased
  docWriter.updateFlushedDocCount(docCount);
        
  success = true;
} finally {
  if (!success) {
    rollbackTransaction();
  } else {
    commitTransaction();
  }
}
{code}

* A new segment name is generated
* All readers but the current one are merged
* The new SI is added to the writer's SIs
* DocWriter's updateFlushedDocCount is updated
* The transaction is committed or rolled back if there was an error.

So this looks like it already does the merge ""on the side"" and when it's done the new segment is registered?","15/May/10 04:12;shaie;In fact, I've create newAddIndexes (just for the review) which works like that:
{code}
  public void newAddIndexes(IndexReader... readers) throws CorruptIndexException, IOException {

    ensureOpen();

    try {
      String mergedName = newSegmentName();
      SegmentMerger merger = new SegmentMerger(this, mergedName, null);
      
      for (IndexReader reader : readers)      // add new indexes
        merger.add(reader);
      
      int docCount = merger.merge();                // merge 'em
      
      SegmentInfo info = null;
      synchronized(this) {
        info = new SegmentInfo(mergedName, docCount, directory, false, true,
            -1, null, false, merger.hasProx());
        setDiagnostics(info, ""addIndexes(IndexReader...)"");
        segmentInfos.add(info);
      }
      
      // Notify DocumentsWriter that the flushed count just increased
      docWriter.updateFlushedDocCount(docCount);
      
      // Now create the compound file if needed
      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {

        List<String> files = null;

        synchronized(this) {
          // Must incRef our files so that if another thread
          // is running merge/optimize, it doesn't delete our
          // segment's files before we have a chance to
          // finish making the compound file.
          if (segmentInfos.contains(info)) {
            files = info.files();
            deleter.incRef(files);
          }
        }

        if (files != null) {
          try {
            merger.createCompoundFile(mergedName + "".cfs"");
            synchronized(this) {
              info.setUseCompoundFile(true);
            }
          } finally {
            deleter.decRef(files);
          }
        }
      }
    } catch (OutOfMemoryError oom) {
      handleOOM(oom, ""addIndexes(IndexReader...)"");
    } finally {
      if (docWriter != null) {
        docWriter.resumeAllThreads();
      }
    }
  }
{code}

Question: if we've just added the new SI to segmentInfos, why do we sync on _this_ and check if it exists (when we create the compound file)? Is it because there could be a running merge which will merge it into a new segment before we reach that point?

What do you think? Is that what you had in mind about merging on the side and committing in the end?","15/May/10 12:51;mikemccand;bq. Question: if we've just added the new SI to segmentInfos, why do we sync on this and check if it exists (when we create the compound file)? Is it because there could be a running merge which will merge it into a new segment before we reach that point?

Yes, exactly.

bq. What do you think? Is that what you had in mind about merging on the side and committing in the end?

Yup!  This looks great.... though I think you should move the docWriter.updateFlushedDocCount into the sync above it?  We didn't have to do this before because we blocked all add/updateDocument calls.

Also, you shouldn't call docWriter.resumeAllThreads (you didn't pause them).

So this change is a great step forward in concurrency of addIndexes(IndexReader...)!","15/May/10 13:07;shaie;bq. Also, you shouldn't call docWriter.resumeAllThreads (you didn't pause them).

Oops, missed that :). Thanks !

I'll replace addIndexes w/ this code and run tests to check how it flies.","16/May/10 04:23;shaie;I've looked into implementing registerIndexes, and that's the approach I'd like to take:
* For each incoming Directory, read its SegmentInfos
* For each SegmentInfo:
** Generate a new segment name
** List its files
** Copy them from incoming Dir to local Dir, w/ the new segment name
** Add such SI to the local IW segmentInfos
** Update DW docCount, like it's done in the addIndexes* methods.

Few things:
# Does that sound reasonable? Am I missing something?
# Directory exposes a copyTo(Dir, Collection) which I thought to use. But the files are copied to the target Dir w/ their current name - while I need to copy them over w/ their new name.
#* Adding rename to Dir feels wrong and dangerous to me
#* Adding copyFile(Dir, String old, String new) seems ok
#* Adding a variant of copyTo which accepts a Collection of the new names - the src and new should align. This also seems ok to me.

I'd like to use Directory for the copy, since impls of Dir may do the copy very efficiently (i.e. FSDir vs. RAMDir) and I don't want to use IndexInput/Output for that.

Do you know of another way I can achieve that? I only want to copy the actual segment files, w/o .gen and segments_N, so calling SI.files() seems ok?

Another question that popped into my head was about consistency of the incoming Dirs vs. the local one, w.r.t. to CFS files - should I worry about that? I think not because today one can create an index w/ CFS and then turn it off and some segments will be compound and others not?","16/May/10 18:03;mikemccand;bq. I've looked into implementing registerIndexes, and that's the approach I'd like to take:

This looks good.

Though if the src segments share docStores, you can't do a simple
copy (I think you have to fallback to the resolveExternalSegments
approach for such segments).

bq. Does that sound reasonable? Am I missing something?

I think this should work!

If the src segments are an older index rev, I think you are still OK.
They will just remain ""old"" on copy, and merge will eventually migrate
them forward.

For trunk... you should note in the jdocs that no codec conversion
takes place.  So the CodecProvider used in IW (and later used to read
this index) must know how to provide the codec used by the src
segments.

{quote}
Directory exposes a copyTo(Dir, Collection) which I thought to use. But the files are copied to the target Dir w/ their current name - while I need to copy them over w/ their new name.
Adding rename to Dir feels wrong and dangerous to me
Adding copyFile(Dir, String old, String new) seems ok
Adding a variant of copyTo which accepts a Collection of the new names - the src and new should align. This also seems ok to me.
I'd like to use Directory for the copy, since impls of Dir may do the copy very efficiently (i.e. FSDir vs. RAMDir) and I don't want to use IndexInput/Output for that.

Do you know of another way I can achieve that? I only want to copy the actual segment files, w/o .gen and segments_N, so calling SI.files() seems ok?
{quote}

SI.files() should be fine.

I think falling back to copyFile is best?  Then copyTo could use it.

{quote}
Another question that popped into my head was about consistency of the
incoming Dirs vs. the local one, w.r.t. to CFS files - should I worry
about that? I think not because today one can create an index w/ CFS
and then turn it off and some segments will be compound and others
not?
{quote}
I think that's fine, but we should advertise in the jdocs.
","16/May/10 19:55;shaie;So it sounds like addIndexes should really be that registerIndexes. Specifically it should do a quick and dirty copy of all segments that don't share doc stores, and then resolveExternals those that do? Maybe we can get rid of those transactions and not block add/update/delete/commit/addIndexes attempts anymore?

Usually, I expect this to be a win-win. In cases where you add Directories w/ plenty of segments that share doc stores it will be slower, because we won't utilize MP and MS. But this can be improved in the future as well by e.g. just taking care of the shared doc stores, and don't remove deleted document entries etc.

But .. it will prevent (in some cases) the use of PayloadProcessorProvider ... hmm. So it seems we do need a separate registerIndexes for the really quick & dirty addIndexes operation.

BTW, Directory.copyTo* should be replaced w/ Directory.copy(Dir, File, File), for a couple of reasons:
* There's no reason to believe the dest file should be named the same as the source file
* The method is not entirely safe - only jdocs protect the user from doing really stupid thing such as overwriting the segments* files.
* I don't see a proper usecase for that method, other than copying a Directory into an empty one. At least, other use cases are very dangerous.

Instead, the user should do this logic outside - call dir.listAll(), w/ and w/o FilenameFilter and copy the files of interest, and be allowed to rename them in the process.","17/May/10 06:51;shaie;So ... after I slept over it, I don't think I can easily let go of the so-near victory -- having addIndexes not blocking anything, done on the side, be as efficient as possible and give up a chance to do some serious code cleanup :). So I'd like to propose the following, we can discuss them one by one, but they all aim at the same thing:
# addIndexes(Directory ...) will be a quick & dirty copy of segments. No deleted docs removal in the process, no merges.
#* It's clear how this can be done for segments that don't share doc stores.
#* What isn't clear to me is why can't this work for segments that do share doc stores - can't we copy all of them at once, and add to segmentInfos when the segments + their doc store were copied? So, if I have 5 segments, w/ the following doc stores: 1-2, 3 and 4-5 (3 stores for 5 segments), can't I copy 1+2+store, 3, 4+5+store? Wouldn't that work? I'll give it a shot.
# PayloadProcessorProvider won't be used in the process. If you want, you can set it on the source writer, and call optimize(). Performance-wise it will be nearly identical - instead of processing the postings + payloads during addIndexes, you'll do that during optimize() (all segments will be processed anyway) and addIndexes will do a bulk IO copy, which on most modern machines is really cheap.
#* Further more, you will end up w/ just one segment, which means it can be copied at once for sure.
#* It will also simplify PPP -- no need for DirPP anymore. PPP would get a Term and return a PP for that term, as it will always work on just one Directory.
#* People can still use it with the target IW if they want, but not through addIndexes.
# Apps can call maybeMerge or optimize() following addIndexes, if they want to.

What remains is addIndexes(IndexReader...) and I'm not sure why this cannot be removed. In the back of my head I remember a discussion about it once, so I guess there is a good reason. But at least from what I see now, and armed w/ my use cases only, it seems like even if you use an extension of IndexReader you should still be able to do a bulk copy? Hmm ... unless if your extension assumes different postings structure or something like that, which the regular SegmentReader won't know about -- then during addIndexes those postings are converted.

But, how common is this? And wouldn't it be better if such indexes are migrated beforehand? I mean, anyway after addIndexes those postings won't retain the custom IndexReader-assuming format? Or is there another reason?

If we go with that, then SegmentMerger can be simplified as well, assuming only SegmentReader?

What do you think?","17/May/10 10:05;mikemccand;bq. What isn't clear to me is why can't this work for segments that do share doc stores 

You are right!

If we copy over the doc stores, also renaming them, and fixup the incoming SegmentInto to reference the newly named one, this should work fine!

bq. PayloadProcessorProvider won't be used in the process.

This (and not needing DirPP anymore) is a great simplification.

bq. What remains is addIndexes(IndexReader...) and I'm not sure why this cannot be removed.

I think we still need it... look at how multi-pass index splitter (contrib/misc) works.","17/May/10 11:06;shaie;bq. look at how multi-pass index splitter (contrib/misc) works.

I see ...

I think this can be achieved by also deleting docs that fall outside the split range and calling optimize() / expungeDeletes()? So, you copy the index once (using one of the copying addIndexes methods), delete the docs that you don't care about and optimize/expunge. Then copy the index again and repeat the process, w/ a different range of ids. In fact, that's more or less what the method does - only it calls addIndexes, to copy into an existing/empty Directory.

So I think it can be changed to not call addIndexes? Only problem is that now the method adds the docs into the target Directory directly, while in the other solution it will need to create a Directory on the side w/ the range of requested IDs and then copy that one into the target Dir?

But I'm not sure that's worth it to have addIndexes(IndexReader...) and the relevant code in SM which handles the non-SegmentReader readers? Of course, this is just one scenario, but if that's our justifying case, then I'm not sure about how justifying it is.","17/May/10 11:33;ab;FYI, I'm working on a different version of IndexSplitter that uses the logic in SegmentMerger directly, without going through IW.addIndexes(FilterIndexReader).

However, there are other applications for which this API is crucial, e.g. LUCENE-1812 or IndexSorter (in Nutch) - in short, any client apps that want to merge-in index data that does not correspond 1:1 to a Directory. For this reason I think the pair of IndexWriter.addIndexes(IndexReader...) and FilterIndexReader abstraction is extremely useful and that IndexWriter.addIndexes(Directory...) is not a sufficient replacement.

(edit: unless there is a better user-level API based on the flex producers/consumers...)","17/May/10 12:11;shaie;bq. any client apps that want to merge-in index data that does not correspond 1:1 to a Directory

I understand that. But when you call addIndexes w/ such IndexReaders, all they do is read the postings. Those are written down using the logic of the target IndexWriter. So I wonder how important is it for addIndexes to be in place, rather than say rewriting those indexes before they are added? I mean, all that addIndexes will do is call SegmentMerger and iterate on the readers and segments, merging the posting lists ...

I don't object to that API. But, SM is used extensively, and is more of a main-path code, while addIndexes(IndexReader) is something only few out there use. Yet it affects everyone else who reads SM code, as well as those of us who are confused about which method to call (Reader or Directory) ... It almost feels like such operation - the relevant code from SM which handles non-SegmentReaders, should be extracted to a utility or something. But if I'm the only one that's bothered by it, then so be it. I can take care of the rest now, and resolve that one later.","17/May/10 12:26;ab;I understand - see the edited section in my comment: I think that extracting this non-SR code would be great. I would be in fact glad if there was an easier to control API that allows us to directly stream-process postings / stored / tvf-s / etc. in a way that results in a functioning index. Take for example LUCENE-1812 - the only reason it uses addIndexes(IndexReader) is that there was no easy way to modify postings in a way that would still result in a valid index, and there was no other API to add artificially created postings (i.e. not coming from a Directory) to a target index.","17/May/10 13:23;shaie;So can't the PrunningReader run on the side, converting the postings to whatever they're supposed to look like in the index they are about to be added to, and then call addIndexes w/ the Directory to do the bulk copy? I mean, instead of looking for a standalone tool, perhaps this can be solved on a case by case basis? Of course, if this can be made generic enough, then we can add it as a core utility, or IW method.","17/May/10 13:45;ab;bq. So can't the PrunningReader run on the side, converting the postings to whatever they're supposed to look like 

Erhm ... Currently the only way in the user API to write out existing postings (no matter how created) is to use IndexWriter.addIndexes(IndexReader). We can read postings just fine, using various *Enum classes that we can obtain from IndexReader, but there are no comparable high-level output methods  - Codecs and other flex classes are IMHO too low-level.

Also, with large indexes the amount of IO/CPU for writing out a Directory and reopening it is non-trivial - it's much more efficient to do this via streaming from the original, already open index.

Also, if we remove this method, then FilterIndexReader may as well go too, because it loses its utility.","17/May/10 17:01;shaie;Ok let's keep addIndexes(IndexReader) around. This means though that we cannot simplify the PPP API. We'll still need DirPP.","20/May/10 11:07;shaie;I've started to implement addIndexes(Directory...) as agreed - copy files from the incoming ones into the local directory, while renaming them on the fly. This works really well with non-CFS segments: a new segment name is generated, the incoming files are renamed and this all flies smoothly (didn't test w/ deletions yet) - even shared doc stores work great.

But with CFS it doesn't work well because CFS writes the file names in the CFS file itself, and so even if the segment is renamed to _5 (for example), the names that are written in the file are _2.* (for example), and openInput fails to locate them. To overcome this, I propose we do the following:

* Introduce on IndexFileNames a stripName method (3x and trunk) - will return the file name w/o the _x part.
* CFR ctor - strip names of read file names by calling IFN.stripName --> 3x only
* CFR.openInput - strip name by calling IFN.stripName --> 3x and trunk
* Document that files should be created through IFN only --> 3x (for clarity) and trunk (otherwise may not be supported).
* Not save the name in CFS --> trunk only. Will remove the need to strip it off when it's read.

That will ensure that files are named following a certain convention which we can rely on in CFR. I don't think it's too hard to ask for. CFS itself already knows the name - it's named like it. So there's no value in storing the names of the files it holds.

For 3x it should work well b/c we don't allow for custom index files. For trunk we'll ask to go through IFN to name files - so one can create mycustom.file through IFN which will be called _x_mycustom.file.

What do you think?","20/May/10 12:15;mikemccand;Ahh sneaky that CFS still embeds the old segment's name (you're right).  The only other option would be to rewrite the CFS header, but then that's not easy to do a bulk copy on.  So I like you're approach!

We should  document in Codec.java that this (you must gen your filename via IFN's APIs) is a requirement of any custom files your codec wants to store in the index.","22/May/10 06:14;shaie;Patch includes the following:
* addIndexesNoOpt renamed to addIndexes2 for now, until we resolve the failing test (see below). I'll remove it and fix jdocs accordingly afterwards.
* addIndexes(Dir...) implements the simple file copy strategy.
* Tests updated accordingly.
* Some minor changes to CompoundFileReader and IndexFileNames, as discussed before.

All tests pass except for TestIndexWriter.testAddIndexesWithThreads. I've debugged it, but cannot find the reason. addIndexes copies all segments, before it adds them to the writer's segmentInfos. Maybe I need to use start/commit transaction on that part only, to lock all ops? I don't see why, but maybe?

Also, TestAddIndexes.testWithPendingDeletes2() (and some others) fail before I added a call to flush to addIndexes. It seems that w/o it, existing buffered deleted docs are ignored after addIndexes returns (even when no multi-threading is involved). Can someone please confirm that?

Also, I cannot simplify PPP (to remove DirPP) because we kept addIndexes(Reader...). It's an annoyance if you don't call this method (need to return a DirPP for the target Dir always - if you want to use it), but maybe not so bad ...","23/May/10 11:16;shaie;Attached patch includes:
* Fixes a bug that caused some tests to fail.
* CFS is now versioned:
** CFW writes a version header, and CFR reads it
** CFW strips the segment name from the filename before writing it
** CFR back-supports pre-3.1 indexes depending on the existence/absence of the version header.
* TestBackwardsCompatibility now covers 3.0 indexes as well, and addIndexes* ops.

The beauty of all this is that IndexWriter no longer needs those transactions, and is now 500 lines of code + jdoc down !

After we've iterated through this patch, I'll do the same changes on trunk. Backwards support should be much easier there, because we will provide an index migration tool anyway, and so CFW/CFR can always assume they're reading the latest version (at least in 4.0). CFW should probably use CodecUtils in trunk - it cannot be used in 3x because of how CFW works today - writing a VInt first, while CodecUtils assumes an Int. And I don't think it's healthy to do so much changes on 3x.","24/May/10 09:56;mikemccand;Patch looks great!  So awesome seeing all the -'s in IW.java!!  Keep it up :)

And it's great that you added 3.0 back compat case to
TestBackwardsCompatibility...

Some feedback:

  * Can you change the code to read to a ""int firstInt"" instead of
    version?  And make an explicit version (say ""PRE_VERSION""), and
    then check if version is PRE_VERSION in the code.  Ie, any tests
    against version (eg version > 0) should be against constants
    (version == PRE_VEFRSION) not against 0.

  * CFW's comment should be ""make it 1 lower"" than the current one
    right?  Ie, -2 is the next version?
","24/May/10 09:59;mikemccand;bq. Backwards support should be much easier there, because we will provide an index migration tool anyway, and so CFW/CFR can always assume they're reading the latest version (at least in 4.0).

Hmm I think we should do live migration for this (ie don't require a
migration tool to fix your index)?  This is trivial to do on the fly
right (ie as you've done in 3.x).

bq. CFW should probably use CodecUtils in trunk - it cannot be used in 3x because of how CFW works today - writing a VInt first, while CodecUtils assumes an Int. And I don't think it's healthy to do so much changes on 3x.

Hmm yeah because of the live migration I think CodecUtils is not
actually a fit here (trunk or 3x).
","24/May/10 16:31;shaie;I'm not sure about the live migration, Mike. First because all the problems I've mentioned about CodecUtils in 3x will apply to live migration of 3.x indexes in 4.0 code. Second, if everyone who upgrades to 4.0 will need to run the migration tool, then why do any work in supporting online migration? What's the benefit? Do u think of a case where someone upgrades to 4.0 w/o migrating his indexes (unless he reindexes of course, in which case there is no problem)?

I just think it's weird that we support online migration together w/ a migration tool. If we migrate the indexes w/ the tool to include the new format of CFS, then the online migration code won't ever run, right? And not doing this in the tool seems just a waste? I mean the user already migrates his indexes, so why incur the cost of an additional online migration?","24/May/10 17:27;mikemccand;Sorry -- for each major release, I think it'll be either live
migration or offline migration, but not both.

So far for 4.0 we haven't had a major enough structural change to the
index format, that'd make live migration too hard/risky, so, so far I
think we can offer live migration for 4.0.

The biggest change was flex, but it has the preflex codec to read (not
write) the pre-4.0 format... so, so far I think we can still offer
live migration for 4.0?","24/May/10 17:44;shaie;Ahh, I knew we must be talking past each other :). I assumed that the flex changes will go under the migration tool. If we have live migration for it, then I agree we should do live migration here.

With that behind us, did someone start an API migration guide? Since I remove addIndexesnoOptimize in favor of the new addIndexes, I wanted to document it somewhere. It's a tiny change, so perhaps it can go other the API Changes in CHANGES?","24/May/10 17:54;mikemccand;bq. With that behind us, did someone start an API migration guide?

Not yet, I think?  Go for it!","24/May/10 18:24;shaie;I will document it in CHANGES under API section. I think the migration guide format will need its own discussion, and I don't want to block that issue. When we've agreed on the format (people have made few suggestions), I don't mind helping w/ porting everything relevant from changes to that guide.","25/May/10 06:41;shaie;bq. CFW's comment should be ""make it 1 lower""

Right ! I copied it from FieldsWriter where the versions are kept as positive ints. Will post a patch shortly.","25/May/10 06:43;shaie;Patch applies Mike's comments. I think this is ready to go in. I'd like to commit to 3x before trunk, because there are lots of changes here.","25/May/10 09:52;mikemccand;Could you fix ""firstInt' to have a very short life?

Meaning, you read firstInt, and very quickly use that to assign to version & count, and no longer use it again.  Ie, all subsequent checks when loading should be against version, not firstInt...

Also, can you maybe rename CFW.PRE_VERSION -> CFW.FORMAT_PRE_VERSION?  (to match the other FORMAT_X).

Otherwise looks great!","25/May/10 12:17;shaie;The only place I see firstInt is used perhaps unnecessarily is in the for-loop. So I've changed the code to look like this:

{code}
int count, version;
if (firstInt < CompoundFileWriter.FORMAT_PRE_VERSION) {
  count = stream.readVInt();
  version = firstInt;
} else {
  count = firstInt;
  version = CompoundFileWriter.FORMAT_PRE_VERSION;
}
{code}

And then I query for version == CompoundFileWriter.FORMAT_PRE_VERSION inside the for-loop. Is that what you meant?

There is a check before all that ensuring that read firstInt does not indicate an index corruption -- that should remain as-is, right?","25/May/10 16:14;shaie;Update w/ comments. I plan to commit this either later today or tomorrow (and then port it to trunk). So if you haven't done so and want a last chance review - that's your chance.","25/May/10 16:30;mikemccand;Patch looks good Shai!  Thanks.","26/May/10 11:11;shaie;Committed revision 948394 (3x).

Will now port everything to trunk","26/May/10 11:30;uschindler;Hi Shai,

I have seen this only lately. You added a 3.0 Index ZIP to the tests. This conflicts a little bit with trunk, where a 3.0 Index ZIP is already available. I would prefer to keep the ""older version"" ZIPs equal against each release, so it would be fine, if the trunk-added numerics backwards test could also be in 3.x branch. Would this be possible? You have to just merge the code.

Also it looks strange that the 3.0 backwards tests now contain also 3.0 index ZIPs, but there is no code for that??? Why have you added this to backwards? The 3.0 backwards tests should only modify this one addindexes test, but not add the zips. Maybe simple delete, they are not used.

By the way the 3.0 index zip file generation code is in the 3.0 branch, have you edited it there? You should commit the code there so one is able to regenerate the 3.0 ZIPs from the stable 3.0.x branch.","26/May/10 11:35;uschindler;I looked at the code, it simply tests trhat old indexes can be added. Maybe you just copy the trunk ZIPs for 3.0 to the 3x branch to keep them consistent. The files dont seem to be equal.","26/May/10 12:10;shaie;Ok I added the indexes from trunk (didn't know they were there). I've changed CFS to write a version header in the file, so that's why I've added a 3.0 index - to make sure it can be read properly by 3.1. What I've added to TestBackwardsCompatibility are tests to ensure that addIndexes work on old indexes (which was good, because after the changes they weren't !).

bq. Maybe simple delete, they are not used.

The testAddIndexes were just added, and the 30 indexes are used. So I cannot delete them (see my comment above)

bq. By the way the 3.0 index zip file generation code is in the 3.0 branch, have you edited it there?

Nope, it exists in TestBackwardsCompatibility as commented out, w/ instructions to uncomment. I've used that code.","26/May/10 12:11;shaie;While porting the code to trunk, I've noticed that acquireRead/Write, releaseRead/Write, upgradeReadToWrite are either not called anymore, or called in relation to addIndexes. So I think these can be safely removed as well (from 3x and trunk)?","26/May/10 12:34;mikemccand;bq.  So I think these can be safely removed as well (from 3x and trunk)?

I think so!","26/May/10 12:54;shaie;Committed revision 948415 (copied the 3.0 indexes from trunk) and removed more unnecessary code from IndexWriter.","27/May/10 07:45;shaie;Like the 3x patch, only this one changes IndexFileNames.segmentFileName to take another parameter for custom names, as well as update some jdocs to match flex (Codecs). I think this is ready to go in.","27/May/10 08:32;uschindler;Should we not add a 3.1 index (created with HEAD 3.x branch) to the TestBackwardsCompatibility? So we can verify that preflex indexes with new CFS header also work?","27/May/10 08:55;shaie;Yes! I'll add them and update the tests. Will post a patch after I get more comments","27/May/10 09:13;shaie;Hmm ... I've created the indexes using the 3x branch, copied them to trunk and updated TestBackwardsCompatibility to refer to them. All tests pass except for testNumericFields. It fails on both CFS and non-CFS indexes, and so I'm not sure it's related to this issue at all. The failure is this:

{code}
junit.framework.AssertionFailedError: wrong number of hits expected:<1> but was:<0>
	at org.apache.lucene.index.TestBackwardsCompatibility.testNumericFields(TestBackwardsCompatibility.java:773)
{code}

Can you try to run it on your checkout?","27/May/10 09:43;uschindler;For me it passes.

Are you sure that you used the *latest* checkout of 3x. I added the index generation code yesterday after your last 3x commit. This code was not merged to 3x from trunk, as it was postflex added. This is done sice yesterday:
{noformat}
Author: uschindler
Date: Wed May 26 13:13:10 2010
New Revision: 948420

URL: http://svn.apache.org/viewvc?rev=948420&view=rev
Log:
Merge the 3.0 index backwards tests from trunk (numeric field support). This makes it consistent across all branches.

Modified:
    lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/index/   (props changed)
    lucene/dev/branches/branch_3x/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java   (contents, props changed)
{noformat}

I attached the generated ZIP files from my 3x checkout.","27/May/10 11:23;shaie;Yes - after I updated my checkout and re-create the indexes, the test passes. So I will include them with this patch as well.","27/May/10 15:37;shaie;Committed revision 948861 (trunk).","30/Mar/11 15:50;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QueryUtils should check that equals properly handles null,LUCENE-1840,12433698,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,markrmiller@gmail.com,markrmiller@gmail.com,21/Aug/09 15:38,30/Mar/11 15:50,30/Sep/19 08:38,24/Sep/10 18:53,,,,,,,,,,,3.1,4.0-ALPHA,,,general/build,,,0,,,,"Its part of the equals contract, but many classes currently violate",,,,,,,,,,,,,,,,"24/Sep/10 02:32;rcmuir;LUCENE-1840.patch;https://issues.apache.org/jira/secure/attachment/12455459/LUCENE-1840.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-09-24 18:53:24.09,,,false,,,,,,,,,,,,,,,11923,,,Wed Mar 30 15:50:27 UTC 2011,New,,,,,,,"0|i04uhr:",26205,,,,,,,,,"24/Sep/10 18:53;rcmuir;Committed revision 1001006, 1001025 (3x).

Nice catch Mark.","30/Mar/11 15:50;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StringHelper#stringDifference is wrong about supplementary chars ,LUCENE-2643,12474076,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,simonw,simonw,simonw,14/Sep/10 14:00,30/Mar/11 15:50,30/Sep/19 08:38,08/Dec/10 22:32,3.0,3.0.1,3.0.2,,,,,,,,3.1,4.0-ALPHA,,,,,,0,,,,StringHelper#stringDifference does not take supplementary characters into account. Since this is not used internally at all we should think about removing it but I guess since it is not too complex we should just or fix it for bwcompat reasons. For released versions we should really fix it since folks might use it though. For trunk we could just drop it.,,,,,,,,,,,,,,,,"08/Dec/10 22:27;simonw;LUCENE-2643.patch;https://issues.apache.org/jira/secure/attachment/12465847/LUCENE-2643.patch","19/Oct/10 00:43;rcmuir;LUCENE-2643.patch;https://issues.apache.org/jira/secure/attachment/12457508/LUCENE-2643.patch","14/Sep/10 14:02;simonw;LUCENE-2643.patch;https://issues.apache.org/jira/secure/attachment/12454541/LUCENE-2643.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-09-14 14:04:20.846,,,false,,,,,,,,,,,,,,,11194,,,Wed Mar 30 15:50:26 UTC 2011,New,Patch Available,,,,,,"0|i04pbj:",25367,,,,,,,,,"14/Sep/10 14:02;simonw;here is a patch","14/Sep/10 14:04;rcmuir;My vote would be to drop it if we arent using it, its @lucene.internal.

since its unused, its not obvious that its ""wrong"" (its correct if you want the first code unit difference)
","14/Sep/10 14:07;simonw;bq. since its unused, its not obvious that its ""wrong"" (its correct if you want the first code unit difference)

yeah - my interpretation would be its wrong since you use Character.charAt(int) with the index of the first code unit. anyway - we should drop for trunk but I am not sure if we should for 3.x. I mean this is not that much of a deal anyway.","14/Sep/10 14:19;rcmuir;drop in trunk and mark deprecated in 3.x?

regardless of whether its right or wrong, if we arent using it, i think its good to clean house.
","14/Sep/10 19:00;simonw;bq. drop in trunk and mark deprecated in 3.x?
yeah that makes sense... shall we fix it in 3.x (fix means support  suppl. chars)? I don't have strong feelings about it. ","19/Oct/10 00:43;rcmuir;patch to deprecate (we should just remove from trunk though!)","19/Oct/10 06:29;simonw;bq. patch to deprecate (we should just remove from trunk though!)

+1 - I committed that in 1024128

thanks robert","19/Oct/10 06:55;simonw;I also merged into 3_x - should we backport to 3.0.x too?

simon","19/Oct/10 09:18;mikemccand;Why not drop it from 3.x/3.0.x as well?

It's an internal API so there's no need to deprecate it first?","19/Oct/10 13:02;rcmuir;+1 to at least removing from 4.0.","20/Oct/10 06:55;simonw;Lets drop it from 4.0 and 3.x and deprecate in the 3.0 branch so if we do a 3.0.3 release one day people get a heads up.

thoughts?","20/Oct/10 08:07;mikemccand;bq. Lets drop it from 4.0 and 3.x and deprecate in the 3.0 branch so if we do a 3.0.3 release one day people get a heads up.

I think doing anything in 3.0.x is optional here... above & beyond the call of duty ;)

And we should drop it in 3.x and 4.0.","29/Oct/10 12:48;rcmuir;moving out","08/Dec/10 21:38;simonw;bq. And we should drop it in 3.x and 4.0.
since this it gone from 4.0 I will remove from 3.x now too.","08/Dec/10 22:27;simonw;here is a patch against 3x - I will commit shortly","08/Dec/10 22:32;simonw;Committed revision 1043738.
","30/Mar/11 15:50;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.toString on empty MultiPhraseQuery hits NPE,LUCENE-2526,12468520,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,04/Jul/10 17:27,30/Mar/11 15:50,30/Sep/19 08:38,04/Jul/10 17:37,,,,,,,,,,,3.1,4.0-ALPHA,,,core/search,,,0,,,,"Ross Woolf hit this on java-user thread ""MultiPhraseQuery.toString() throws null pointer exception"".  It's still present on trunk...",,,,,,,,,,,,,,,,"04/Jul/10 17:29;mikemccand;LUCENE-2526.patch;https://issues.apache.org/jira/secure/attachment/12448651/LUCENE-2526.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-30 15:50:24.12,,,false,,,,,,,,,,,,,,,11301,,,Wed Mar 30 15:50:24 UTC 2011,New,,,,,,,"0|i04q1j:",25484,,,,,,,,,"04/Jul/10 17:29;mikemccand;Simple fix... I'll commit shortly.","30/Mar/11 15:50;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
changes-to-html: fixes and improvements,LUCENE-2517,12468046,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,sarowe,sarowe,28/Jun/10 12:05,30/Mar/11 15:50,30/Sep/19 08:38,05/Jul/10 00:40,,,,,,,,,,,3.1,4.0-ALPHA,,,,,,0,,,,"The [Lucene Hudson Changes.html|http://hudson.zones.apache.org/hudson/job/Lucene-trunk/lastSuccessfulBuild/artifact/lucene/build/docs/changes/Changes.html] looks bad because changes2html.pl doesn't properly handle some new usages in CHANGES.txt.",,,,,,,,,,,,,,,,"28/Jun/10 12:17;sarowe;LUCENE-2517-CHANGES.patch;https://issues.apache.org/jira/secure/attachment/12448209/LUCENE-2517-CHANGES.patch","04/Jul/10 22:18;sarowe;LUCENE-2517-branch3x-CHANGES.patch;https://issues.apache.org/jira/secure/attachment/12448658/LUCENE-2517-branch3x-CHANGES.patch","28/Jun/10 12:13;sarowe;LUCENE-2517.patch;https://issues.apache.org/jira/secure/attachment/12448207/LUCENE-2517.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-07-04 18:13:52.424,,,false,,,,,,,,,,,,,,,11307,,,Wed Mar 30 15:50:19 UTC 2011,New,Patch Available,,,,,,"0|i04q3j:",25493,,,,,,,,,"28/Jun/10 12:13;sarowe;This patch addresses the following issues:

# Release headers (==== Release X.X.X .... ====) can be spelled with ""Lucene"" instead of ""Release"": ==== Lucene X.X.X ... ====
# Release versions can have .X, .Y or .Z components (e.g. 3.x)
# JavaScript isOlder() method had a (no longer correct) hard-coded regex to recognize the two latest releases.  This regex is now generated from the two latest releases.
# <code> -> <code><pre> (to make them look better in HTML) is now blocked for inline usages, so that newlines are not added to the HTML version when they are not present in CHANGES.txt.
# Imbalanced parentheses were incorrectly recognized as attribution onset.","28/Jun/10 12:17;sarowe;This patch contains several typo fixes for CHANGES.txt, and one change: the first item under Changes in backwards compatibility policy (LUCENE-1458, LUCENE-2111, LUCENE-2354: Changes from flexible indexing) includes attribution for individual bulleted items, which is a new usage that I *think* is not intended (copy-paste-o, likely), and several of the people that worked on those three issues are not given credit anywhere AFAICT, so I've moved the attributions attached to bulleted items to below the entire bulleted list, and added everyone who posted a patch to any of the three issues.","04/Jul/10 18:13;mikemccand;Looks good!  I'll commit shortly.  I don't understand the Perl ;)  But I ran ""ant changes-to-html"" and the output looks good :)","04/Jul/10 18:16;mikemccand;Steven can you post a 3x only patch for your fixes to lucene/CHANGES.txt?  Looks like most of your fixes were on trunk-only entries?  Thanks.","04/Jul/10 22:18;sarowe;bq. Steven can you post a 3x only patch for your fixes to lucene/CHANGES.txt? Looks like most of your fixes were on trunk-only entries?

This patch addresses the only two (very minor) differences that were in both trunk and the 3.x branch.","05/Jul/10 00:40;mikemccand;Thanks Steven!","30/Mar/11 15:50;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improved javadocs for PriorityQueue#lessThan,LUCENE-2931,12499143,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,simonw,karl.wettin,karl.wettin,20/Feb/11 01:01,30/Mar/11 15:50,30/Sep/19 08:38,20/Feb/11 12:32,2.9.5,3.0.4,3.1,4.0-ALPHA,,,,,,,2.9.5,3.0.4,3.1,4.0-ALPHA,,,,0,,,,It kills me that I have to inspect the code every time I implement a PriorityQueue. :),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-20 07:42:31.371,,,false,,,,,,,,,,,,,,,10941,,,Wed Mar 30 15:50:17 UTC 2011,New,Patch Available,,,,,,"0|i04njj:",25079,,,,,,,,,"20/Feb/11 01:04;karl.wettin;{code}
Index: src/java/org/apache/lucene/util/PriorityQueue.java
===================================================================
--- src/java/org/apache/lucene/util/PriorityQueue.java	(revision 1072471)
+++ src/java/org/apache/lucene/util/PriorityQueue.java	(working copy)
@@ -31,8 +31,11 @@
   private int maxSize;
   protected T[] heap;
 
-  /** Determines the ordering of objects in this priority queue.  Subclasses
-    must define this one method. */
+  /** 
+   * Determines the ordering of objects in this priority queue.  Subclasses
+   * must define this one method.
+   * @return true if parameter a is less than parameter b. 
+   */
   protected abstract boolean lessThan(T a, T b);
 
   /**
{code}","20/Feb/11 07:42;simonw;bq. It kills me that I have to inspect the code every time I implement a PriorityQueue. :)

its a no-brainer ey ;) - thanks I will add to trunk and backport","20/Feb/11 12:32;simonw;Committed to trunk in r1072567 & backported: 
 * 3x: r1072569
 * 3.0: r1072571
 * 2.9: r1072580","30/Mar/11 15:50;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastVectorHighlighter: add a method to set an arbitrary char that is used when concatenating multiValued data,LUCENE-2603,12471671,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,koji,koji,koji,16/Aug/10 09:36,30/Mar/11 15:50,30/Sep/19 08:38,17/Aug/10 02:46,2.9.3,3.0.2,,,,,,,,,3.1,4.0-ALPHA,,,modules/highlighter,,,0,,,,"If the following multiValued names are in authors field:

* Michael McCandless
* Erik Hatcher
* Otis Gospodnetić

Since FragmentsBuilder concatenates multiValued data with a space in BaseFragmentsBuilder.getFragmentSource():

{code}
while( buffer.length() < endOffset && index[0] < values.length ){
  if( index[0] > 0 && values[index[0]].isTokenized() && values[index[0]].stringValue().length() > 0 )
    buffer.append( ' ' );
  buffer.append( values[index[0]++].stringValue() );
}
{code}

an entire field snippet (using LUCENE-2464) will be ""Michael McCandless Erik Hatcher Otis Gospodnetić"". There is a requirement an arbitrary char (e.g. '/') can be set so that client can separate the snippet easily. i.e. ""Michael McCandless/Erik Hatcher/Otis Gospodnetić""",,,,,,,,,,,,,,,,"17/Aug/10 01:33;koji;LUCENE-2603.patch;https://issues.apache.org/jira/secure/attachment/12452241/LUCENE-2603.patch","16/Aug/10 15:14;koji;LUCENE-2603.patch;https://issues.apache.org/jira/secure/attachment/12452180/LUCENE-2603.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-30 15:50:17.132,,,false,,,,,,,,,,,,,,,11231,,,Wed Mar 30 15:50:17 UTC 2011,New,,,,,,,"0|i04pkf:",25407,,,,,,,,,"17/Aug/10 01:33;koji;Updated patch attached. I'll commit shortly.","17/Aug/10 02:46;koji;trunk: Committed revision 986173.
branch_3x: Committed revision 986175.
","30/Mar/11 15:50;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rename KeywordMarkerTokenFilter,LUCENE-2358,12460744,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,rcmuir,rcmuir,30/Mar/10 21:24,30/Mar/11 15:50,30/Sep/19 08:38,11/Apr/10 09:15,,,,,,,,,,,3.1,4.0-ALPHA,,,modules/analysis,,,0,,,,"I would like to rename KeywordMarkerTokenFilter to KeywordMarkerFilter.
We havent released it yet, so its a good time to keep the name brief and consistent.",,,,,,,,,,,,,,,,"30/Mar/10 21:26;rcmuir;LUCENE-2358.patch;https://issues.apache.org/jira/secure/attachment/12440283/LUCENE-2358.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-03-30 23:11:28.359,,,false,,,,,,,,,,,,,,,11438,,,Wed Mar 30 15:50:04 UTC 2011,New,Patch Available,,,,,,"0|i04r2f:",25650,,,,,,,,,"30/Mar/10 21:26;rcmuir;attached is a patch (really svn move of KeywordMarkerTokenFilter and its test will be done)","30/Mar/10 23:11;sarowe;Hi Robert,

I'm working on a change to ShingleFilter to not output ""_"" filler token unigrams (or generally, filler-only ngrams, to cover the case where position increment gaps exceed n).  

I needed to be able to mark cached tokens as being filler tokens (or not) - a boolean attribute.  After trying to write a new private-use attribute and failing (I didn't make both an interface and an implementation, I think - I should figure it out and improve the docs I guess), I found KeywordAttribute and have used it to mark whether or not a cached token is a filler token (keyword:yes => filler-token:yes).

Would it make sense to have a generalized boolean attribute, specialized for keywords or (fill-in-the-blank)?  It's a small leap to say that ""iskeyword"" means true for whatever boolean attribute you want to carry, so this isn't really a big deal, but I thought I'd bring it up while you're thinking about naming this thing.

(This may be a can of worms: if there  is a generic boolean attribute, should there be generic string/int/float/etc. attributes too?)

Steve
","30/Mar/10 23:33;rcmuir;{quote}
I needed to be able to mark cached tokens as being filler tokens (or not) - a boolean attribute. After trying to write a new private-use attribute and failing (I didn't make both an interface and an implementation, I think - I should figure it out and improve the docs I guess), I found KeywordAttribute and have used it to mark whether or not a cached token is a filler token (keyword:yes => filler-token:yes).
{quote}

I'm not really sure the KeywordAttribute is the best fit here, because its purpose is for the token 
to not be changed by some later filter. I'm not sure how your filter works (I would have to see the patch),
but I think using this attribute for this purpose could introduce some bugs?

I guess the key is that its not a private-use attribute really, these things are visible by all tokenstreams.
so stemmers etc will see your 'internal' attribute.

{quote}
Would it make sense to have a generalized boolean attribute, specialized for keywords or (fill-in-the-blank)? It's a small leap to say that ""iskeyword"" means true for whatever boolean attribute you want to carry, so this isn't really a big deal, but I thought I'd bring it up while you're thinking about naming this thing.

(This may be a can of worms: if there is a generic boolean attribute, should there be generic string/int/float/etc. attributes too?)
{quote}

I don't really think so. Since there can only be one of any attribute in the tokenstream, you would have
various TokenFilters clashing on how they interpret and use some generic boolean attribute!
","30/Mar/10 23:44;sarowe;Sorry for cluttering this issue...

{quote}
I'm not really sure the KeywordAttribute is the best fit here, because its purpose is for the token
to not be changed by some later filter. I'm not sure how your filter works (I would have to see the patch),
but I think using this attribute for this purpose could introduce some bugs?

I guess the key is that its not a private-use attribute really, these things are visible by all tokenstreams.
so stemmers etc will see your 'internal' attribute.
{quote}

Yep, you're right, I hadn't thought it through that far.

{quote}
bq. Would it make sense to have a generalized boolean attribute [...]?

I don't really think so. Since there can only be one of any attribute in the tokenstream, you would have
various TokenFilters clashing on how they interpret and use some generic boolean attribute!
{quote}

Um, yes, I should have realized that...

(Re-writing private FillerTokenAttribute! Hooray!)","11/Apr/10 09:15;rcmuir;Committed revision 932856.","30/Mar/11 15:50;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastVectorHighlighter: enable FragListBuilder and FragmentsBuilder to be set per-field override,LUCENE-2626,12472708,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,koji,koji,koji,27/Aug/10 15:50,30/Mar/11 15:49,30/Sep/19 08:38,27/Aug/10 23:52,2.9.3,3.0,,,,,,,,,3.1,4.0-ALPHA,,,modules/highlighter,,,0,,,,,,,,,,,,,,,,,,,,"27/Aug/10 15:53;koji;LUCENE-2626.patch;https://issues.apache.org/jira/secure/attachment/12453247/LUCENE-2626.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-30 15:49:59.048,,,false,,,,,,,,,,,,,,,11211,,,Wed Mar 30 15:49:59 UTC 2011,New,,,,,,,"0|i04pfb:",25384,,,,,,,,,"27/Aug/10 23:52;koji;trunk: Committed revision 990301.
branch_3x: Committed revision 990304.","30/Mar/11 15:49;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spellchecker uses default IW mergefactor/ramMB settings of 300/10,LUCENE-2391,12461756,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,markrmiller@gmail.com,markrmiller@gmail.com,11/Apr/10 17:12,30/Mar/11 15:49,30/Sep/19 08:38,05/Jan/11 03:46,,,,,,,,,,,3.1,4.0-ALPHA,,,modules/spellchecker,,,0,,,,These settings seem odd - I'd like to investigate what makes most sense here.,,,,,,,,,,,,,,,,"22/Dec/10 17:54;rcmuir;LUCENE-2391.patch;https://issues.apache.org/jira/secure/attachment/12466819/LUCENE-2391.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-12-22 17:54:15.141,,,false,,,,,,,,,,,,,,,11410,,,Wed Mar 30 15:49:58 UTC 2011,New,,,,,,,"0|i04qvb:",25618,,,,,,,,,"22/Dec/10 17:54;rcmuir;Here's a patch to speed up the spellchecker build.

* i wired the default RamMB to IWConfig's default
* i didnt mess with the mergefactor for now (because the default is still to optimize)
* but i added an additional 'optimize' parameter so you can update your spellcheck index without re-optimizing.
* when updating, i changed the exists() to work per-segment, so its reasonable if the index isn't optimized.
* the exists() check now bypasses the term dictionary cache, which is stupid and just slows it down.
* we don't do any of the exists() logic if the index is empty (this is the case for i think solr which completely rebuilds
  and doesnt do an incremental update)
* the startXXX, endXXX, and word fields can only contain one term per document. I turned off norms, positions,
  and tf for these.
* the gramXXX field is unchanged, i didnt want to change spellchecker scoring in any way. But we could
  reasonably in the future likely omit norms here too since i think its gonna be very short.

{noformat}
trunk:
scratch build time: 229,803ms
index size: 214,322,200 bytes
no-op update time (updating but there is no new terms to add): 4,619ms

patch:
scratch build time: 99,214ms
index size: 177,781,273 bytes
no-op update time: 2,504ms
{noformat}

i still left the optimize default on, but really i think for most users (e.g. solr) they should set 
mergefactor to be maybe a bit more reasonable, set optimize to false, and the scratch build 
is then must faster (60,000 ms), but the no-op update time is heavier (eg 16,000ms). Still, 
if you are rebuilding on every commit for smallish updates something like 20-30 seconds 
is a lot better than 100seconds, but for now I kept the defaults as is (optimizing every time).
","04/Jan/11 16:19;mikemccand;Patch looks great Robert!

Do we really need to handle subclasses that override exists?","04/Jan/11 16:34;rcmuir;bq. Do we really need to handle subclasses that override exists?

The only reason i did this is because i want to backport this to branch_3x too, since it
significantly speeds up spellchecker rebuilds.

But a simpler option would be to just mark the spellchecker final, does anyone actually
subclass this thing? It seems like a scary thing to subclass (the synchronization etc inside of it)","05/Jan/11 03:46;rcmuir;Committed revision 1055285, 1055289 (3x).

it would be good to make a follow-on-issue to allow solr users to control optimize-on-build, 
and also to control the clearIndex(), so they can reasonably use incremental update rather 
than fully rebuilding the entire spellcheck index every time.

","30/Mar/11 15:49;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utility to output total term frequency and df from a lucene index,LUCENE-2393,12462059,,New Feature,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,tburtonwest,tburtonwest,14/Apr/10 17:18,30/Mar/11 15:49,30/Sep/19 08:38,24/Jun/10 12:23,,,,,,,,,,,3.1,4.0-ALPHA,,,modules/other,,,0,,,,"This is a pair of command line utilities that provide information on the total number of occurrences of a term in a Lucene index.  The first  takes a field name, term, and index directory and outputs the document frequency for the term and the total number of occurrences of the term in the index (i.e. the sum of the tf of the term for each document).   The second reads the index to determine the top N most frequent terms (by document frequency) and then outputs a list of those terms along with  the document frequency and the total number of occurrences of the term. Both utilities are useful for estimating the size of the term's entry in the *prx files and consequent Disk I/O demands. ",,,,,,,,,,,,,,,,"16/Apr/10 19:13;tburtonwest;ASF.LICENSE.NOT.GRANTED--LUCENE-2393.patch;https://issues.apache.org/jira/secure/attachment/12441996/ASF.LICENSE.NOT.GRANTED--LUCENE-2393.patch","15/Apr/10 18:11;tburtonwest;ASF.LICENSE.NOT.GRANTED--LUCENE-2393.patch;https://issues.apache.org/jira/secure/attachment/12441859/ASF.LICENSE.NOT.GRANTED--LUCENE-2393.patch","14/Apr/10 17:20;tburtonwest;ASF.LICENSE.NOT.GRANTED--LUCENE-2393.patch;https://issues.apache.org/jira/secure/attachment/12441750/ASF.LICENSE.NOT.GRANTED--LUCENE-2393.patch","18/Jun/10 09:31;mikemccand;LUCENE-2393-3x.patch;https://issues.apache.org/jira/secure/attachment/12447446/LUCENE-2393-3x.patch","17/Jun/10 17:35;tburtonwest;LUCENE-2393-3xbranch.patch;https://issues.apache.org/jira/secure/attachment/12447367/LUCENE-2393-3xbranch.patch","12/May/10 22:16;tburtonwest;LUCENE-2393.patch;https://issues.apache.org/jira/secure/attachment/12444351/LUCENE-2393.patch","04/May/10 17:12;mikemccand;LUCENE-2393.patch;https://issues.apache.org/jira/secure/attachment/12443597/LUCENE-2393.patch","30/Apr/10 22:18;tburtonwest;LUCENE-2393.patch;https://issues.apache.org/jira/secure/attachment/12443337/LUCENE-2393.patch","20/Apr/10 18:19;tburtonwest;LUCENE-2393.patch;https://issues.apache.org/jira/secure/attachment/12442338/LUCENE-2393.patch",,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,2010-04-14 21:38:19.595,,,false,,,,,,,,,,,,,,,11408,,,Wed Mar 30 15:49:56 UTC 2011,New,Patch Available,,,,,,"0|i04quv:",25616,,,,,,,,,"14/Apr/10 17:20;tburtonwest;Patch against recent trunk.   Can someone please suggest an appropriate existing unit test to use as a model for creating a unit test for this?   Would it be appropriate to include a small index file for testing or is it better to programatically create the index file?","14/Apr/10 17:23;tburtonwest;For an example of how this utility can be used please see: http://www.hathitrust.org/blogs/large-scale-search/slow-queries-and-common-words-part-1","14/Apr/10 21:38;otis;I think creating a small index with a couple of docs would be the way to go.","14/Apr/10 22:19;mikemccand;Programmatically indexing those docs is fine -- most tests make a MockRAMDir, index a few docs into it, and test against that.

This tool looks useful, thanks Tom!

Note that with flex scoring (LUCENE-2392) we are planning on storing this statistic (sum of tf for the term across all docs) in the terms dict, for fields that enable statistics.  So when that lands, this tool can pull from that, or regenerate it if the field didn't store stats.","15/Apr/10 00:06;markrmiller@gmail.com;Perhaps this should be combined with high freq terms tool ... could make a ton of this little guys, so prob best to consolidate them.","15/Apr/10 18:11;tburtonwest;New patch includes a (pre-flex ) version of HighFreqTerms that finds the top N terms with the highest docFreq and looks up the total term frequency and outputs the list of terms sorted by highest term frequency (which approximates the largest entries in the *prx files).    I'm not sure how to combine the GetTermInfo program, with either version of HighFreqTerms  in a way that leads to sane command line arguments and argument processing.   I suppose that HighFreqTerms could have a flag that turns on or off the inclusion of total term frequency.","16/Apr/10 19:13;tburtonwest;Updated the HighFreqTermsWithTF to use flex API. 

 I don't understand the flex API well enough yet to determine if I should have used DocsEnum.read/DocsEnum.getBulkResult()  to do a bulk read instead of DocsEnum.nextDoc() and DocsEnum.freq()..","17/Apr/10 15:58;mikemccand;Patch looks good Tom -- thanks for cutting over to flex.  You could in fact use the bulk read API here; it'd be faster.  But performance isn't a big deal here :)

Maybe you should require a field instead of defaulting to ""ocr""?

Why does GetTermInfo.getTermInfo take a String[] fields (it's not used I think)?

Probably we should cutover to BytesRef here too, eg TermInfoWithTotalTF?

Maybe you could share the code between HighFreqTermsWithTF.getTermFreqOrdered & GetTermInfo.getTermInfo?  (They both loop, summing up the .freq() of each doc to get the total term freq).

Small typo in javadoc thier -> their.

","20/Apr/10 18:19;tburtonwest;Revised patch updated everything  to flex.  Replaces all references to Term with BytesRef and field.  
GetTermInfo now requires a field instead of default= ocr
removed unused String[] fields argument
GetTermInfo now uses shared code HighFreqTermsWithTF.getTotalTF(); to get total tf.
Removed GetTermInfo dependency on TermInfoWithTotalTF[] and inlined it into HighFreqTermsWithTF.

Still don't understand the bulk read API, but given that I have indexes with *frq files of 60GB I'd like to use it.  Is there some documentation, code, or a test case I might look at ?

","22/Apr/10 13:18;mikemccand;bq. Still don't understand the bulk read API, but given that I have indexes with *frq files of 60GB I'd like to use it. Is there some documentation, code, or a test case I might look at ?

I just committed some small improvements to the javdadocs for this -- can you look & see if it's understandable now?

Also, have a look at oal.search.TermScorer -- it consumes the bulk API.","22/Apr/10 13:40;mikemccand;Thanks for the updated patch Tom... feedback:

  * Maybe do away with the ""hack to allow tokens with whitespace""?
    One should use quotes with their shell for this?  (And eg the hack
    doens't work with tokens that have 2 spaces).

  * Can you rename things like total_tf --> totalTF (consistent w/
    Lucene's standard code style)

  * Maybe rename TermInfoWithTotalTF -> TermStats?  (It also has
    .docFreq)

  * Maybe rename TermInfoWithTotalTF.termFreq -> .totalTermFreq?

  * Maybe rename .getTermFreqOrdered -> .sortByTotalTermFreq?

  * You don't really need a priority queue to for the
    getTermFreqOrdered case?  Ie, instead, just fill in the
    .totalTermFreq and then do a normal sort (make a
    Comparator<TermStats> that sorts by the .totalTermFreq)
","27/Apr/10 20:49;tburtonwest;Added unit tests. Made changes outlined by Mike.  Still working on BulkRead.","27/Apr/10 21:05;tburtonwest;Patch that includes unit tests and changes outlined in Mike's comment","30/Apr/10 22:18;tburtonwest;Updated to use BulkResult api.  ","04/May/10 17:12;mikemccand;Patch looks good Tom!

I cleaned things up a bit -- eg, you don't need to use the class members when interacting w/ the bulk DocsEnum API.

I think it's ready to go in!","10/May/10 18:42;mikemccand;I think we should just replace the current HighFreqTerms with the HighFreqTermsWithTF?","10/May/10 19:52;tburtonwest;Hi Mike,

Thanks for all your help. 

If we replace the current HighFreqTerms with the HighFreqTermsWithTF should there be a command line switch so that you could ask for the default behavior of the current HighFreqTerms?  Or perhaps the default should be the current behavior and the switch should turn on the additional step of gathering and reporting on the totalTF for the terms.   

I haven't bench-marked it but I'm wondering if  getting the totalTF  could  take a significant  additional amount of time for large indexes.  When I ask for the top 10,000 terms using HighFreqTermsWithTF for our 500,000 document indexes it takes about 40 minutes to an hour.  I'm guessing that most of that time is taken in the first step of getting the top 10,000 terms by docFreq, but still it seems that reading the data and calculating the totalTF for 10,000 terms might be a significant enough fraction of the total time that the option to skip that step might be useful.


Tom


","10/May/10 20:07;mikemccand;Tom, I agree, we should make it optional to compute the totalTF, and probably default it to off?  Can you tweak the latest patch to do this?","12/May/10 16:08;tburtonwest;I tweaked the latest patch to mimic the current HighFreqTerms unless you give it a -t argument.  However, while testing the argument parsing I found a bug I suspect I inserted into the patch a few versions ago.   Am in the process of writing a unit test to exercise the bug and then will fix bug and post both tests and code.","12/May/10 22:16;tburtonwest;Rewrote argument processing so the default behavior is that of HighFreqTerms.  The field and number of terms are now both optional with the default being all fields and 100 terms (same default as currrent HighFreqTerms).  If a -t flag is used the totalTermFreq stats will be read,calculated, and displayed. 

The bug surfaced when not specifying a field.  Added test data with multiple fields and tests to check that correct results are returned with and without a field being specified.  Fixed bug and new tests pass.

With the increasing number of options, I started thinking about more robust command line argument processing.  I'm used to languages where there is a commonly used Getopt(s)  library.  There appear to be several for Java with different features, different levels of active development and different licenses. Is it worth the overhead of using one, and if so which one would be the best to use?

Tom
","13/May/10 16:12;mikemccand;Patch looks good Tom!  I'll re-merge my small changes from the prior patch, add a CHANGES, and commit.

I don't think we need to upgrade to CL processing lib...","13/May/10 17:04;mikemccand;Thanks Tom!","17/Jun/10 17:35;tburtonwest;Since many people will want to use branch 3.x instead of trunk, I back-ported the flex version to 3x ( patched against http://svn.apache.org/repos/asf/lucene/dev/branches/branch_3x/lucene : 955141)
Mike, can this be committed to branch_3x?

Tom","18/Jun/10 09:30;mikemccand;Thanks Tom!

Reopening for backport to 3x....","18/Jun/10 09:31;mikemccand;New patch, just cleans up a few minor things...","30/Mar/11 15:49;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move NoDeletionPolicy from benchmark to core,LUCENE-2385,12461560,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,shaie,shaie,shaie,08/Apr/10 17:57,30/Mar/11 15:49,30/Sep/19 08:38,05/May/10 20:11,,,,,,,,,,,3.1,4.0-ALPHA,,,core/index,modules/benchmark,,0,,,,"As the subject says, but I'll also make it a singleton + add some unit tests, as well as some documentation. I'll post a patch hopefully today.",,,,,,,,,,,,,,,,"08/Apr/10 21:08;shaie;LUCENE-2385.patch;https://issues.apache.org/jira/secure/attachment/12441208/LUCENE-2385.patch","08/Apr/10 20:09;shaie;LUCENE-2385.patch;https://issues.apache.org/jira/secure/attachment/12441205/LUCENE-2385.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-04-08 20:42:56.474,,,false,,,,,,,,,,,,,,,11416,,,Wed Mar 30 15:49:55 UTC 2011,New,Patch Available,,,,,,"0|i04qwn:",25624,,,,,,,,,"08/Apr/10 20:09;shaie;Move NoDeletionPolicy to core, adds javadocs + TestNoDeletionPolicy. Also includes the relevant changes to benchmark (algorithms + CreateIndexTask).
I've fixed a typo I had in NoMergeScheduler - not related to this issue, but since it was just a typo, thought it's no harm to do it here.

Tests pass. Planning to commit shortly.","08/Apr/10 20:42;uschindler;The patch does not look like you svn moved the files. To preserve history, you should do a ""svn move"" of the file in your local repository and then modify it to reflect the package changes (if any).

Did you do this?","08/Apr/10 20:49;shaie;I did that first, but then remembered that when I did that in the past, people were unable to apply my patches, w/o doing the svn move themselves. Anyway, for this file it's not really important I think - a very simple and tiny file, w/ no history to preserve? Is that ok for this file (b/c I have no idea how to do the svn move now ... after I've made all the changes already) :)","08/Apr/10 21:08;uschindler;In general we place a list of all svn move/copy command together with the patch, executeable from the root dir. If you paste those commands into your terminal and then apply the patch, it works. One example is the jflex issue (ok, the commands are shortened).

Another possibility is to have a second checkout, where you arrange the files correctly (svn moved/copied) and one for creating the patches.","08/Apr/10 21:08;shaie;Is it better now?","08/Apr/10 21:13;shaie;Forgot to mention that the only move I made was of NoDeletionPolicy:

svn move contrib/benchmark/src/java/org/apache/lucene/benchmark/utils/NoDeletionPolicy.java src/java/org/apache/lucene/index/NoDeletionPolicy.java

I'll remember that in the future Uwe - thanks for the heads up !","08/Apr/10 21:26;uschindler;Yeah thats fine!","08/Apr/10 21:36;shaie;Committed revision 932129.","05/May/10 20:06;shaie;backport to 3.1","05/May/10 20:11;shaie;Committed revision 941460.","30/Mar/11 15:49;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Small imprecision in Search package Javadocs,LUCENE-2579,12470515,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,smolav,smolav,30/Jul/10 14:06,30/Mar/11 15:49,30/Sep/19 08:38,30/Jul/10 18:51,3.0.2,,,,,,,,,,3.1,4.0-ALPHA,,,general/javadocs,,,0,,,,"Search package Javadocs states that Scorer#score(Collector) will be abstract in Lucene 3.0, which is not accurate anymore.",,600,600,,0%,600,600,,,,,,,,,"30/Jul/10 14:07;smolav;nitpicky_doc.patch;https://issues.apache.org/jira/secure/attachment/12450904/nitpicky_doc.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-30 18:51:20.432,,,false,,,,,,,,,,,,,,,11254,,,Wed Mar 30 15:49:53 UTC 2011,New,Patch Available,,,,,,"0|i04ppr:",25431,,,,,,,,,"30/Jul/10 18:51;mikemccand;Thanks Santiago!","30/Mar/11 15:49;gsingers;Bulk close for 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jarify target gives misleading message when svnversion doesn't exist,LUCENE-1159,12387323,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Not A Problem,gsingers,gsingers,gsingers,28/Jan/08 20:28,25/Jan/11 16:17,30/Sep/19 08:38,25/Jan/11 16:17,,,,,,,,,,,,,,,general/build,,,0,,,,"The jarify command in common-build.xml seems to indicate failure when it can't find svnversion, but this is, in fact, just a warning.  We should check to see if svnversion exists before attempting the command at all, if possible.

The message looks something like:
 [exec] Execute failed: java.io.IOException: java.io.IOException: svnversion: not found

Which is understandable, but it is not clear what the ramifications are of this missing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-25 16:17:26.918,,,false,,,,,,,,,,,,,,,12585,,,Tue Jan 25 16:17:26 UTC 2011,New,,,,,,,"0|i04ypj:",26888,,,,,,,,,"25/Jan/11 16:17;shaie;This seems to be fixed already. From common-build.xml:

{noformat}
<!-- If possible, include the svnversion -->
<exec dir=""."" executable=""${svnversion.exe}""
       outputproperty=""svnversion"" failifexecutionfails=""false"">
    <arg line="".""/>
</exec>
 {noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoostingTermQuery#defaultTermBoost,LUCENE-1155,12387152,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,karl.wettin,karl.wettin,25/Jan/08 13:43,25/Jan/11 16:05,30/Sep/19 08:38,25/Jan/11 16:05,,,,,,,,,,,,,,,core/search,,,0,,,,"This patch allows a null payload to mean something different than 1f.

(I have this use case where 99% of my tokens share the same rather large token position payload boost.)

{code}
Index: src/java/org/apache/lucene/search/payloads/BoostingTermQuery.java
===================================================================
--- src/java/org/apache/lucene/search/payloads/BoostingTermQuery.java   (revision 615215)
+++ src/java/org/apache/lucene/search/payloads/BoostingTermQuery.java   (working copy)
@@ -41,11 +41,16 @@
  */
 public class BoostingTermQuery extends SpanTermQuery{
 
+  private Float defaultTermBoost = null;
 
   public BoostingTermQuery(Term term) {
     super(term);
   }
 
+  public BoostingTermQuery(Term term, Float defaultTermBoost) {
+    super(term);
+    this.defaultTermBoost = defaultTermBoost;
+  }
 
   protected Weight createWeight(Searcher searcher) throws IOException {
     return new BoostingTermWeight(this, searcher);
@@ -107,7 +112,9 @@
           payload = positions.getPayload(payload, 0);
           payloadScore += similarity.scorePayload(term.field(), payload, 0, positions.getPayloadLength());
           payloadsSeen++;
-
+        } else if (defaultTermBoost != null) {
+          payloadScore += defaultTermBoost;
+          payloadsSeen++;
         } else {
           //zero out the payload?
         }
@@ -146,7 +153,14 @@
 
   }
 
+  public Float getDefaultTermBoost() {
+    return defaultTermBoost;
+  }
 
+  public void setDefaultTermBoost(Float defaultTermBoost) {
+    this.defaultTermBoost = defaultTermBoost;
+  }
+
   public boolean equals(Object o) {
     if (!(o instanceof BoostingTermQuery))
       return false;
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-01-25 13:51:46.716,,,false,,,,,,,,,,,,,,,12589,,,Tue Jan 25 16:05:40 UTC 2011,New,Patch Available,,,,,,"0|i04yqf:",26892,,,,,,,,,"25/Jan/08 13:45;karl.wettin;missed first two lines in the diff","25/Jan/08 13:51;gsingers;I think I am missing something here.  If they are all the same, except a few, why even store a payload for those that are the same?  Why not just rely on the usual term scoring for those and just get the boost from those tokens that truly need a payload?","25/Jan/08 14:20;karl.wettin;{quote}
I think I am missing something here. If they are all the same, except a few, why even store a payload for those that are the same? Why not just rely on the usual term scoring for those and just get the boost from those tokens that truly need a payload?
{quote}

Thanks to this patch I don't store payload for all those that are the same. 

I only store the payload for 1% of the tokens. The remaining 99% of the term positions, them with a null payload,  are boosted according to a value I configure my query to use. In my case I want a null payload to mean 5f boost, and the 1% positions with payload set have something between 4.9f and 1f boost.

I'm not at all surprised if it is considered backwards, works great though. I have no idea to how one can do this another way.

","23/Feb/08 22:23;karl.wettin;{quote}
I think I am missing something here. If they are all the same, except a few, why even store a payload for those that are the same? Why not just rely on the usual term scoring for those and just get the boost from those tokens that truly need a payload?
{quote}

It might be worth mentioning that I have omitted the norms.","24/Aug/08 12:26;karl.wettin;I still think this patch makes sense and would very much like to see it in 2.4.","12/Sep/08 17:50;mikemccand;Grant, what do you think of this one?  The patch looks reasonable to me; it's backwards compatible.  Should we do this for 2.4?","12/Sep/08 18:54;gsingers;Sorry, I still don't get it, but I'm probably dense.  Maybe an actual example would help.

Next, it incs payloadsSeen when it hasn't, in fact, seen a payload.  That seems weird.

Second, why not just scale the results from 1 to 5 to be between 0 and 1 (and possibly just call Query.setBoost(5))?  That is, for the 1% of positions, make it range between 0.9 and 0.1 (or whatever).  

That being said, I suppose it doesn't hurt, but it just seems confusing.  If anything, I would rename the variable something like nullPayloadBoost or something else, as I think defaultTermBoost implies it's the default boost applied when there is a payload, but the boost isn't specified, which doesn't make sense either.","12/Sep/08 21:43;karl.wettin;bq. Sorry, I still don't get it, but I'm probably dense. Maybe an actual example would help.

In my single case all term positions contains a payload boost. In order to optimize the index size I analyzed the payloads and came to the conclution that 30% or so had the boost 5f, a clear majority of the boosts. In order to minimize the index size I choosed to use this patch and set the default payload to 5f rather than storing 5f in the payload.

How and why I managed to get a majority of the payloads to be 5f and if I could avoid this by manageing boosts in another layer is a completely different story.","13/Sep/08 11:55;gsingers;How is that any different than marking the 70% as having a boost ranging between 1/5 = 0.2 and 5/5 = 1 whereby no patch is needed?  You still have the same 30% null payloads","13/Sep/08 13:46;karl.wettin;bq. How is that any different than marking the 70% as having a boost ranging between 1/5 = 0.2 and 5/5 = 1 whereby no patch is needed? You still have the same 30% null payloads

There would be absolutly no difference to the end result. 

I want to turn this question around and ask why you don't normalize all boosts at query time to top index 1f and then use negative boost for everything else? 

It's about gutfeelings. 

Nulling out the payloads could be gained by dividing the weight by 5 at creation time (due to lots of reasons this was not a good chooise), by a dividing all weight by 5 in a filter in the end of the chain, or by introducing this patch. For me BoostingTermQuery felt like the most obvious place to handle this stuff, and it still does.


","25/Jan/11 16:05;shaie;We don't have BoostingTermQuery anymore, and there was never consensus here to fix it within Lucene, vs. e.g. the workarounds Grant proposed. Given that, and the fact that the issue is inactive since Sep-2008, and that today we give enough API for someone to write this sort of capability in his application, I'm closing the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmarker tasks for the TPB data collection,LUCENE-988,12376905,,New Feature,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Not A Problem,,karl.wettin,karl.wettin,27/Aug/07 00:03,25/Jan/11 15:57,30/Sep/19 08:38,25/Jan/11 15:57,2.3,,,,,,,,,,,,,,modules/benchmark,,,0,,,,"Very simple DocMaker and QueryMaker for the TPB data collection (~150,000 content items, ~500,000 comments to the contents and ~3,700,000 user queries).

URL to dataset:
http://thepiratebay.org/tor/3783572/db_dump_and_query_log_from_piratebay.org__summer_of_2006
",,,,,,,,,,,,,,,,"27/Aug/07 00:04;karl.wettin;LUCENE-988.txt;https://issues.apache.org/jira/secure/attachment/12364583/LUCENE-988.txt",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-01-25 15:57:07.207,,,false,,,,,,,,,,,,,,,12755,,,Tue Jan 25 15:57:07 UTC 2011,New,Patch Available,,,,,,"0|i04zrj:",27059,,,,,,,,,"25/Jan/11 15:57;shaie;Closing because I'm not sure what's the license level of ""The Pirate Bay"" DB and also not sure that we want to have such DB in Lucene. Benchmark's API allows for someone to write a ContentSource which reads whatever source he wants, and convert it to DocData that is later fed and index by DocMaker.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add sizeof to OpenBitSet,LUCENE-1767,12431720,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,jasonrutherglen,jasonrutherglen,29/Jul/09 17:12,24/Jan/11 21:16,30/Sep/19 08:38,24/Jan/11 21:16,2.4.1,,,,,,,,,,4.0-ALPHA,,,,core/index,,,0,,,,Adding a sizeof method to OpenBitSet will facilitate estimating RAM usage when many OBS' are cached (such as Solr).,,7200,7200,,0%,7200,7200,,,,,,,,,"29/Jul/09 17:35;jasonrutherglen;LUCENE-1767.patch;https://issues.apache.org/jira/secure/attachment/12414927/LUCENE-1767.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-07-29 17:57:33.938,,,false,,,,,,,,,,,,,,,11994,,,Mon Jan 24 21:16:53 UTC 2011,New,,,,,,,"0|i04uxr:",26277,,,,,,,,,"29/Jul/09 17:35;jasonrutherglen;Added sizeOf method","29/Jul/09 17:57;simonw;Jason, I would expect a sizeOf method to return the size of the bitset itself (what #size()) returns. Maybe you find another name for that method. I also think you can safely leave the constants out - once you leave those out this method is almost identical to #capacity / #size.

I'm not sure if such a method would rather confuse users / developers. If we add it I would rather go for a very meaningful name like allocatedBytes.

simon","04/Aug/09 16:10;markrmiller@gmail.com;Anyone have anything to add or any committer want to take this on in the next couple days? If not, I'm going to push it out of 2.9.","06/Aug/09 12:47;markrmiller@gmail.com;I'm about to push this to 3.1 unless someone speaks up","24/Jan/11 21:16;jasonrutherglen;Won't be working on these and they're old",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InstantiatedIndexReader.clone,LUCENE-1537,12414257,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,karl.wettin,jasonrutherglen,jasonrutherglen,06/Feb/09 20:03,24/Jan/11 21:16,30/Sep/19 08:38,24/Jan/11 21:16,2.4,,,,,,,,,,4.0-ALPHA,,,,modules/other,,,0,,,,This patch will implement IndexReader.clone for InstantiatedIndexReader.  ,,7200,7200,,0%,7200,7200,,,,,,,,,"10/Feb/09 18:38;jasonrutherglen;LUCENE-1537.patch;https://issues.apache.org/jira/secure/attachment/12399935/LUCENE-1537.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-02-15 12:58:17.565,,,false,,,,,,,,,,,,,,,12217,,,Mon Jan 24 21:16:52 UTC 2011,New,,,,,,,"0|i04wdj:",26510,,,,,,,,,"10/Feb/09 18:38;jasonrutherglen;- Added InstantiatedIndexReader.clone method 
- Added TestIndicesEquals.testClone method 
- I tried to remove the InstantiatedIndex requirement 
that the incoming reader be optimized, though I ran 
into errors, so backed out","15/Feb/09 12:58;karl.wettin;I didn't try it out yet, but I have a few comments and questions on the patch:

{code}
Index: contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java
+  
+  public Object clone() {
+    try {
+      doCommit();
+      InstantiatedIndex clonedIndex = index.cloneWithDeletesNorms();
+      return new InstantiatedIndexReader(clonedIndex);
+    } catch (IOException ioe) {
+      throw new RuntimeException("""", ioe);
+    }
+  }

Index: contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndex.java
+
+  InstantiatedIndex cloneWithDeletesNorms() {
+    InstantiatedIndex clone = new InstantiatedIndex();
+    clone.version = System.currentTimeMillis();
+    clone.documentsByNumber = documentsByNumber;
+    clone.deletedDocuments = new HashSet<Integer>(deletedDocuments);
+    clone.termsByFieldAndText = termsByFieldAndText;
+    clone.orderedTerms = orderedTerms;
+    clone.normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(normsByFieldNameAndDocumentNumber);
+    clone.fieldSettings = fieldSettings;
+    return clone;
+  }
{code}

Perhaps we should move deleted documents to the reader? It might be a bit of work to hook it up with term enum et c, but it could be worth looking in to. I think it makes more sense to keep the same instance of InstantiatedIndex and only produce a cloned InstantiatedIndexReader. It is the reader#clone we call upon so cloning the store sounds like a future placeholder for unwanted bugs.



I see there are some left overs from your attempt to handle none  optimized readers:

{code}
-    documentsByNumber = new InstantiatedDocument[sourceIndexReader.numDocs()];
+    documentsByNumber = new InstantiatedDocument[sourceIndexReader.maxDoc()];
 
     // create documents
     for (int i = 0; i < sourceIndexReader.numDocs(); i++) {
{code}

I think if you switch to maxDoc it should also use maxDoc int the loop and skip any deleted document. 



{code}
-    for (InstantiatedDocument document : getDocumentsByNumber()) {
+    //for (InstantiatedDocument document : getDocumentsByNumber()) {
+    for (InstantiatedDocument document : getDocumentsNotDeleted()) {
       for (Field field : (List<Field>) document.getDocument().getFields()) {
         if (field.isTermVectorStored() && field.isStoreOffsetWithTermVector()) {
           TermPositionVector termPositionVector = (TermPositionVector) sourceIndexReader.getTermFreqVector(document.getDocumentNumber(), field.name());
@@ -312,7 +325,15 @@
   public InstantiatedDocument[] getDocumentsByNumber() {
     return documentsByNumber;
   }
-
+  
+  public List<InstantiatedDocument> getDocumentsNotDeleted() {
+    List<InstantiatedDocument> list = new ArrayList<InstantiatedDocument>(documentsByNumber.length-deletedDocuments.size());
+    for (int x=0; x < documentsByNumber.length; x++) {
+      if (!deletedDocuments.contains(x)) list.add(documentsByNumber[x]);
+    }
+    return list;
+  } 
+  
{code}

As the source never contains any deleted documents this really doesn't do anything but consume a bit of resources, or?



{code}
-    int maxVal = getAssociatedDocuments()[max].getDocument().getDocumentNumber();
+    InstantiatedTermDocumentInformation itdi = getAssociatedDocuments()[max];
+    InstantiatedDocument id = itdi.getDocument();
+    int maxVal = id.getDocumentNumber();
+    //int maxVal = getAssociatedDocuments()[max].getDocument().getDocumentNumber();
{code}

Is this refactor just for debugging purposes? I find it harder to read than the original one-liner.","17/Feb/09 19:20;jasonrutherglen;bq. handle non-optimized readers

Why is it that the incoming reader must be optimized?  I did try to change this and ran into issues.

bq. Perhaps we should move deleted documents to the reader?

That makes sense.  Was there a reason to have it in both?

bq. Is this refactor just for debugging purposes

It is for debugging.  ","17/Feb/09 19:23;uschindler;sorry, wrong issue :( - reverted","14/Jul/09 18:38;mikemccand;Karl/Jason are you still actively working on this one?  Should we push out?","14/Jul/09 18:43;jasonrutherglen;Pushing it out","24/Jan/11 21:16;jasonrutherglen;Won't be working on these and they're old",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"BitVector implement DocIdSet, IndexReader returns DocIdSet deleted docs",LUCENE-1476,12409846,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,jasonrutherglen,jasonrutherglen,03/Dec/08 22:28,24/Jan/11 21:16,30/Sep/19 08:38,24/Jan/11 21:16,2.4,,,,,,,,,,,,,,core/index,,,0,,,,Update BitVector to implement DocIdSet.  Expose deleted docs DocIdSet from IndexReader.,,43200,43200,,0%,43200,43200,,,,,,,,,"29/Jan/09 16:01;mikemccand;LUCENE-1476.patch;https://issues.apache.org/jira/secure/attachment/12399015/LUCENE-1476.patch","28/Jan/09 23:04;jasonrutherglen;LUCENE-1476.patch;https://issues.apache.org/jira/secure/attachment/12398939/LUCENE-1476.patch","21/Jan/09 19:30;jasonrutherglen;LUCENE-1476.patch;https://issues.apache.org/jira/secure/attachment/12398423/LUCENE-1476.patch","21/Jan/09 01:41;jasonrutherglen;LUCENE-1476.patch;https://issues.apache.org/jira/secure/attachment/12398361/LUCENE-1476.patch","04/Dec/08 00:00;jasonrutherglen;LUCENE-1476.patch;https://issues.apache.org/jira/secure/attachment/12395236/LUCENE-1476.patch","28/Jan/09 22:54;jasonrutherglen;TestDeletesDocIdSet.java;https://issues.apache.org/jira/secure/attachment/12398938/TestDeletesDocIdSet.java","30/Jan/09 12:13;mikemccand;hacked-deliterator.patch;https://issues.apache.org/jira/secure/attachment/12399117/hacked-deliterator.patch","09/Jan/09 03:36;marvin;quasi_iterator_deletions.diff;https://issues.apache.org/jira/secure/attachment/12397479/quasi_iterator_deletions.diff","21/Jan/09 02:29;marvin;quasi_iterator_deletions_r2.diff;https://issues.apache.org/jira/secure/attachment/12398365/quasi_iterator_deletions_r2.diff","29/Jan/09 22:26;marvin;quasi_iterator_deletions_r3.diff;https://issues.apache.org/jira/secure/attachment/12399050/quasi_iterator_deletions_r3.diff","23/Jan/09 21:02;jasonrutherglen;searchdeletes.alg;https://issues.apache.org/jira/secure/attachment/12398605/searchdeletes.alg","29/Jan/09 19:26;mikemccand;sortBench2.py;https://issues.apache.org/jira/secure/attachment/12399028/sortBench2.py","29/Jan/09 19:26;mikemccand;sortCollate2.py;https://issues.apache.org/jira/secure/attachment/12399029/sortCollate2.py",,,,,,,,,13.0,,,,,,,,,,,,,,,,,,,2008-12-04 00:00:53.702,,,false,,,,,,,,,,,,,,,12275,,,Mon Jan 24 21:16:47 UTC 2011,New,,,,,,,"0|i04wqn:",26569,,,,,,,,,"04/Dec/08 00:00;jasonrutherglen;LUCENE-1476.patch

BitVector extends DocIdSet.  

TestBitVector implements testDocIdSet method that is based on TestSortedVIntList tests ","04/Dec/08 00:00;mikemccand;But, SegmentReader needs random access to the bits (DocIdSet only provides an iterator)?","04/Dec/08 00:09;jasonrutherglen;Looks like we need a new abstract class.  RABitSet?  ","04/Dec/08 00:11;rengels@ix.netcom.com;BitSet is already random access, DocIdSet is not.","04/Dec/08 00:16;jasonrutherglen;BitVector does not implement the methods of java.util.BitSet.  RABitSet could be implemented by OpenBitSet and BitVector.  This way an OpenBitSet or another filter such as P4Delta could be used in place of BitVector in SegmentReader.  

The IndexReader.flush type of methods would need to either automatically not save, throw an exception, and there needs to be a setting.  This helps the synchronization issue in SegmentReader.isDeleted by allowing access to it.  ","05/Dec/08 11:29;mikemccand;bq. But, SegmentReader needs random access to the bits (DocIdSet only provides an iterator)? 

Although IndexReader.isDeleted exposes a random-access API to deleted docs, I think it may be overkill.

Ie, in most (all?) uses of deleted docs throughout Lucene core/contrib, a simple iterator (DocIdSet) would in fact suffice.

EG in SegmentTermDocs iteration we are always checking deletedDocs by ascending docID.  It might be a performance gain (pure speculation) if we used an iterator API, because we could hold ""nextDelDocID"" and only advance that (skipTo) when the term's docID has moved past it.  It's just like an ""AND NOT X"" clause.

Similarly, norms, which also now expose a random-access API, should be fine with an iterator type API as well.

This may also imply better VM behavior, since we don't actually require norms/deletions to be fully memory resident.

This would be a biggish change, and it's not clear whether/when we should explore it, but I wanted to get the idea out there.

Marvin, in KS/Lucy are you using random-access or iterator to access deletedDocs & norms?","05/Dec/08 14:21;rengels@ix.netcom.com;I don't think you can change this...

In many cases after you have read an index, and retrieved document numbers, these are lazily returned to the client.

By the time some records are needed to be read, they may have already been deleted (at least this was the usage in old lucene, where deletions happened in the reader).

I think a lot of code assumes this, and calls the isDeleted() to ensure the document is still valid.","05/Dec/08 18:22;marvin;> Marvin, in KS/Lucy are you using random-access or iterator to access 
> deletedDocs & norms?

Both. There's a DelEnum class which is used by NOTScorer and MatchAllScorer, but it's implemented using BitVectors which get the next deleted doc num by calling nextSetBit() internally. 

 I happened to be coding up those classes this spring when there was the big brouhaha about IndexReader.isDeleted().  It seemed wrong to pay the method call overhead for IndexReader.isDeleted() on each iter in NOTScorer.next() or MatchAllScorer.next(), when we could just store the next deletion:

{code}
i32_t
MatchAllScorer_next(MatchAllScorer* self) 
{
    do {
        if (++self->doc_num > self->max_docs) {
            self->doc_num--;
            return 0;
        }
        if (self->doc_num > self->next_deletion) {
            self->next_deletion 
                = DelEnum_Skip_To(self->del_enum, self->doc_num);
        }
    } while (self->doc_num == self->next_deletion);
    return self->doc_num;
}
{code}

(Note: Scorer.next() in KS returns the document number; doc nums start at 1, and 0 is the sentinel signaling iterator termination. I expect that Lucy will be the same.)

Perhaps we could get away without needing the random access, but that's because IndexReader.isDeleted() isn't exposed and because IndexReader.fetchDoc(int docNum) returns the doc even if it's deleted -- unlike Lucene which throws an exception. Also, you can't delete documents against an IndexReader, so Robert's objection doesn't apply to us.

I had always assumed we were going to have to expose isDeleted() eventually, but maybe we can get away with zapping it. Interesting!

I've actually been trying to figure out a new design for deletions because writing them out for big segments is our last big write bottleneck, now that we've theoretically solved the sort cache warming issue.  I figured we would continue to need bit-vector files because they're straightforward to mmap, but if we only need iterator access, we can use vbyte encoding instead... Hmm, we still face the problem of outsized write cost when a segment has a large number of deletions and you add one more...","05/Dec/08 18:42;mikemccand;
bq. It seemed wrong to pay the method call overhead for IndexReader.isDeleted() on each iter in NOTScorer.next() or MatchAllScorer.next(), when we could just store the next deletion:

Nice!  This is what I had in mind.

I think we could [almost] do this across the board for Lucene.
SegmentTermDocs would similarly store nextDeleted and apply the same
""AND NOT"" logic.

bq. that's because IndexReader.isDeleted() isn't exposed and because IndexReader.fetchDoc(int docNum) returns the doc even if it's deleted

Hmm -- that is very nicely enabling.

bq. I've actually been trying to figure out a new design for deletions because writing them out for big segments is our last big write bottleneck

One approach would be to use a ""segmented"" model.  IE, if a few
deletions are added, write that to a new ""deletes segment"", ie a
single ""normal segment"" would then have multiple deletion files
associated with it.  These would have to be merged (iterator) when
used during searching, and, periodically coalesced.

bq. if we only need iterator access, we can use vbyte encoding instead

Right: if there are relatively few deletes against a segment, encoding
the ""on bits"" directly (or deltas) should be a decent win since
iteration is much faster.
","05/Dec/08 18:48;mikemccand;
{quote}
In many cases after you have read an index, and retrieved document numbers, these are lazily returned to the client.

By the time some records are needed to be read, they may have already been deleted (at least this was the usage in old lucene, where deletions happened in the reader).

I think a lot of code assumes this, and calls the isDeleted() to ensure the document is still valid.
{quote}

But isn't that an uncommon use case?  It's dangerous to wait a long
time after getting a docID from a reader, before looking up the
document.  Most apps pull the doc right away, send it to the user, and
the docID isn't kept (I think?).

But still I agree: we can't eliminate random access to isDeleted
entirely.  We'd still have to offer it for such external cases.

I'm just saying the internal uses of isDeleted could all be switched
to iteration instead, and, we might get some performance gains from
it especially when the number of deletes on a segment is relatively low.
","05/Dec/08 19:55;rengels@ix.netcom.com;but IndexReader.document(n) throws an exception if the document is deleted...0 so you still need random access","05/Dec/08 20:01;mikemccand;bq. but IndexReader.document throws an exception if the document is deleted...0 so you still need random access 

Does it really need to throw an exception?  (Of course for back compat it does, but we could move away from that to a new method that doesn't check).","05/Dec/08 20:51;jasonrutherglen;It would be great if instead of relying on Lucene to manage the deletedDocs file, the API would be pluggable enough such that a DocIdBitSet (DocIdSet with random access) could be set in a SegmentReader, and the file access (reading and writing) could be managed from outside.  Of course this is difficult to do and still make things backwards compatible, however for 3.0 I would *really* like to be a part of efforts to create a completely generic and pluggable API that is cleanly separated from the underlying index format and files.  This would mean that the analyzing, querying, scoring portions of Lucene could access an IndexReader like pluggable class where the underlying index files, when and how the index files are written to disk is completely separated.  

One motivation for this patch is to allow custom queries access to the deletedDocs in a clean way (meaning not needing to be a part of the o.a.l.s. package) 

I am wondering if it is good to try to get IndexReader.clone working again, or if there is some other better way related to this patch to externally manage the deletedDocs.  

Improving the performance of deletedDocs would help for every query so it's worth looking at.  ","05/Dec/08 21:55;marvin;> Does it really need to throw an exception?

Aside from back compat, I don't see why it would need to.  I think the only rationale is to serve as a backstop protecting against invalid reads.","05/Dec/08 22:44;rengels@ix.netcom.com;That's my point, in complex multi-treaded software with multiple readers, etc. it is a good backspot against errors.. :)","05/Dec/08 22:46;marvin;> It would be great if instead of relying on Lucene to manage the 
> deletedDocs file, the API would be pluggable

In LUCENE-1478, ""IndexComponent"" was proposed, with potential subclasses including PostingsComponent, LexiconComponent/TermDictComponent, TermVectorsComponent, and so on.  Since then, it has become apparent that SnapshotComponent and DeletionsComponent also belong at the top level.

In Lucy/KS, these would all be specified within a Schema: 

{code}
class MySchema extends Schema {
  DeletionsComponent deletionsComponent() { 
    return new DocIdBitSetDeletionsComponent();
  }

  void initFields() {
    addField(""title"", ""text"");
    addField(""content"", ""text"");
  }

  Analyzer analyzer() {
    return new PolyAnalyzer(""en"");
  }
}
{code}

Mike, you were planning on managing IndexComponents via IndexReader and IndexWriter constructor args, but won't that get unwieldy if there are too many components?  A Schema class allows you to group them together.  You don't have to use it to manage fields the way KS does -- just leave that out.","06/Dec/08 10:18;mikemccand;bq. Mike, you were planning on managing IndexComponents via IndexReader and IndexWriter constructor args, but won't that get unwieldy if there are too many components? A Schema class allows you to group them together. You don't have to use it to manage fields the way KS does - just leave that out.

Agreed.  I'll try to do something along these lines under LUCENE-1458.","08/Dec/08 00:57;marvin;> One approach would be to use a ""segmented"" model. 

That would improve the average performance of deleting a document, at the cost
of some added complexity.  Worst-case performance -- which you'd hit when you
consolidated those sub-segment deletions files -- would actually degrade a
bit.

To manage consolidation, you'd need a deletions merge policy that operated
independently from the primary merge policy.  Aside from the complexity penalty, 
having two un-coordinated merge policies would be bad for real-time search, 
because you want to be able to control exactly when you pay for a big merge.

I'm also bothered by the proliferation of small deletions files.  Probably
you'd want automatic consolidation of files under 4k, but you still could end
up with a lot of files in a big index.

So... what if we wrote, merged, and removed deletions files on the same
schedule as ordinary segment files?  Instead of going back and quasi-modifying
an existing segment by associating a next-generation .del file with it, we write
deletions to a NEW segment and have them reference older segments.  

In other words, we add ""tombstones"" rather than ""delete"" documents.

Logically speaking, each tombstone segment file would consist of an array of
segment identifiers, each of which would point to a ""tombstone row"" array of
vbyte-encoded doc nums:

{code}
// _6.tombstone
   _2: [3, 4, 25]
   _3: [13]

// _7.tombstone
   _2: [5]

// _8.tombstone
   _1: [94]
   _2: [7, 8]
   _5: [54, 55]
{code}

The thing that makes this possible is that the dead docs marked by tombstones
never get their doc nums shuffled during segment merging -- they just
disappear.   If deleted docs lived to be consolidated into new segments and
acquire new doc nums, tombstones wouldn't work.  However, we can associate
tombstone rows with segment names and they only need remain valid as long 
as the segments they reference survive.  

Some tombstone rows will become obsolete once the segments they reference go
away, but we never arrive at a scenario where we are forced to discard valid
tombstones.  Merging tombstone files simply involves dropping obsolete
tombstone rows and collating valid ones.

At search time, we'd use an iterator with an internal priority queue to
collate tombstone rows into a stream -- so there's still no need to slurp the
files at IndexReader startup.","08/Dec/08 21:05;mikemccand;
I like this approach!!

It's also incremental in cost (cost of flush/commit is in proportion
to how many deletes were done), but you are storing the ""packet"" of
incremental deletes with the segment you just flushed and not against
the N segments that had deletes.  And you write only one file to hold
all the tombstones, which for commit() (file sync) is much less cost.

And it's great that we don't need a new merge policy to handle all the
delete files.

Though one possible downside is, for a very large segment in a very
large index you will likely be merging (at search time) quite a few
delete packets.  But, with the cutover to
deletes-accessed-only-by-iterator, this cost is probably not high
until a large pctg of the segment's docs are deleted, at which point
you should really expungeDeletes() or optimize() or optimize(int)
anyway.

If only we could write code as quickly as we can dream...
","08/Dec/08 21:58;jasonrutherglen;Marvin: 
""I'm also bothered by the proliferation of small deletions files. Probably
you'd want automatic consolidation of files under 4k, but you still could end
up with a lot of files in a big index.""

A transaction log might be better here if we want to go to 0ish millisecond realtime. 
On Windows at least creating files rapidly and deleting them creates significant IO overhead.
UNIX is probably faster but I do not know.

","08/Dec/08 22:06;jasonrutherglen;Wouldn't it be good to remove BitVector and replace it with OpenBitSet?  OBS is faster, has the DocIdSetIterator already.  It just needs to implement write to disk compression of the bitset (dgaps?).  This would be a big win for almost *all* searches.  We could also create an interface so that any bitset implementation could be used.

Such as:
{code}
public interface WriteableBitSet {
 public void write(IndexOutput output) throws IOException;
}
{code}","08/Jan/09 00:00;marvin;While converting over PostingList (the Lucy/KS analogue to TermDocs) to use a deletions iterator, it occurred to me that the because the iterator has to keep state, a unique DelEnum object has to be created for every PostingList. In contrast, a BitVector object, which is accessed only via get(), can be shared.

It bugs me that each PostingList will have its own DelEnum performing interleaving of tombstones.  With very large queries against indexes with large numbers of deletions, it seems like a lot of duplicated work.

To minimize CPU cycles, it would theoretically make more sense to handle deletions much higher up, at the top level Scorer, Searcher, or even the HitCollector level.  PostingList would be completely ignorant of deletions, as would classes like NOTScorer and MatchAllScorer:

{code}
i32_t
MatchAllScorer_next(MatchAllScorer* self) 
{
    if (++self->doc_num > self->max_docs) {
        self->doc_num--;
        return 0;
    }
    return self->doc_num;
}
{code}

{code}
void
Scorer_collect(Scorer *self, Hitcollector *collector, DelEnum *del_enum)
{
    i32_t next_deletion = del_enum ? 0 : I32_MAX;
    i32_t doc_num = 1;
    while (1) {
        while (doc_num >= next_deletion) {
            next_deletion = DelEnum_Skip_To(del_enum, target);
            while (doc_num == next_deletion) {
                doc_num++;
                next_deletion = DelEnum_Skip_To(del_enum, doc_num);
            }
        }
        doc_num = Scorer_Skip_To(scorer, doc_num);
        if (doc_num) {
            HC_Collect(collector, doc_num, Scorer_Tally(scorer));
        }
        else { 
            break; 
        }
    }
}
{code}

The problem is that PostingLists spun off from indexReader.postingList(field, term) would include deleted documents, as would Scorers.

I suppose you could band-aid indexReader.postingList() by adding a boolean suppressDeletions argument which would default to true, but that seems messy.

Jason, I think the inefficiency of needing individual iterator objects applies to OpenBitSet as well, right?  As soon as we do away with random access, we have to keep state.  Dunno if it's going to be noticeable, but it's conceptually annoying.","08/Jan/09 01:44;jasonrutherglen;I like this idea of tombstones and we should figure out a way to support it.  This issue https://issues.apache.org/jira/browse/LUCENE-584 had an implementation of a ""matcher"" class that the scorers implemented which I do not think made it into the committed patch.  

> I think the inefficiency of needing individual iterator objects applies to OpenBitSet as well, right? 

Yes that is true.  

For realtime search where a new transaction may only have a handful of deletes the tombstones may not be optimal because too many tombstones would accumulate (I believe).  For this scenario rolling bitsets may be better.  Meaning pool bit sets and throw away unused readers.  ","08/Jan/09 02:38;marvin;Jason Rutherglen:

> For realtime search where a new transaction may only have a handful of 
> deletes the tombstones may not be optimal 

The whole tombstone idea arose out of the need for (close to) realtime search!  It's intended to improve write speed.

When you make deletes with the BitSet model, you have to rewrite files that scale with segment size, regardless of how few deletions you make. Deletion of a single document in a large segment may necessitate writing out a substantial bit vector file. 

In contrast, i/o throughput for writing out a tombstone file scales with the number of tombstones.

> because too many tombstones would accumulate (I believe).

Say that you make a string of commits that are nothing but deleting a single document -- thus adding a new segment each time that contains nothing but a single tombstone.  Those are going to be cheap to merge, so it seems unlikely that we'll end up with an unwieldy number of tombstone streams to interleave at search-time.

The more likely problem is the one McCandless articulated regarding a large segment accumulating a lot of tombstone streams against it.  But I agree with him that it only gets truly serious if your merge policy neglects such segments and allows them to deteriorate for too long.

> For this scenario rolling bitsets may be better. Meaning pool bit sets and 
> throw away unused readers. 

I don't think I understand.  Is this the ""combination index reader/writer"" model, where the writer prepares a data structure that then gets handed off to the reader?","08/Jan/09 07:51;paul.elschot@xs4all.nl;Jason,

bq. This issue LUCENE-584 had an implementation of a ""matcher"" class that the scorers implemented which I do not think made it into the committed patch.

The only functional difference between the DocIdSetIterator as committed and the Matcher class at 584 is the Matcher.explain() method, which did not make it into DocIdSetIterator.","08/Jan/09 10:57;mikemccand;
{quote}
> PostingList would be completely ignorant of deletions, as would classes like NOTScorer and MatchAllScorer:
{quote}

This is a neat idea! Deletions are then applied just like a Filter.

For a TermQuery (one term) the cost of the two approaches should be
the same.

For OR'd Term queries, it actually seems like your proposed approach
may be lower cost?  Ie rather than each TermEnum doing the ""AND NOT
deleted"" intersection, you only do it once at the top.  There is added
cost in that each TermEnum is now returning more docIDs than before,
but the deleted ones are eliminated before scoring.

For AND (and other) queries I'm not sure.  In theory, having to
process more docIDs is more costly, eg a PhraseQuery or SpanXXXQuery
may see much higher net cost.  We should test.

Conceivably, a future ""search optimization phase"" could pick & choose
the best point to inject the ""AND NOT deleted"" filter.  In fact, it
could also pick when to inject a Filter... a costly per-docID search
with a very restrictive filter could be far more efficient if you
applied the Filter earlier in the chain.

I'm also curious what cost you see of doing the merge sort for every
search; I think it could be uncomfortably high since it's so
hard-for-cpu-to-predict-branch-intensive.  We could take the first
search that doesn't use skipTo and save the result of the merge sort,
essentially doing an in-RAM-only ""merge"" of those deletes, and let
subsequent searches use that single merged stream.  (This is not MMAP
friendly, though).

In my initial rough testing, I switched to iterator API for
SegmentTermEnum and found if %tg deletes is < 10% the search was a bit
faster using an iterator vs random access, but above that was slower.
This was with an already ""merged"" list of in-order docIDs.

Switching to an iterator API for accessing field values for many docs
(LUCENE-831 -- new FieldCache API, LUCENE-1231 -- column stride
fields) shouldn't have this same problem since it's the ""top level""
that's accessing the values (ie, one iterator per field X query).

","08/Jan/09 11:35;paul.elschot@xs4all.nl;bq. To minimize CPU cycles, it would theoretically make more sense to handle deletions much higher up, at the top level Scorer, Searcher, or even the HitCollector level.

How about a SegmentSearcher?","08/Jan/09 14:32;marvin;Paul Elschot:

> How about a SegmentSearcher?

I like the idea of a SegmentSearcher in general.  A little while back, I wondered whether exposing SegmentReaders was really the best way to handle segment-centric search.  Upon reflection, I think it is.  Segments are a good unit.  They're pure inverted indexes (notwithstanding doc stores and tombstones); the larger composite only masquerades as one.

I noticed that in one version of the patch for segment-centric search (LUCENE-1483), each sorted search involved the creation of sub-searchers, which were then used to compile Scorers. It would make sense to cache those as individual SegmentSearcher objects, no? 

And then, to respond to the original suggestion, the SegmentSearcher level seems like a good place to handle application of a deletions quasi-filter.  I think we could avoid having to deal with segment-start offsets that way. ","08/Jan/09 14:48;marvin;How about if we model deletions-as-iterator on BitSet.nextSetBit(int tick) instead of a true iterator that keeps state? 

You can do that now by implementing BitVector.nextSetBit(int tick) and using that in TermDocs to set a nextDeletion member var instead of checking every doc num with BitVector.get().

That way, the object that provides deletions can still be shared.","08/Jan/09 15:18;marvin;Mike McCandless:

> For a TermQuery (one term) the cost of the two approaches should be
> the same.

It'll be close, but I don't think that's quite true.  TermScorer pre-fetches
document numbers in batches from the TermDocs object.  At present, only
non-deleted doc nums get cached.  If we move the deletions filtering up, then
we'd increase traffic through that cache.  However, filling it would be
slightly cheaper, because we wouldn't be performing the deletions check.

In theory.  I'm not sure there's a way to streamline away that deletions check
in TermDocs and maintain backwards compatibility.  And while this is a fun
brainstorm, I'm still far from convinced that having TermDocs.next() and
Scorer.next() return deleted docs by default is a good idea.

> For AND (and other) queries I'm not sure. In theory, having to
> process more docIDs is more costly, eg a PhraseQuery or SpanXXXQuery
> may see much higher net cost.

If you were applying deletions filtering after Scorer.next(), then it seems
likely that costs would go up because of extra hit processing.  However, if
you use Scorer.skipTo() to jump past deletions, as in the loop I provided
above, then PhraseScorer etc. shouldn't incur any more costs themselves.

> a costly per-docID search
> with a very restrictive filter could be far more efficient if you
> applied the Filter earlier in the chain.

Under the skipTo() loop, I think the filter effectively *does* get applied
earlier in the chain.  Does that make sense?

I think the potential performance downside comes down to prefetching in
TermScorer, unless there are other classes that do similar prefetching.


","08/Jan/09 15:19;markrmiller@gmail.com;bq. I noticed that in one version of the patch for segment-centric search (LUCENE-1483), each sorted search involved the creation of sub-searchers, which were then used to compile Scorers. It would make sense to cache those as individual SegmentSearcher objects, no?

Thats a fairly old version I think (based on using MutliSearcher as a hack). Now we are using one queue and running it through each subreader of the MultiReader.","08/Jan/09 16:45;jasonrutherglen;Marvin: ""The whole tombstone idea arose out of the need for (close to) realtime search! It's intended to improve write speed.""

It does improve the write speed.  I found in making the realtime search write speed fast enough that writing to individual files per segment can become too costly (they accumulate fast, appending to a single file is faster than creating new files, deleting the files becomes costly).  For example, writing to small individual files per commit, if the number of segments is large and the delete spans multiple segments will generate many files.  This is variable based on how often the updates are expected to occur.  I modeled this after the extreme case of the frequency of updates of a MySQL instance backing data for a web application.

The MySQL design, translated to Lucene is a transaction log per index.  Where the updates consisting of documents and deletes are written to the transaction log file.  If Lucene crashed for some reason the transaction log would be replayed.  The in memory indexes and newly deleted document bitvectors would be held in RAM (LUCENE-1314) until flushed (the in memory indexes and deleted documents) manually or based on memory usage.  Many users may not want a transaction log as they may be storing the updates in a separate SQL database instance (this is the case where I work) and so a transaction log is redundant and should be optional.  The first implementation of this will not have a transaction log.

Marvin: ""I don't think I understand. Is this the ""combination index reader/writer"" model, where the writer prepares a data structure that then gets handed off to the reader?""

It would be exposed as a combination reader writer that manages the transaction status of each update.  The internal architecture is such that after each update a new reader representing the new documents and deletes for the transaction is generated and put onto a stack.  The reader stack is drained based on whether a reader is too old to be useful anymore (i.e. no references to it, or it's has N number of readers ahead of it).  ","08/Jan/09 19:11;marvin;Jason Rutherglen:

> I found in making the realtime search write speed fast enough that writing
> to individual files per segment can become too costly (they accumulate fast,
> appending to a single file is faster than creating new files, deleting the
> files becomes costly). 

I saw you mentioning i/o overhead on Windows in particular.  I can't see a way
to mod Lucene so that it doesn't generate a bunch of files for each commit,
and FWIW Lucy/KS is going to generate even more files than Lucene.

Half-seriously... how about writing a single-file Directory implementation?

> For example, writing to small individual files per commit, if the number of
> segments is large and the delete spans multiple segments will generate many
> files. 

There would be a maximum of two files per segment to hold the tombstones: one
to hold the tombstone rows, and one to map segment identifiers to tombstone
rows.  (In Lucy/KS, the mappings would probably be stored in the JSON-encoded
""segmeta"" file, which stores human-readable metadata on behalf of multiple 
components.)

Segments containing tombstones would be merged according to whatever merge
policy was in place.  So there won't ever be an obscene number of tombstone
files unless you allow an obscene number of segments to accumulate.

> Many users may not want a transaction log as they may be storing the updates
> in a separate SQL database instance (this is the case where I work) and so a
> transaction log is redundant and should be optional. 

I can see how this would be quite useful at the application level.  However, I
think it might be challenging to generalize the transaction log concept at the
library level:

{code}
CustomAnalyzer analyzer = new CustomAnalyzer();
IndexWriter indexWriter = new IndexWriter(analyzer, ""/path/to/index"");
indexWriter.add(nextDoc());
analyzer.setFoo(2); // change of state not recorded by transaction log
indexWriter.add(nextDoc());
{code}

MySQL is more of a closed system than Lucene, which I think makes options
available that aren't available to us.

> The reader stack is drained based on whether a reader is too old to be
> useful anymore (i.e. no references to it, or it's has N number of readers
> ahead of it).

Right, this is the kind of thing that Lucene has to do because of the
single-reader model, and that were trying to get away from in Lucy/KS by
exploiting mmap and making IndexReaders cheap wrappers around the system i/o
cache.

I don't think I can offer any alternative design suggestions that meet your
needs.   There's going to be a change rate that overwhelms the multi-file
commit system, and it seems that you've determined you're up against it.  

What's killing us is something different: not absolute change rate, but poor 
worst-case performance.

FWIW, we contemplated a multi-index system with an index on a RAM disk for
fast changes and a primary index on the main file system.  It would have
worked fine for pure adds, but it was very tricky to manage state for
documents which were being ""updated"", i.e.  deleted and re-added.  How are you
handling all these small adds with your combo reader/writer?  Do you not have
that problem?","08/Jan/09 19:43;mikemccand;{quote}
> There's going to be a change rate that overwhelms the multi-file
> commit system, and it seems that you've determined you're up against
> it.
{quote}

Well... IndexWriter need not ""commit"" in order to allow a reader to
see the files?

Commit is for crash recovery, and for knowing when it's OK to delete
prior commits.  Simply writing the files (and not syncing them), and
perhaps giving IndexReader.open the SegmentInfos to use directly (and
not writing a segments_N via the filesystem) would allow us to search
added docs without paying the cost of sync'ing all the files.

Also: brand new, tiny segments should be written into a RAMDirectory
and then merged over time into the real Directory.
","08/Jan/09 19:48;mikemccand;{quote}
> If Lucene crashed for some reason the transaction log would be replayed.
{quote}

I think the transaction log is useful for some applications, but could
(should) be built as a separate (optional) layer entirely on top of
Lucene's core.  Ie, neither IndexWriter nor IndexReader need to be
aware of the transaction log, which update belongs to which
transaction, etc?
","08/Jan/09 20:00;mikemccand;
{quote}
> It would be exposed as a combination reader writer that manages the transaction status of each update.
{quote} 

I think the transactions layer would also sit on top of this
""realtime"" layer?  EG this ""realtime"" layer would expose a commit()
method, and the transaction layer above it would maintain the
transaction log, periodically calling commit() and truncating the
transaction log?

This ""realtime"" layer, then, would internally maintain a single
IndexWriter and the readers.  IndexWriter would flush (not commit) new
segments into a RAMDir and yield its in-RAM SegmentInfos to
IndexReader.reopen.  MergePolicy periodically gets those into the real
Directory.  When reopening a reader we have the freedom to use old
(already merged away) segments if the newly merged segment isn't warm
yet.

We ""just"" need to open some things up in IndexWriter:
 
  * IndexReader.reopen with the in-RAM SegmentInfos

  * Willingness to allow an IndexReader to maintain & updated deleted
    docs even though IndexWriter has the write lock

  * Access to segments that were already merged away (I think we could
    make a DeletionPolicy that pays attention to when the newly merged
    segment is not yet warmed and keeps thue prior segments around).
    I think this'd require allowing DeletionPolicy to see ""flush
    points"" in addition to commit points (it doesn't today).

But I'm still hazy on the details on exactly how to open up
IndexWriter.
","08/Jan/09 20:05;marvin;Mike McCandless:

> I'm also curious what cost you see of doing the merge sort for every
> search; I think it could be uncomfortably high since it's so
> hard-for-cpu-to-predict-branch-intensive. 

Probably true.  You're going to get accelerating degradation as the number of
deletions increases.  In a large index, you could end up merging 20, 30
streams.  Based on how the priority queue in ORScorer tends to take up space
in profiling data, that might not be good.

It'd be manageable if you can keep your index reasonably in good shape, but you'll 
be suckin' pondwater if it gets flabby.

> We could take the first search that doesn't use skipTo and save the result
> of the merge sort, essentially doing an in-RAM-only ""merge"" of those
> deletes, and let subsequent searches use that single merged stream. 

That was what I had in mind when proposing the pseudo-iterator model.

{code}
class TombStoneDelEnum extends DelEnum {
  int nextDeletion(int docNum) {
    while (currentMax < docNum) { nextInternal(); }
    return bits.nextSetBit(docNum);
  }
  // ...
}
{code}

> (This is not MMAP friendly, though).

Yeah.  Ironically, that use of tombstones is more compatible with the Lucene
model. :-)

I'd be reluctant to have Lucy/KS realize those large BitVectors in per-object 
process RAM.  That'd spoil the ""cheap wrapper around system i/o cache"" 
IndexReader plan.

I can't see an answer yet.  But the one thing I do know is that Lucy/KS needs
a pluggable deletions mechanism to make experimentation easier -- so that's
what I'm working on today.","08/Jan/09 20:09;mikemccand;{quote}
> If we move the deletions filtering up, then we'd increase traffic through that cache
{quote}

OK, right.  So we may have some added cost because of this.  I think
it's only TermScorer that uses the bulk API though.

{quote}
> If you were applying deletions filtering after Scorer.next(), then it seems
> likely that costs would go up because of extra hit processing. However, if
> you use Scorer.skipTo() to jump past deletions, as in the loop I provided
> above, then PhraseScorer etc. shouldn't incur any more costs themselves.
{quote}

Ahhh, now I got it!  Good, you're right.

{quote}
> Under the skipTo() loop, I think the filter effectively does get applied
> earlier in the chain. Does that make sense?
{quote}

Right.  This is how Lucene works today.  Excellent.

So, net/net it seems like ""deletes-as-a-filter"" approach is compelling?
","08/Jan/09 20:11;mikemccand;{quote}
> How about if we model deletions-as-iterator on BitSet.nextSetBit(int tick) instead of a true iterator that keeps state? 
{quote}
That works if under-the-hood it's a non-sparse representation.  But if it's sparse, you need an iterator (state) to remember where you are.","08/Jan/09 20:28;marvin;Mike McCandless:

> Commit is for crash recovery, and for knowing when it's OK to delete
> prior commits. Simply writing the files (and not syncing them), and
> perhaps giving IndexReader.open the SegmentInfos to use directly (and
> not writing a segments_N via the filesystem) would allow us to search
> added docs without paying the cost of sync'ing all the files.

Mmm.  I think I might have given IndexWriter.commit() slightly different
semantics.  Specifically, I might have given it a boolean ""sync"" argument
which defaults to false.

> Also: brand new, tiny segments should be written into a RAMDirectory
> and then merged over time into the real Directory.

Two comments.  First, if you don't sync, but rather leave it up to the OS when
it wants to actually perform the actual disk i/o, how expensive is flushing? Can 
we make it cheap enough to meet Jason's absolute change rate requirements?

Second, the multi-index model is very tricky when dealing with ""updates"".  How
do you guarantee that you always see the ""current"" version of a given
document, and only that version?  When do you expose new deletes in the
RAMDirectory, when do you expose new deletes in the FSDirectory, how do you
manage slow merges from the RAMDirectory to the FSDirectory, how do you manage
new adds to the RAMDirectory that take place during slow merges...

Building a single-index, two-writer model that could handle fast updates while
performing background merging was one of the main drivers behind the tombstone
design.
","08/Jan/09 20:33;marvin;Mike McCandless:

> if it's sparse, you need an iterator (state) to remember where you are.

We can hide the sparse representation and the internal state, having the
object lazily build the a non-sparse representation.  That's what I had in
mind with the code for TombstoneDelEnum.nextDeletion().
TombstoneDelEnum.nextInternal() would be a private method used for building up
the internal BitVector.","08/Jan/09 22:11;marvin;Mike McCandless:

> So, net/net it seems like ""deletes-as-a-filter"" approach is compelling?

In terms of CPU-cycles, maybe.  

My gut tells me that it's all but mandatory if we use merged-on-the-fly
tombstone streams, but if Lucene goes that route it should cache a BitVector
and use a shared pseudo-iterator -- in which case the costs will no longer be
significantly more than the current system.

Under the current system, I'm not certain that the deletions checks are that
excessive.  Consider this loop from TermDocs.read():

{code}
    while (i < length && count < df) {
      // manually inlined call to next() for speed
      final int docCode = freqStream.readVInt();
      doc += docCode >>> 1;       // shift off low bit
      if ((docCode & 1) != 0)       // if low bit is set
        freq = 1;         // freq is one
      else
        freq = freqStream.readVInt();     // else read freq
      count++;

      if (deletedDocs == null || !deletedDocs.get(doc)) {
        docs[i] = doc;
        freqs[i] = freq;
        ++i;
      }
    }
{code}

The CPU probably does a good job of predicting the result of the null check on
deletedDocs.  The readVInt() method call is already a pipeline killer.

Here's how that loop looks after I patch the deletions check for pseudo-iteration.

{code}
      while (i < length && count < df) {
        // manually inlined call to next() for speed
        final int docCode = freqStream.readVInt();
        doc += docCode >>> 1;       // shift off low bit
        if ((docCode & 1) != 0)       // if low bit is set
          freq = 1;         // freq is one
        else
          freq = freqStream.readVInt();     // else read freq
        count++;

        if (docNum >= nextDeletion) {
            if (docNum > nextDeletion) {
              nextDeletion = deletedDocs.nextSetBit(docNum);
            }
            if (docNum == nextDeletion) {
              continue;
            }
        }

        docs[i] = doc;
        freqs[i] = freq;
        ++i;
      }
      return i;
    }
{code}

Again, the CPU is probably going to do a pretty good job of predicting the
results of the deletion check.  And even then, we're accessing the same shared
BitVector across all TermDocs, and its bits are hopefully a cache hit.

To really tighten this loop, you have to do what Nate and I want with Lucy/KS:

  * Remove all function/method call overhead.
  * Operate directly on the memory mapped postings file.

{code}
SegPList_bulk_read(SegPostingList *self, i32_t *doc_nums, i32_t *freqs,
                   u32_t request)
{
    i32_t       doc_num   = self->doc_num;
    const u32_t remaining = self->doc_freq - self->count;
    const u32_t num_got   = request < remaining ? request : remaining;
    char       *buf       = InStream_Buf(instream, C32_MAX_BYTES * num_got);
    u32_t       i;

    for (i = 0; i < num_got; i++) {
        u32_t doc_code = Math_decode_c32(&buf); /* static inline function */
        u32_t freq     = (doc_code & 1) ? 1 : Math_decode_c32(&buf);
        doc_num        += doc_code >> 1; 
        doc_nums[i]    = doc_num;
        freqs[i]       = freq;
        ++i;
    }

    InStream_Advance_Buf(instream, buf);
    self->doc_num = doc_num;
    self->count += num_got;

    return num_got;
}
{code}

(That loop would be even better using PFOR instead of vbyte.)

In terms of public API, I don't think it's reasonable to change Lucene's
Scorer and TermDocs classes so that their iterators start returning deleted
docs.  

We could potentially make that choice with Lucy/KS, thus allowing us to remove
the deletions check in the PostingList iterator (as above) and getting a
potential speedup.  But even then I hesitate to push the deletions API upwards
into a space where users of raw Scorer and TermDocs classes have to deal with
it -- especially since iterator-style deletions aren't very user-friendly.
","09/Jan/09 01:40;jasonrutherglen;M.M.:"" I think the transactions layer would also sit on top of this
""realtime"" layer? EG this ""realtime"" layer would expose a commit()
method, and the transaction layer above it would maintain the
transaction log, periodically calling commit() and truncating the
transaction log?""

One approach that may be optimal is to expose from IndexWriter a createTransaction method that accepts new documents and deletes.  All documents have an associated UID.  The new documents could feasibly be encoded into a single segment that represents the added documents for that transaction.  The deletes would be represented as document long UIDs rather than int doc ids.  Then the commit method would be called on the transaction object who returns a reader representing the latest version of the index, plus the changes created by the transaction.  This system would be a part of IndexWriter and would not rely on a transaction log.  IndexWriter.commit would flush the in ram realtime indexes to disk.  The realtime merge policy would flush based on the RAM usage or number of docs.

{code}
IndexWriter iw = new IndexWriter();
Transaction tr = iw.createTransaction();
tr.addDocument(new Document());
tr.addDocument(new Document());
tr.deleteDocument(1200l);
IndexReader ir = tr.flush(); // flushes transaction to the index (probably to a ram index)
IndexReader latestReader = iw.getReader(); // same as ir
iw.commit(boolean doWait); // commits the in ram realtime index to disk
{code}

When commit is called, the disk segment reader flush their deletes to disk which is fast.  The in ram realtime index is merged to disk.  The process is described in more detail further down.

M.H.: ""how about writing a single-file Directory implementation?""

I'm not sure we need this because and appending rolling transaction log should work.  Segments don't change, only things like norms and deletes which can be appended to a rolling transaction log file system.  If we had a generic transaction logging system, the future column stride fields, deletes, norms, and future realtime features could use it and be realtime.  

M.H.: ""How do you guarantee that you always see the ""current"" version of a given document, and only that version? 

Each transaction returns an IndexReader.  Each ""row"" or ""object"" could use a unique id in the transaction log model which would allow documents that were merged into other segments to be deleted during a transaction log replay.  

M.H.: ""When do you expose new deletes in the RAMDirectory, when do you expose new deletes in the FSDirectory""

When do you expose new deletes in the RAMDir, when do you expose new deletes in the FSDirectory, how do you manage slow merges from the RAMDir to the FSDirectory, how do you manage new adds to the RAMDir that take place during slow merges...""

Queue deletes to the RAMDir, while copying the RAMDir to the FSDir in the background, perform the deletes after the copy is completed, then instantiate a new reader with the newly merged FSDirectory and a new RAMDirs.  Writes that were occurring during this process would be happening to another new RAMDir.  

One way to think of the realtime problem is in terms of segments rather than FSDirs and RAMDirs.  Some segments are on disk, some in RAM.  Each transaction is an instance of some segments and their deletes (and we're not worried about the deletes being flushed or not so assume they exist as BitVectors).  The system should expose an API to checkpoint/flush at a given transaction level (usually the current) and should not stop new updates from happening.    

When I wrote this type of system, I managed individual segments outside of IndexWriter's merge policy and performed the merging manually by placing each segment in it's own FSDirectory (the segment size was 64MB) which minimized the number of directories.  I do not know the best approach for this when performed within IndexWriter.  

M.H.: ""Two comments. First, if you don't sync, but rather leave it up to the OS when
it wants to actually perform the actual disk i/o, how expensive is flushing? Can
we make it cheap enough to meet Jason's absolute change rate requirements?""

When I tried out the transaction log a write usually mapped pretty quickly to a hard disk write.  I don't think it's safe to leave writes up to the OS.

M.M.: ""maintain & updated deleted docs even though IndexWriter has the write lock""

In my previous realtime search implementation I got around this by having each segment in it's own directory.  Assuming this is non-optimal, we will need to expose an IndexReader that has the writelock of the IndexWriter.    
","09/Jan/09 03:36;marvin;Here's a patch implementing BitVector.nextSetBit() and converting
SegmentTermDocs over to use the quasi-iterator style. Tested but 
not benchmarked.","09/Jan/09 05:31;cutting;bq. To really tighten this loop, you have to [ ... ] remove all function/method call overhead [and] operate directly on the memory mapped postings file.

That sounds familiar...

http://svn.apache.org/viewvc/lucene/java/trunk/src/gcj/org/apache/lucene/index/GCJTermDocs.cc?view=markup
","09/Jan/09 11:58;mikemccand;{quote}
> Mmm. I think I might have given IndexWriter.commit() slightly different
> semantics. Specifically, I might have given it a boolean ""sync"" argument
> which defaults to false.
{quote}

It seems like there are various levels of increasing ""durability"" here:

  * Make available to a reader in the same JVM (eg flush new segment
    to a RAMDir) -- not exposed today.

  * Make available to a reader sharing the filesystem right now (flush
    to Directory in ""real"" filesystem, but don't sync) -- exposed
    today (but deprecated as a public API) as flush.

  * Make available to readers even if OS/machine crashes (flush to
    Directory in ""real"" filesystem, and sync) -- exposed today as commit.

{quote}
> Two comments. First, if you don't sync, but rather leave it up to the OS when
> it wants to actually perform the actual disk i/o, how expensive is flushing? Can
> we make it cheap enough to meet Jason's absolute change rate requirements?
{quote}

Right I've been wondering the same thing.  I think this should be our
first approach to realtime indexing, and then we swap in RAMDir if
performance is not good enough.

{quote}
> Second, the multi-index model is very tricky when dealing with ""updates"". How
> do you guarantee that you always see the ""current"" version of a given
> document, and only that version? When do you expose new deletes in the
> RAMDirectory, when do you expose new deletes in the FSDirectory, how do you
> manage slow merges from the RAMDirectory to the FSDirectory, how do you manage
> new adds to the RAMDirectory that take place during slow merges...
>
> Building a single-index, two-writer model that could handle fast updates while
> performing background merging was one of the main drivers behind the tombstone
> design.
{quote}

I'm not proposing multi-index model (at least I think I'm not!).  A
single IW could flush new tiny segments into a RAMDir and later merge
them into a real Dir.  But I agree: let's start w/ a single Dir and
move to RAMDir only if necessary.

{quote}
> Building a single-index, two-writer model that could handle fast updates while
> performing background merging was one of the main drivers behind the tombstone
> design.
{quote}

I think carrying the deletions in RAM (reopening the reader) is
probably fastest for Lucene.  Lucene with the ""reopened stream of
readers"" approach can do this, but Lucy/KS (with mmap) must use
filesystem as the intermediary.
","09/Jan/09 12:01;mikemccand;
{quote}
> We can hide the sparse representation and the internal state, having the
> object lazily build the a non-sparse representation. That's what I had in
> mind with the code for TombstoneDelEnum.nextDeletion().
> TombstoneDelEnum.nextInternal() would be a private method used for building up
> the internal BitVector.
{quote}

Got it, though for a low deletion rate presumably you'd want to store
the int docIDs directly so iterating through them doesn't require O(N)
scan for the next set bit.

I think what you'd want to lazily do is merge the N tombstone streams
for this one segment into a single data structure; whether that data
structure is sparse or unsparse is a separate decision.
","09/Jan/09 12:04;mikemccand;{quote}
> We could potentially make that choice with Lucy/KS, thus allowing us to remove
> the deletions check in the PostingList iterator (as above) and getting a
> potential speedup. But even then I hesitate to push the deletions API upwards
> into a space where users of raw Scorer and TermDocs classes have to deal with
> it - especially since iterator-style deletions aren't very user-friendly.
{quote}

Can't you just put sugar on top?  Ie, add an API that matches today's
API and efficiently applies the ""deleted docs filter"" for you?  Then
you have 100% back compat?
","09/Jan/09 12:08;mikemccand;
{quote}
> Under the current system, I'm not certain that the deletions checks are that
> excessive. 
{quote}

I made a simple test that up-front converted the deleted docs
BitVector into an int[] containing the deleted docsIDs, and then did
the same nextDeletedDoc change to SegmentTermDocs.

This was only faster if < 10% of docs were deleted, though I didn't do
very thorough testing.

I think the problem is with this change the CPU must predict a new
branch (docNum >= nextDeletion) vs doing a no-branching lookup of the
bit.
","09/Jan/09 13:28;mikemccand;
{quote}
> One way to think of the realtime problem is in terms of segments rather than FSDirs and RAMDirs. Some segments are on disk, some in RAM.
{quote}

I think that's a good approach.  IndexWriter could care less which dir
each segment resides in.

For starters let's flush all new segments into a single Directory (and
swap in RAMDir, single-file-Dir, etc., if/when necessary for better
performance).

Thinking out loud.... what if we added IndexWriter.getReader(), which
returns an IndexReader for the current index?

It would read segments that are not ""visible"" to a newly opened
IndexReader on that directory (ie, the SegmentInfos inside IndexWriter
was used to do the open, not the latest segments_N in the Directory).

That IndexReader is free to do deletions / set norms (shares write
lock under the hood w/ IndexWriter), and when reopen is called it
grabs the current SegmentInfos from IndexWriter and re-opens that.

We would un-deprecate flush().  The, the transaction layer would make
changes, call flush(), call reopen(), and return the reopened reader.
","14/Jan/09 17:50;jasonrutherglen;To simplify the patch, can we agree to add a method,
IndexReader.getDeletedDocs that returns a DocIdSet? This way
applications may have access to deleteDocs and not be encumbered by
the IR.isDeleted method.","14/Jan/09 18:36;yseeley@gmail.com;bq. IndexReader.getDeletedDocs that returns a DocIdSet

Seems like a good idea, but the difficulty lies in defining the semantics of such a call.
Are subsequent deletes reflected in the returned DocIdSet?  Perhaps make this undefined - may or may not be.
Can the DocIdSet be used concurrently with IndexReader.deleteDocument()?  This could work (w/o extra undesirable synchronization) if we're careful.","14/Jan/09 22:01;jasonrutherglen;I think it should be up to the user. If the user concurrently
modifies then they're responsible for the possibly spurious effects.
However if we want to be protective, a philosophy I don't think works
well in LUCEN-1516 either, we can offer IR.getDeletedDocs only from a
read only index reader. This solves the issues brought up such as
""undesirable synchronization"".","15/Jan/09 10:42;mikemccand;I think it's fine to expose this API, mark it expert & subject to
change, and state that it simply returns the current deleted docs
BitVector and any synchronization issues must be handled by the app.
Probably it should be package private or protected.

Also, I think Multi*Reader would not implement this API?  Meaning
you must get  the individual SegmentReader and ask it.

Since we are discussing possible changes to deleted docs, eg switching
to iterator-only accesss, maybe applying deletions as a filter, maybe
using ""tombstones"" under-the-hood, etc., this API could very well
change.

For our current thinking on realtime search, I think one weak part of
the design is the blocking copy-on-write required for the first
deletion on a newly cloned reader.  The time taken is in proportion to
the size of the segment.  Switching to a design that can background
this copy-on-write will necessarily impact how we represent
and access deletions.
","16/Jan/09 17:05;jasonrutherglen;{quote}
it's fine to expose this API, mark it expert & subject to
change, and state that it simply returns the current deleted docs
BitVector and any synchronization issues must be handled by the app.
Probably it should be package private or protected. 
{quote}

+1 Sounds good

{quote}
Also, I think Multi*Reader would not implement this API? Meaning
you must get the individual SegmentReader and ask it.
{quote}

Why?  The returned iterator can traverse the multiple bitvectors.

{quote}
I think one weak part of
the design is the blocking copy-on-write required for the first
deletion on a newly cloned reader. The time taken is in proportion to
the size of the segment. 
{quote}

If the segment is large, tombstones can solve this.","16/Jan/09 17:19;mikemccand;bq. Why? The returned iterator can traverse the multiple bitvectors.

Woops, sorry: I missed that it would return a DocIdSet (iterator only) vs underlying (current) BitVector.  So then MultiReader could return a DocIdSet.

bq. If the segment is large, tombstones can solve this.

Right; I was saying, as things are today (single BitVector holds all deleted docs), one limitation of the realtime approach we are moving towards is the copy-on-write cost of the first delete on a freshly cloned reader for a large segment.

If we moved to using only iterator API for accessing deleted docs within Lucene then we could explore fixes for the copy-on-write cost w/o changing on-disk representation of deletes.  IE tombstones are perhaps overkill for Lucene, since we're not using the filesystem as the intermediary for communicating deletes to a reopened reader.  We only need an in-RAM incremental solution.","16/Jan/09 17:28;jasonrutherglen;{quote}
If we moved to using only iterator API for accessing deleted docs within Lucene then we could explore fixes for the copy-on-write cost w/o changing on-disk representation of deletes. IE tombstones are perhaps overkill for Lucene, since we're not using the filesystem as the intermediary for communicating deletes to a reopened reader. We only need an in-RAM incremental solution.
{quote}

+1 Agreed.  Good point about not needing to change the on disk representation as that would make implementation a bit more complicated.  Sounds like we need a tombstones patch as well that plays well with IndexReader.clone.

Exposing deleted docs as a DocIdSet allows possible future implementations that DO return deleted docs as discussed (via a flag to IndexReader) from TermDocs.  Deleted docs DocIdSet can then be used on a higher level as a filter/query.  ","16/Jan/09 23:22;jasonrutherglen;Do we want to implement SegmentTermDocs using DocIdSet in another patch?","21/Jan/09 01:41;jasonrutherglen;First cut.  All tests pass.

* Implemented IndexReader.getDeletedDocs in all readers

* Created MultiDocIdSet that iterates over multiple DocIdSets which is
used by MultiSegmentReader and MultiReader

* Incorporated Marvin's patch into SegmentTermDocs and BitVector.
SegmentTermDocs iterates using deleted docs DocIdSet instead of
calling BitVector.get. ","21/Jan/09 02:29;marvin;Jason,

> Incorporated Marvin's patch into SegmentTermDocs and BitVector.

I believe there's an inefficiency in my original patch.  Code like this occurs in three places:

{code}
+      if (doc >= nextDeletion) {
+        if (doc > nextDeletion) 
+          nextDeletion = deletedDocs.nextSetBit(doc);
+        if (doc == nextDeletion)
+          continue;
       }
{code}

While technically correct, extra work is being done.  When nextSetBit() can't
find any more bits, it returns -1 (just like java.util.BitSet.nextSetBit() does).  
This causes the deletion loop to be checked on each iteration.

The solution is to test for -1, as in the new version of the patch (also
tested but not benchmarked):

{code}
+      if (doc >= nextDeletion) {
+        if (doc > nextDeletion) {
+          nextDeletion = deletedDocs.nextSetBit(doc);
+          if (nextDeletion == -1) {
+            nextDeletion = Integer.MAX_VALUE;
+          }
+        }
+        if (doc == nextDeletion) {
+          continue;
+        }
       }
{code}

Theoretically, we could also change the behavior of nextSetBit() so that it
returns Integer.MAX_VALUE when it can't find any more set bits.  However,
that's a little misleading (since it's a positive number and could thus
represent a true set bit), and also would break the intended API mimicry by
BitVector.nextSetBit() of java.util.BitSet.nextSetBit().","21/Jan/09 04:22;marvin;Mike McCandless:

>> If we move the deletions filtering up, then we'd increase traffic through
>> that cache
>
> OK, right. So we may have some added cost because of this. I think it's only
> TermScorer that uses the bulk API though.

The original BooleanScorer also pre-fetches.  (That doesn't affect KS because
ORScorer, ANDScorer, NOTScorer and RequiredOptionScorer (which have
collectively replaced BooleanScorer) all proceed doc-at-a-time and implement
skipping.)

And I'm still not certain it's a good idea from an API standpoint: it's
strange to have the PostingList and Scorer iterators included deleted docs.

Nevertheless, I've now changed over KS to use the deletions-as-filter approach
in svn trunk. The tie-breaker was related to the ongoing modularization of
IndexReader: if PostingList doesn't have to handle deletions, then
PostingsReader doesn't have to know about DeletionsReader, and if
PostingsReader doesn't have to know about DeletionsReader, then all
IndexReader sub-components can be implemented independently.

The code to implement the deletion skipping turned out to be more verbose and
fiddly than anticipated.  It's easy to make fencepost errors when dealing with
advancing two iterators in sync, especially when you can only invoke the
skipping iterator method once for a given doc num.

{code}
void
Scorer_collect(Scorer *self, HitCollector *collector, DelEnum *deletions,
               i32_t doc_base)
{
    i32_t   doc_num        = 0;
    i32_t   next_deletion  = deletions ? 0 : I32_MAX;

    /* Execute scoring loop. */
    while (1) {
        if (doc_num > next_deletion) {
            next_deletion = DelEnum_Advance(deletions, doc_num);
            if (next_deletion == 0) { next_deletion = I32_MAX; }
            continue;
        }
        else if (doc_num == next_deletion) {
            /* Skip past deletions. */
            while (doc_num == next_deletion) {
                /* Artifically advance scorer. */
                while (doc_num == next_deletion) {
                    doc_num++;
                    next_deletion = DelEnum_Advance(deletions, doc_num);
                    if (next_deletion == 0) { next_deletion = I32_MAX; }
                }
                /* Verify that the artificial advance actually worked. */
                doc_num = Scorer_Advance(self, doc_num);
                if (doc_num > next_deletion) {
                    next_deletion = DelEnum_Advance(deletions, doc_num);
                }
            }
        }
        else {
            doc_num = Scorer_Advance(self, doc_num + 1);
            if (doc_num >= next_deletion) { 
                next_deletion = DelEnum_Advance(deletions, doc_num);
                if (doc_num == next_deletion) { continue; }
            }
        }

        if (doc_num) {
            HC_Collect(collector, doc_num + doc_base, Scorer_Tally(self));
        }
        else { 
            break; 
        }
    }
}
{code}

","21/Jan/09 07:36;paul.elschot@xs4all.nl;>>> If we move the deletions filtering up, then we'd increase traffic through
>>> that cache
>>
>> OK, right. So we may have some added cost because of this. I think it's only
>> TermScorer that uses the bulk API though.

>The original BooleanScorer also pre-fetches. (That doesn't affect KS because
> ORScorer, ANDScorer, NOTScorer and RequiredOptionScorer (which have
> collectively replaced BooleanScorer) all proceed doc-at-a-time and implement
> skipping.)

For OR like queries, that use Scorer.next(), deletions might be treated as early
as possible, since each hit will cause a matching doc and a corresponding score calculation.

For AND like queries, that use Scorer.skipTo(), deletions can be treated later,
for example in skipTo() of the conjunction/and/scorer.

In the same way, prefetching into a larger term documents buffer helps for OR queries,
but gets in the way for AND queries.
","21/Jan/09 17:58;marvin;> For OR like queries, that use Scorer.next(), deletions might be treated as
> early as possible, since each hit will cause a matching doc and a
> corresponding score calculation.

It depends whether the OR-like query spawns a Scorer that pre-fetches.

At present, deletions are handled in Lucene at the SegmentTermDocs level.
Filtering deletions that early minimizes overhead from *all* Scorer.next()
implementations, including those that pre-fetch like the original
BooleanScorer.  The cost is that we must check every single posting in every
single SegmentTermDocs to see if the doc has been deleted.

The Scorer_Collect() routine above, which skips past deletions, consolidates
the cost of checking every single posting into one check.  However, the
original BooleanScorer can't skip, so it will incur overhead from scoring
documents which have been deleted.  Do the savings and the additional costs
balance each other out?  Its hard to say.

However, with OR-like Scorers which implement skipping and don't pre-fetch --
Lucene's DisjunctionSumScorer and its KS analogue ORScorer -- you get the best
of both worlds.  Not only do you avoid the per-posting-per-SegTermDocs
deletions check, you get to skip past deletions and avoid the extra overhead
in Scorer.next().

Of course the problem is that because ScorerDocQueue isn't very efficient,
DisjunctionSumScorer and ORScorer are often slower than the distributed sort
in the original BooleanScorer.

> For AND like queries, that use Scorer.skipTo(), deletions can be treated
> later, for example in skipTo() of the conjunction/and/scorer.

Yes, and it could be implemented by adding a Filter sub-clause to the
conjunction/and/scorer.

","21/Jan/09 18:27;jasonrutherglen;{quote}
M.M.: I believe there's an inefficiency in my original patch.
{quote}

Marvin, I implemented the SegmentTermsDocs using DocIdSetIterator.skipTo rather than nextSetBit.  Do you think the skipTo can be optimized?","21/Jan/09 19:24;marvin;> I implemented the SegmentTermsDocs using DocIdSetIterator.skipTo rather than
> nextSetBit. 

Ah, I see.  And you handled the -1 value properly.  Well done.

> Do you think the skipTo can be optimized?

Provided that the compiler is clever enough to inline the various methods
within BitVectorDocIdSetIterator (it should be), I don't see a way to improve
on that part.

It's probably possible to optimize this loop from my patch in
BitVector.nextSetBit(), by retrieving the byte value and operating directly on
it rather than calling get().  (The compiler would have to be quite clever to
figure out that optimization on its own.)

{code}
+        // There's a non-zero bit in this byte. 
+        final int base = pos * 8;
+        for (int i = 0; i < 8; i++) {
+          final int candidate = base + i;
+          if (candidate < size && candidate >= bit) {
+            if (get(candidate)) {
+              return candidate;
+            }
+          }
+        }
{code}

At a higher level, I'd imagine we'd want to store the deleted doc IDs as an
integer array rather than a BitVector if there aren't very many of them.  But
I think that will mess with the random access required by
IndexReader.isDeleted().","21/Jan/09 19:30;jasonrutherglen;Previous patch did not have MultiDocIdSet, this one does.","21/Jan/09 19:38;jasonrutherglen;{quote}
M.M.: ""At a higher level, I'd imagine we'd want to store the deleted doc IDs as an
integer array rather than a BitVector if there aren't very many of them. But
I think that will mess with the random access required by
IndexReader.isDeleted().""
{quote}

Indeed, how should we solve isDeleted for the tombstones
implementation? Or do we simply assume it will be slow (requiring a
linear scan?) Or perhaps borrow from Solr's HashDocSet (a minimal
primitive int based hash) to implement?","21/Jan/09 19:54;jasonrutherglen;I am going to run the contrib benchmark tests on this patch vs trunk to see if there is a difference in performance.  Do the benchmarks have deleted docs?","21/Jan/09 19:57;mikemccand;{quote}
Indeed, how should we solve isDeleted for the tombstones
implementation?
{quote}

Should we deprecate isDeleted?  And also deprecate document() checking whether a document is deleted (ie switch to a new API that returns document w/o checking deletions).

Or perhaps move isDeleted to a new API that makes it clear that there is a performance cost to random-access of deletions?  (And the first time it's called, it materializes a full bit vector).","21/Jan/09 21:08;mikemccand;
bq. I am going to run the contrib benchmark tests on this patch vs trunk to see if there is a difference in performance

I think this is the most important next step before we go too much
further.

There are two separate questions we are exploring here:

  1) Should we switch to iterator only API for accessing deleted docs
     (vs random-access API we use today)?

  2) Should we take deletes into account at a higher level
     (BooleanScorer/2, as a top-level Filter) vs the lowest level
     (each posting, in SegmentTermDocs that we do today)?

We need to understand the performance impact with each combination of
1) and 2), across different queries.  Likely the answer will be ""it
depends on the query"", but we should understand just how much.  I
think performance cost/benefit is the driver here,
","21/Jan/09 23:01;jasonrutherglen;Is there info on the wiki about how to use the contrib/benchmarks? (didn't find any)

{quote}
M.M.: ""1) Should we switch to iterator only API for accessing deleted docs
(vs random-access API we use today)?""
{quote}

I think we can by default support both. We may want to test (later
on) the difference in speed of iterating over an OpenBitSet vs
BitVector for the deleted docs.

{quote}
M.M.: ""2) Should we take deletes into account at a higher level
(BooleanScorer/2, as a top-level Filter) vs the lowest level (each
posting, in SegmentTermDocs that we do today)?""
{quote}

We probably need to support both as implementing top level deleted
docs filtering may have unknown side effects. The user may decide
based on their queries and other variables such as the number of
deleted docs. 
","22/Jan/09 01:44;jasonrutherglen;{quote} 
Should we deprecate isDeleted? And also deprecate document()
checking whether a document is deleted (ie switch to a new API that
returns document w/o checking deletions). 
{quote}

Deprecating isDeleted might be good. Would we need the read only
readers? We can still offer get method access to the deleted docs if
it's a bit set by creating an abstract class DocIdBitSet that
BitVector, java.util.BitSet, and OpenBItSet implement. This can
happen by casting IR.getDeletedDocs to DocIdBItSet. 

{quote} 
Or perhaps move isDeleted to a new API that makes it clear
that there is a performance cost to random-access of deletions? (And
the first time it's called, it materializes a full bit vector).
{quote}

Seems best to keep deleted docs access to the DocIdSet and DocIdBitSet
and not have IR.isDeleted.","23/Jan/09 20:17;mikemccand;
{quote}
We probably need to support both as implementing top level deleted
docs filtering may have unknown side effects. The user may decide
based on their queries and other variables such as the number of
deleted docs.
{quote}

I agree... and then, if the performance difference is large enough, it
seems like we'll need some simple ""search policy"" for the interesting
(Boolean) query scorers to pick the best way to execute a query.

This could include which order to visit the segments in (we broached
this in LUCENE-1483, since depending on the query different orders may
perform better).  And when (high vs low) & how (iterator vs random
access) to apply a filter would also be decided by the search policy.

bq. Deprecating isDeleted might be good.

I wonder how this method is used [externally] by applications,
today... I'll go ask on java-user.  And, whether all such uses could
migrate to an iterator API instead without much cost.

bq. Would we need the read only readers?

Good question... I'm guessing there would still be a performance
benefit if the underlying data structures for deletions &
column-stride fields know they cannot change?
","23/Jan/09 20:44;mikemccand;
For the perf tests, I would use an optimized index with maybe 2M
wikipedia docs in it.

Then test with maybe 0, 1, 5, 10, 25, 50, 75 percent deletions, across
various kinds of queries (single term, OR, AND, phrase/span).
Baseline w/ trunk, and then test w/ this patch (keeps deletion access
low (@ SegmentTermDocs) but switches to iterator API).  I'd love to
also see numbers for deletion-applied-as-filter (high) eventually.

[Actually if ever deletion %tg is > 50%, we should presumably invert
the bit set and work with that instead.  And same with filters.]

You might want to start with the Python scripts attached to
LUCENE-1483; with some small mods you could easily fix them to run
these tests.
","23/Jan/09 21:02;jasonrutherglen;searchdeletes.alg uses reuters, deletes many docs, then performs
searches. If it's working properly, iteration rather than calling
BitVector.get has some serious performance drawbacks. 

Compare:
DocIdSet SrchNewRdr_8: 32.0 rec/s
DelDoc.get SrchNewRdr_8: 2,959.5 rec/s

Next step is running JProfiler. Perhaps BitVector needs to be
replaced by OpenBitSet for iterating, or there's some other issue. 

BitVector.get:
[java] Operation         round mrg buf   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
[java] CreateIndex           0  10 100        1            1         17.2        0.06     3,953,984      9,072,640
[java] CloseIndex -  -  -  - 0  10 100 -  -   1 -  -  -  - 1 -  - 1,000.0 -  -   0.00 -   3,953,984 -  - 9,072,640
[java] Populate              0  10 100        1       200003      6,539.7       30.58     8,665,528     10,420,224
[java] Deletions -  -  -  -  0  10 100 -  -   1 -  -  - 8002 -  533,466.7 -  -   0.01 -   8,665,528 -   10,420,224
[java] OpenReader(false)     0  10 100        1            1      1,000.0        0.00     8,691,040     10,420,224
[java] Seq_8000 -  -  -  -   0  10 100 -  -   1 -  -  - 8000 -  800,000.0 -  -   0.01 -   8,833,912 -   10,420,224
[java] CloseReader           0  10 100        9            1      2,250.0        0.00     7,672,217     10,420,224
[java] SrchNewRdr_8 -  -  -  0  10 100 -  -   1 -  -  - 4016 -  - 2,959.5 -  -   1.36 -   8,232,384 -   10,420,224
[java] OpenReader            0  10 100        8            1      1,333.3        0.01     7,579,584     10,420,224
[java] Seq_500 -  -  -  -  - 0  10 100 -  -   8 -  -  -  500 -  - 2,963.0 -  -   1.35 -   8,591,199 -   10,420,224

DocIdSet:
[java] Operation         round mrg buf   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
[java] CreateIndex           0  10 100        1            1         17.5        0.06     3,954,376      9,076,736
[java] CloseIndex -  -  -  - 0  10 100 -  -   1 -  -  -  - 1 -  - 1,000.0 -  -   0.00 -   3,954,376 -  - 9,076,736
[java] Populate              0  10 100        1       200003      6,503.1       30.75     5,951,816     10,321,920
[java] Deletions -  -  -  -  0  10 100 -  -   1 -  -  - 8002 -  500,125.0 -  -   0.02 -   6,190,816 -   10,321,920
[java] OpenReader(false)     0  10 100        1            1      1,000.0        0.00     5,976,960     10,321,920
[java] Seq_8000 -  -  -  -   0  10 100 -  -   1 -  -  - 8000 -  727,272.8 -  -   0.01 -   6,122,904 -   10,321,920
[java] CloseReader           0  10 100        9            1      3,000.0        0.00     7,727,980     10,321,920
[java] SrchNewRdr_8 -  -  -  0  10 100 -  -   1 -  -  - 4016 -  -  - 32.0 -  - 125.67 -   7,960,824 -   10,321,920
[java] OpenReader            0  10 100        8            1      1,333.3        0.01     7,742,855     10,321,920
[java] Seq_500 -  -  -  -  - 0  10 100 -  -   8 -  -  -  500 -  -  - 31.8 -  - 125.66 -   8,744,057 -   10,321,920
","23/Jan/09 21:10;mikemccand;Jason can you format those results using a Jira table?  It's real hard to read as is (and something tells me we will be looking at alot of these tables shortly.... ;) ).

Also, showing %tg change vs baseline helps.","27/Jan/09 16:24;jasonrutherglen;The percentage performance decrease in the previous 
results is 99%.  

{quote} 
Jason can you format those results using a Jira table? 
{quote}

Perhaps this should be an option in the benchmark output?

{quote} 
M.M. LUCENE-1516 comment: ""I think the larger number of
[harder-for-cpu-to-predict] if statements may be the cause of the
slowdown once %tg deletes gets high enough?"" 
{quote}

I have been looking at the performance with YourKit and don't have
any conclusions yet. The main difference between using skipto and 
BV.get is the if statements and some added method calls, which even 
if they are inlined I suspect will not make up the difference.

Next steps: 
1. Deletes as a NOT boolean query which probably should
be it's own patch 
2.  Pluggable alternative representations such as
OpenBitSet and int array, part of this patch? 

","27/Jan/09 18:33;marvin;> The percentage performance decrease in the previous
> results is 99%. 

That's pretty strange. I look forward to seeing profiling data.","27/Jan/09 19:38;mikemccand;bq. Perhaps this should be an option in the benchmark output?

That's a great idea!

Something silly must be going on... 99% performance drop can't be right.","28/Jan/09 22:54;jasonrutherglen;I created basic test code TestDeletesDocIdSet.java because I thought
the previous results may have something to do with my
misunderstanding of the contrib/benchmark code. 

The results show a 17% increase in performance for this patch's
deleted docs skipto code. I think now that it's probably the fact
that ANT_OPTS isn't using the -server option properly as
contrib/benchmark build.xml forks the process. -server on the MacOSX
seems to optimize method calls a far better than -client. 

Trunk:
warmup search duration: 6208
final search duration: 6371

LUCENE-1476, skipto:
warmup search duration: 5926
final search duration: 5450","28/Jan/09 23:04;jasonrutherglen;Patch the previously mentioned performance test uses.","29/Jan/09 01:35;jasonrutherglen;I implemented code that uses skipto deleted docs where BitVector is
replaced with OpenBitSet, and BitVector is faster. I'll save
implementing the int array docidset for the tombstone patch
LUCENE-1526.

","29/Jan/09 16:01;mikemccand;
I had some trouble w/ the latest patch (I'm trying to reproduce your
strange problem w/ contrib/benchmark & run perf tests):

  * MultiDocIdSet was missing (I just pulled from earlier patch)

  * Compilation errors because {Filter,Parallel}IndexReader failed to
    mark getDeletedDocs public (I made them public).

  * contrib/instantiated failed to compile, because its IndexReader
    impl failed to implement getDocIdSet... but we can't add abstract
    IndexReader.getDocIdSet (this breaks back compat).  I switched to
    non-abstract default method that throws
    UnsupportedOperationException instead, and added nocommit comment
    to remind us to fix all IndexReader subclasses in contrib.

  * I don't think you should change MemoryIndexReader from private to
    public?  Why was that done?  (I reverted this).

  * Replaced MemoryIndex.NullDocIdSet with the new
    DocIdSet.EMPTY_DOCIDSET

  * Somehow you lost recent fixes to MemoryIndex.java (committed as
    part of LUCENE-1316).  This was causing NPE test failure (I
    reverted this, test passes now).

New patch attached.  All tests pass... next I'll try to repro the
contrib/benchmark oddness.
","29/Jan/09 17:24;mikemccand;Hmm... when I run the TestDeletesDocIdSet, I don't see as much
improvement: trunk gets 10.507 seconds, patch gets 10.158 (~3.3%
faster).  I'm running on OS X 10.5.6, quad core Intel; java version is
""1.6.0_07-b06-153"" and I run ""java -server -Xbatch -Xmx1024M
-Xms1024M"".

But that test is rather synthetic: you create 15,000 docs, then delete
1 in 8, then do a search (for ""text"") that matches all of the docs.

So I went back to contrib/benchmark...  I created a single-segment
index, with 0%, 1%, 2%, 5%, 10%, 20% and 50% deletions, using first 2M
docs from Wikipedia, then ran 5 different queries and compared qps w/
patch vs trunk.  Results:

||%tg deletes||query||hits||qps||qpsnew||%tg change||
|0%|147|   4984|5560.1|5486.2| -1.3%|
|0%|text|  97191| 347.3| 339.4| -2.3%|
|0%|1 AND 2| 234634|  22.9|  22.8| -0.4%|
|0%|1| 386435|  88.4|  87.2| -1.4%|
|0%|1 OR 2| 535584|  20.9|  20.9|  0.0%|
|1%|147|   4933|5082.0|1419.2|-72.1%|
|1%|text|  96143| 313.9| 142.0|-54.8%|
|1%|1 AND 2| 232250|  22.1|  18.6|-15.8%|
|1%|1| 382498|  81.0|  62.2|-23.2%|
|1%|1 OR 2| 530212|  20.2|  17.5|-13.4%|
|2%|147|   4883|5133.6|1959.0|-61.8%|
|2%|text|  95190| 315.8| 149.2|-52.8%|
|2%|1 AND 2| 229870|  22.2|  18.4|-17.1%|
|2%|1| 378641|  81.2|  58.9|-27.5%|
|2%|1 OR 2| 524873|  20.3|  17.1|-15.8%|
|5%|147|   4729|5073.6|2600.8|-48.7%|
|5%|text|  92293| 315.2| 166.9|-47.0%|
|5%|1 AND 2| 222859|  22.5|  17.8|-20.9%|
|5%|1| 367000|  81.0|  56.2|-30.6%|
|5%|1 OR 2| 508632|  20.4|  16.3|-20.1%|
|10%|147|   4475|5049.6|2953.7|-41.5%|
|10%|text|  87504| 314.8| 180.9|-42.5%|
|10%|1 AND 2| 210982|  22.9|  17.8|-22.3%|
|10%|1| 347664|  81.5|  53.8|-34.0%|
|10%|1 OR 2| 481792|  21.2|  16.5|-22.2%|
|20%|147|   4012|5045.0|3455.5|-31.5%|
|20%|text|  77980| 317.2| 204.7|-35.5%|
|20%|1 AND 2| 187605|  23.9|  19.2|-19.7%|
|20%|1| 309040|  82.0|  54.7|-33.3%|
|20%|1 OR 2| 428232|  22.3|  17.5|-21.5%|
|50%|147|   2463|5283.2|4731.4|-10.4%|
|50%|text|  48331| 336.9| 290.2|-13.9%|
|50%|1 AND 2| 116887|  28.4|  25.9| -8.8%|
|50%|1| 193154|  86.4|  74.9|-13.3%|
|50%|1 OR 2| 267525|  27.6|  24.9| -9.8%|

I think one major source of slowness is the BitVector.nextSetBit()
impl: it now checks one bit at a time, but it'd be much better to use
OpenBitSet's approach.

I also think, realistically, this approach won't perform very well
until we switch to a sparse representation for the bit set, so that
next() and skipTo() perform well.","29/Jan/09 19:11;jasonrutherglen;Interesting results. Mike, can you post your test code? The Mac can
be somewhat unreliable for performance results as today the
TestDeletesDocIdSet is about the same speed as trunk. 

OpenBitSet didn't seem to make much of a difference however when
running your tests I can check it out. The other option is something
like P4Delta which stores the doc ids in a compressed form solely for
iterating. Is this what you mean by sparse representation? ","29/Jan/09 19:26;mikemccand;
I'm attaching the Python code I use to run (it's adapted from lucene-1483).  You also need the following nocommit diff applied:

{code}
Index: contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
===================================================================
--- contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java	(revision 738896)
+++ contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java	(working copy)
@@ -62,6 +62,9 @@
     super(runData);
   }
 
+  // nocommit
+  static boolean first = true;
+
   public int doLogic() throws Exception {
     int res = 0;
     boolean closeReader = false;
@@ -102,6 +105,12 @@
         }
         //System.out.println(""q="" + q + "":"" + hits.totalHits + "" total hits""); 
 
+        // nocommit
+        if (first) {
+          System.out.println(""NUMHITS="" + hits.totalHits);
+          first = false;
+        }
+
         if (withTraverse()) {
           final ScoreDoc[] scoreDocs = hits.scoreDocs;
           int traversalSize = Math.min(scoreDocs.length, traversalSize());
{code}

Just run sortBench2.py in contrib/benchmark of trunk & patch areas.  Then run sortCollate2.py to make the Jira table (-jira) or print a human readable output (default).  You'll have to make your own Wikipedia indices with the pctg deletes, then edit sortBench2.py & sortCollate2.py to fix the paths.

All they do is write an alg file, run the test, and parse the output file to gather best of 5.","29/Jan/09 20:17;mikemccand;
Actually I made one mistake running your standalone test -- I had
allowed the ""createIndex"" to run more than once, and so I think I had
tested 30K docs with 1875 deletes (6.25%).

I just removed the index and recreated it, so I have 15K docs and 1875
deletes (12.5%).  On the mac pro I now see the patch at 4.0% slower
(4672 ms to 4859 ms), and on a Debian Linux box (kernel 2.6.22.1, java
1.5.0_08-b03) I see it 0.8% slower (7298 ms to 7357 ms).

bq. The Mac can be somewhat unreliable for performance results 

I've actually found it to be quite reliable.  What I love most about
it is, as long as you shut down all extraneous processes, it gives
very repeatable results.  I haven't found the same true (or, less so)
of various Linux's & Windows.

bq. OpenBitSet didn't seem to make much of a difference

This is very hard to believe -- the nextSetBit impl in BitVector (in
the patch) is extremely inefficient.  OpenBitSet's impl ought to be
much faster.

{quote}
The other option is something like P4Delta which stores the doc
ids in a compressed form solely for iterating.
{quote}

I think that will be too costly here (but is a good fit for
postings).

bq. Is this what you mean by sparse representation?

Actually I meant a simple sorted list of ints, but even for that I'm
worried about the skipTo cost (if we use a normal binary search).  I'm
not sure it can be made fast enough (ie faster than random access
we have today).
","29/Jan/09 22:08;jasonrutherglen;bq: shut down all extraneous processes

It's a desktop machine though so it's going to have some stuff
running the background, most of which I'm not aware of being a Mac
newbie.

bq: Actually I meant a simple sorted list of ints, but even for that
I'm worried about the skipTo cost (if we use a normal binary search)

Skipping is slower because it unnecessarily checks bits that are not
useful to the query. A higher level deletions Filter implemented
perhaps in IndexSearcher requires docs that are deleted, pass through
the SegmentTermDocs doc[] cache which could add unnecessary overhead
from the vint decoding. 

The main problem we're trying to solve is potential allocation of a
large del docs BV byte array for the copy on write of a cloned
reader. An option we haven't looked at is a MultiByteArray where
multiple byte arrays make up a virtual byte array checked by BV.get.
On deleteDocument, only the byte array chunks that are changed are
replaced in the new version, while the previously copied chunks are
kept. The overhead of the BV.get can be minimal, though in our tests
with an int array version the performance can either be equal to or
double based on factors we are not aware of. ","29/Jan/09 22:26;marvin;> the nextSetBit impl in BitVector (in the patch) is extremely inefficient.

Here's an improved version that doesn't call get(). 
(quasi_iterator_deletions_r3.diff)

The private method firstBitInNonZeroByte() in this patch could potentially 
be replaced with a 256-byte lookup table.","30/Jan/09 00:03;mikemccand;Numbers with Marvin's latest patch:

||%tg deletes||query||hits||qps||qpsnew||pctg||
|0%|147|   4984|5560.1|5507.8| -0.9%|
|0%|text|  97191| 347.3| 338.8| -2.4%|
|0%|1 AND 2| 234634|  22.9|  22.8| -0.4%|
|0%|1| 386435|  88.4|  86.9| -1.7%|
|0%|1 OR 2| 535584|  20.9|  20.7| -1.0%|
|1%|147|   4933|5082.0|3292.2|-35.2%|
|1%|text|  96143| 313.9| 260.8|-16.9%|
|1%|1 AND 2| 232250|  22.1|  21.9| -0.9%|
|1%|1| 382498|  81.0|  79.6| -1.7%|
|1%|1 OR 2| 530212|  20.2|  20.3|  0.5%|
|2%|147|   4883|5133.6|3092.0|-39.8%|
|2%|text|  95190| 315.8| 232.8|-26.3%|
|2%|1 AND 2| 229870|  22.2|  21.2| -4.5%|
|2%|1| 378641|  81.2|  76.4| -5.9%|
|2%|1 OR 2| 524873|  20.3|  19.8| -2.5%|
|5%|147|   4729|5073.6|3478.5|-31.4%|
|5%|text|  92293| 315.2| 219.1|-30.5%|
|5%|1 AND 2| 222859|  22.5|  20.5| -8.9%|
|5%|1| 367000|  81.0|  68.5|-15.4%|
|5%|1 OR 2| 508632|  20.4|  18.9| -7.4%|
|10%|147|   4475|5049.6|3547.8|-29.7%|
|10%|text|  87504| 314.8| 222.2|-29.4%|
|10%|1 AND 2| 210982|  22.9|  19.4|-15.3%|
|10%|1| 347664|  81.5|  61.6|-24.4%|
|10%|1 OR 2| 481792|  21.2|  18.4|-13.2%|
|20%|147|   4012|5045.0|3741.8|-25.8%|
|20%|text|  77980| 317.2| 232.9|-26.6%|
|20%|1 AND 2| 187605|  23.9|  20.5|-14.2%|
|20%|1| 309040|  82.0|  59.0|-28.0%|
|20%|1 OR 2| 428232|  22.3|  18.8|-15.7%|
|50%|147|   2463|5283.2|4712.1|-10.8%|
|50%|text|  48331| 336.9| 296.2|-12.1%|
|50%|1 AND 2| 116887|  28.4|  26.6| -6.3%|
|50%|1| 193154|  86.4|  77.3|-10.5%|
|50%|1 OR 2| 267525|  27.6|  25.7| -6.9%|

It's definitely better than before but still slower than trunk.","30/Jan/09 00:28;mikemccand;
{quote}
The main problem we're trying to solve is potential allocation of a
large del docs BV byte array for the copy on write of a cloned
reader.
{quote}

Right, as long as normal search performance does not get worse.
Actually, I was hoping ""deletes as iterator"" and ""deletes higher up as
filter"" might give us some gains in search performance.

{quote}
An option we haven't looked at is a MultiByteArray where
multiple byte arrays make up a virtual byte array checked by BV.get.
On deleteDocument, only the byte array chunks that are changed are
replaced in the new version, while the previously copied chunks are
kept. The overhead of the BV.get can be minimal, though in our tests
with an int array version the performance can either be equal to or
double based on factors we are not aware of. 
{quote}

I think that'd be a good approach (it amortizes the copy on write
cost), though it'd be a double deref per lookup with the
straightforward impl so I think it'll hurt normal search perf too.

And I don't think we should give up on iterator access just yet... I
think we should try list-of-sorted-ints?

","30/Jan/09 04:24;marvin;> Numbers with Marvin's latest patch:

Presumably you spliced the improved nextSetBit into Jason's patch, correct?  I wonder how my patch on its own would do, since there's less abstraction. Didn't you have a close-to-ideal patch using sorted ints that performed well up to 10% deletions?  What did that look like?

> I think we should try list-of-sorted-ints?

That should help with the situation where deletes are sparse, particularly when the term is rare (such as those searches for ""147""), since it will remove the cost of scanning through a bunch of empty bits.

I'm also curious what happens if we do without the null-check here:

{code}
+      if (deletedDocsIt != null) {
+        if (doc > nextDeletion) {
+          if (deletedDocsIt.skipTo(doc)) 
+            nextDeletion = deletedDocsIt.doc();
+        } 
+        if (doc == nextDeletion)
+          continue;
       }
{code}

When there are no deletions, nextDeletion is set and left at Integer.MAX_VALUE, so we'd get a comparison that's always false for the life of the TermDocs instead of an always-null null check. Possibly we'd slow down the no-deletions case while speeding up all others, but maybe the processor does a good job at predicting the result of the comparison.

I also suspect that when there are many deletions, the sheer number of method calls to perform the deletions iteration is a burden.  The iterator has to compete with an inline-able method from a final class (BitVector).

","30/Jan/09 09:53;mikemccand;bq. Presumably you spliced the improved nextSetBit into Jason's patch, correct?

Actually I used your entire patch on its own.

bq. Didn't you have a close-to-ideal patch using sorted ints that performed well up to 10% deletions? What did that look like?

I thought I did -- but it was rather hacked up (I ""fixed"" SegmentReader to do always do an up-front conversion into int[] deletedDocs).  I'll re-test it to try to repro my initial rough results.

bq. I also suspect that when there are many deletions, the sheer number of method calls to perform the deletions iteration is a burden. The iterator has to compete with an inline-able method from a final class (BitVector).

Right, for a highish %tg deletion it seems likely that random-access will win.","30/Jan/09 12:13;mikemccand;Alas.... I had a bug in my original test (my SegmentTermDocs was
incorrectly returning some deleted docs).  But even with that bug I
can't repro my original ""it's faster at < 10% deletions"".  Here are my
results using a pre-computed array of deleted docIDs:

||%tg deletes||query||hits||qps||qpsnew||pctg||
|0%|147|   4984|5560.1|5392.5| -3.0%|
|0%|text|  97191| 347.3| 334.1| -3.8%|
|0%|1 AND 2| 234634|  22.9|  22.8| -0.4%|
|0%|1| 386435|  88.4|  86.0| -2.7%|
|0%|1 OR 2| 535584|  20.9|  20.8| -0.5%|
|1%|147|   4933|5082.0|3643.5|-28.3%|
|1%|text|  96143| 313.9| 304.9| -2.9%|
|1%|1 AND 2| 232250|  22.1|  22.3|  0.9%|
|1%|1| 382498|  81.0|  82.3|  1.6%|
|1%|1 OR 2| 530212|  20.2|  20.2|  0.0%|
|2%|147|   4883|5133.6|3299.6|-35.7%|
|2%|text|  95190| 315.8| 289.7| -8.3%|
|2%|1 AND 2| 229870|  22.2|  22.1| -0.5%|
|2%|1| 378641|  81.2|  80.9| -0.4%|
|2%|1 OR 2| 524873|  20.3|  20.2| -0.5%|
|5%|147|   4729|5073.6|2405.2|-52.6%|
|5%|text|  92293| 315.2| 259.0|-17.8%|
|5%|1 AND 2| 222859|  22.5|  22.0| -2.2%|
|5%|1| 367000|  81.0|  77.6| -4.2%|
|5%|1 OR 2| 508632|  20.4|  19.7| -3.4%|
|10%|147|   4475|5049.6|1738.8|-65.6%|
|10%|text|  87504| 314.8| 232.6|-26.1%|
|10%|1 AND 2| 210982|  22.9|  21.7| -5.2%|
|10%|1| 347664|  81.5|  74.0| -9.2%|
|10%|1 OR 2| 481792|  21.2|  20.2| -4.7%|
|20%|147|   4012|5045.0|1117.6|-77.8%|
|20%|text|  77980| 317.2| 208.9|-34.1%|
|20%|1 AND 2| 187605|  23.9|  21.4|-10.5%|
|20%|1| 309040|  82.0|  68.2|-16.8%|
|20%|1 OR 2| 428232|  22.3|  20.2| -9.4%|
|50%|147|   2463|5283.2| 522.3|-90.1%|
|50%|text|  48331| 336.9| 176.4|-47.6%|
|50%|1 AND 2| 116887|  28.4|  23.0|-19.0%|
|50%|1| 193154|  86.4|  63.5|-26.5%|
|50%|1 OR 2| 267525|  27.6|  22.4|-18.8%|

I've attached my patch, but note that some tests fail because I don't update the list of deleted docs when deleteDocument is called.

I'm now feeling like we're gonna have to keep random-access to deleted docs....","30/Jan/09 17:38;marvin;
> Actually I used your entire patch on its own.

Ah.  Thanks for running all these benchmarks.  (I'm going to have to port the
benchmark suite sooner rather than later so that Lucy doesn't have to rely on
having algos worked out under Lucene.)

I think it's important to note that the degradation at low deletion
percentages is not as bad as the chart seems to imply.  The worst performance
was on the cheapest query, and the best performance was on the most expensive
query.

Furthermore, a 50% deletion percentage is very high.  Regardless of how
deletions are implemented, the default merge policy ought to trigger
absorbtion of segments with deletion percentages over a certain threshold.
The actual threshold percentage is an arbitrary number because the ideal
tradeoff between index-time sacrifices and search-time sacrifices varies, but
my gut pick would be somewhere between 10% and 30% for bit vector deletions
and between 5% and 20% for iterated deletions.

> I'm now feeling like we're gonna have to keep random-access to deleted
> docs....

Ha, you're giving up a little early, amigo.  :)

I do think it's clear that maintaining an individual deletions iterator for
each posting list isn't going to work well, particularly when it's implemented
using an opaque DocIdSetIterator and virtual methods.  That can't compete with
what is essentially an array lookup (since BitVector.get() can be inlined) on
a shared bit vector, even at low deletion rates.

I also think we can conclude that high deletion rates cause an accelerating
degradation with iterated deletions, and that if they are implemented, that
problem probably needs to be addressed with more aggressive segment merging.

However, I don't think the benchmark data we've seen so far demonstrates that
the filtered deletions model won't work.  Heck, with that model,
deletions get pulled out out TermDocs so we lose the per-iter null-check,
possibly yielding a small performance increase in the common case of zero
deletions.

Maybe we should close this issue with a won't-fix and start a new one for
filtered deletions?","30/Jan/09 17:56;jasonrutherglen;> Maybe we should close this issue with a won't-fix and start a new one for filtered deletions?

We need more performance data before going ahead with tombstone deletions.  Sounds like the next step. 
","30/Jan/09 18:34;jasonrutherglen;{quote} 
Just run sortBench2.py in contrib/benchmark of trunk & patch
areas. Then run sortCollate2.py to make the Jira table (-jira) or
print a human readable output (default). You'll have to make your own
Wikipedia indices with the pctg deletes, then edit sortBench2.py &
sortCollate2.py to fix the paths.

All they do is write an alg file, run the test, and parse the output
file to gather best of 5. 
{quote}

This seems like something we can port to Java and get into
contrib/benchmark. Particularly automatically creating the indexes.","30/Jan/09 21:07;mikemccand;{quote}
Thanks for running all these benchmarks. (I'm going to have to port the
benchmark suite sooner rather than later so that Lucy doesn't have to rely on
having algos worked out under Lucene.)
{quote}

No problem :) And I don't mind working these things out / discussing
them in Lucene -- many of these ideas turn out very well!  EG
LUCENE-1483.

Though... iteration may work better in C than it does in Java, so it's
hard to know what to conclude here for Lucy.

{quote}
I think it's important to note that the degradation at low deletion
percentages is not as bad as the chart seems to imply. The worst performance
was on the cheapest query, and the best performance was on the most expensive
query.
{quote}

True.

{quote}
Furthermore, a 50% deletion percentage is very high. Regardless of how
deletions are implemented, the default merge policy ought to trigger
absorbtion of segments with deletion percentages over a certain threshold.
The actual threshold percentage is an arbitrary number because the ideal
tradeoff between index-time sacrifices and search-time sacrifices varies, but
my gut pick would be somewhere between 10% and 30% for bit vector deletions
and between 5% and 20% for iterated deletions.
{quote}

Right, we should have the default merge policy try to coalsce segments
with too many deletions (Lucene does not do this today -- you have to
call expungeDeletes yourself, and even that's not perfect since it
eliminates every single delete).

bq. Ha, you're giving up a little early, amigo.

:)

{quote}
I do think it's clear that maintaining an individual deletions iterator for
each posting list isn't going to work well, particularly when it's implemented
using an opaque DocIdSetIterator and virtual methods. That can't compete with
what is essentially an array lookup (since BitVector.get() can be inlined) on
a shared bit vector, even at low deletion rates.
{quote}

Right.

{quote}
I also think we can conclude that high deletion rates cause an accelerating
degradation with iterated deletions, and that if they are implemented, that
problem probably needs to be addressed with more aggressive segment merging.
{quote}

Agreed.

{quote}
However, I don't think the benchmark data we've seen so far demonstrates that
the filtered deletions model won't work. Heck, with that model,
deletions get pulled out out TermDocs so we lose the per-iter null-check,
possibly yielding a small performance increase in the common case of zero
deletions.
{quote}

By ""filtered deletions model"", you mean treating deletions as a
""normal"" filter and moving ""up"" when they are applied?  Right, we have
not explored that yet, here.

Except, filters are accessed by iteration in Lucene now (they used to
be random access before LUCENE-584).

So... now I'm wondering if we should go back and scrutinize the
performance cost of switching from random access to iteration for
filters, especially in cases where under-the-hood the filter needed to
create a non-sparse repr anyway.  It looks like there was a fair
amount of discussion about performance, but no hard conclusions, in
LUCENE-584, on first read.  Hmmm.

I think to do a fair test, we need to move deleted docs ""up higher"",
but access it w/ random access API?

Except... the boolean query QPS are not that different with the
no-deletions case vs the 1% deletions.  And it's only the non-simple
queries (boolean, span, phrase, etc.) that would see any difference
in moving deletions ""up"" as a filter?  Ie their QPS is already so low
that the added cost of doing the deletes ""at the bottom"" appears to be
quite minor.

(Conceptually, I completely agree that deletions are really the same
thing as filters).

{quote}
Maybe we should close this issue with a won't-fix and start a new one for
filtered deletions?
{quote}
Maybe discuss a bit more here or on java-dev first?
","30/Jan/09 21:12;mikemccand;{quote}
This seems like something we can port to Java and get into
contrib/benchmark. Particularly automatically creating the indexes.
{quote}
That would be great!  Maybe it's not so much work... contrib/benchmark can already do the ""rounds"" to enumerate through the different combinations of query, index, etc.

Automatically creating the indices is trickier; you'd want to 1) create the first index, 2) copy it and do X%tg deletions, 3) repeat 2 for different values of X.

Also tricky is gathering a series of results and saving it somehow so that you can then make a baseline for other runs.

Or we could keep using Python (pickling is so easy ;)).  It's nice to have a ""scripting"" layer on top of contrib/benchmark.","30/Jan/09 21:19;mikemccand;bq. We need more performance data before going ahead with tombstone deletions. Sounds like the next step. 

Or, I think morphing the tombstones idea into the ""incremental copy on write block BitVector"" idea you mentioned, is a good next step to explore (for realtime search).","24/Jan/11 21:16;jasonrutherglen;Won't be working on these and they're old",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable reader and binary fields in InstantiatedIndexWriter,LUCENE-1332,12400030,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,jasonrutherglen,jasonrutherglen,10/Jul/08 15:15,24/Jan/11 21:16,30/Sep/19 08:38,24/Jan/11 21:16,2.3.1,,,,,,,,,,,,,,modules/other,,,0,,,,Currently InstantiatedIndexWriter does not support fields with a Reader or that are binary.  ,,,,,,,,,,,,,,,,"10/Jul/08 15:16;jasonrutherglen;lucene-1332.patch;https://issues.apache.org/jira/secure/attachment/12385773/lucene-1332.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12416,,,Mon Jan 24 21:16:45 UTC 2011,New,,,,,,,"0|i04xmv:",26714,,,,,,,,,"10/Jul/08 15:16;jasonrutherglen;lucene-1332.patch

Supports Reader fields however not binary yet.  Would like to get Karl's thoughts on supporting binary.","24/Jan/11 21:16;jasonrutherglen;Won't be working on these and they're old",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow user configurable buffersize for RAMDirectory,LUCENE-1319,12399226,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,jasonrutherglen,jasonrutherglen,28/Jun/08 21:41,24/Jan/11 21:16,30/Sep/19 08:38,24/Jan/11 21:16,2.3.1,,,,,,,,,,,,,,core/store,,,0,,,,Currently RAMDirectory via RAMOutputStream has a package protected value of 1024 as the buffer size for use in RAMFile.  This issue proposes adding a single constructor to RAMDirectory allowing the user to specify the buffer size.  ,,,,,,,,,,,,,,,,"29/Jun/08 13:29;jasonrutherglen;lucene-1319.patch;https://issues.apache.org/jira/secure/attachment/12384910/lucene-1319.patch","28/Jun/08 22:11;jasonrutherglen;lucene-1319.patch;https://issues.apache.org/jira/secure/attachment/12384901/lucene-1319.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12429,,,Mon Jan 24 21:16:44 UTC 2011,New,,,,,,,"0|i04xpr:",26727,,,,,,,,,"28/Jun/08 22:11;jasonrutherglen;lucene-1319.patch

Added RAMDirectory(int bufferSize) along with accompanying changes to RAMInputStream and RAMOutputStream.  Also made RAMInputStream and RAMFile public and extensible. ","29/Jun/08 13:29;jasonrutherglen;lucene-1319.patch

Added default constructor for RAMOutputStream.  RAMInputStream.writeTo method.  Made RAMFile buffers publicly accessible.  Making RAMFile public allows implemenations that want to pool byte arrays.","24/Jan/11 21:16;jasonrutherglen;Won't be working on these and they're old",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add IndexReaderFactory to IndexWriter,LUCENE-1674,12426840,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,jasonrutherglen,jasonrutherglen,01/Jun/09 18:11,24/Jan/11 21:12,30/Sep/19 08:38,24/Jan/11 21:12,2.4.1,,,,,,,,,,4.0-ALPHA,,,,core/index,,,0,,,,"With LUCENE-1516, IndexWriter.getReader, we take over the
instantiating of IndexReaders which prevents users who have
implemented custom IndexReader subclasses from using them.

The patch will create an IndexWriter.setReaderFactory method and
a IndexReaderFactory class that allows custom creation of the
internal readers created by IndexWriter.",,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-06-11 18:33:36.223,,,false,,,,,,,,,,,,,,,12084,,,Mon Jan 24 21:12:12 UTC 2011,New,,,,,,,"0|i04vin:",26371,,,,,,,,,"11/Jun/09 18:33;mikemccand;Moving out.","24/Jan/11 21:12;jasonrutherglen;Sorry if this spam's things, however it's unlikely that I'll work on these.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FSDirectory internally caches and clones FSIndexInput,LUCENE-1671,12426799,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,jasonrutherglen,jasonrutherglen,01/Jun/09 04:36,24/Jan/11 21:12,30/Sep/19 08:38,24/Jan/11 21:12,2.4.1,,,,,,,,,,4.0-ALPHA,,,,core/store,,,0,,,,"The patch will fix this small problem where if FSDirectory.openInput is called, a new unnecessary file descriptors is opened (whereas an IndexInput.clone would work).",,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-06-10 20:16:44.42,,,false,,,,,,,,,,,,,,,12087,,,Mon Jan 24 21:12:11 UTC 2011,New,,,,,,,"0|i04vjb:",26374,,,,,,,,,"10/Jun/09 20:16;mikemccand;Moving out.","24/Jan/11 21:12;jasonrutherglen;Sorry if this spam's things, however it's unlikely that I'll work on these.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add fetch-javacc task to common-build.xml,LUCENE-2612,12472250,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Not A Problem,simonw,simonw,simonw,23/Aug/10 10:43,03/Jan/11 10:52,30/Sep/19 08:38,03/Jan/11 10:52,4.0-ALPHA,,,,,,,,,,,,,,general/build,,,0,,,,"I'm kind of tired setting up javacc each time I build a new environment so I added an ant target to common-build.xml. This target fetches javacc 4.1 and extracts it into the configured ${javacc.home} directory. 

Nothing fancy but likely to be helpful.


",,,,,,,,,,,,,,,,"23/Aug/10 10:43;simonw;LUCENE-2612.patch;https://issues.apache.org/jira/secure/attachment/12452801/LUCENE-2612.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-08-23 11:13:45.733,,,false,,,,,,,,,,,,,,,11223,,,Mon Jan 03 10:52:17 UTC 2011,New,Patch Available,,,,,,"0|i04pif:",25398,,,,,,,,,"23/Aug/10 10:43;simonw;here is a patch","23/Aug/10 11:13;rcmuir;Simon, fwiw I don't have to setup javacc each time i build a new environment.

Just put a build.properties in your homedir with the javacc.home set, and it will be sucked in for any new environment and 'ant javacc' will always work.
","23/Aug/10 11:20;simonw;bq. Simon, fwiw I don't have to setup javacc each time i build a new environment.

each time I work on another computer... I have a build.properties too :)

but thanks for the reminder :D

I say its nothing fancy but it gets you the right version and installs it right away.","23/Aug/10 19:51;simonw;Any objections with this issue?

","03/Jan/11 10:52;simonw;doesn't seem to be worth it...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastVectorHighlighter: out of alignment when the first value is empty in multiValued field,LUCENE-2616,12472280,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,koji,koji,koji,23/Aug/10 17:18,01/Dec/10 14:49,30/Sep/19 08:38,30/Oct/10 04:19,2.9.3,,,,,,,,,,2.9.4,3.0.3,3.1,4.0-ALPHA,modules/highlighter,,,0,,,,,,,,,,,,,,,,,,,,"23/Aug/10 17:20;koji;LUCENE-2616.patch;https://issues.apache.org/jira/secure/attachment/12452835/LUCENE-2616.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-29 13:07:30.385,,,false,,,,,,,,,,,,,,,11220,,,Sat Oct 30 04:19:39 UTC 2010,New,,,,,,,"0|i04phj:",25394,,,,,,,,,"25/Aug/10 12:25;koji;trunk: Committed revision 989035.
branch_3x: Committed revision 989056.
","29/Oct/10 13:07;rcmuir;reopening for possible 2.9.4/3.0.3 backport.
","30/Oct/10 04:19;koji;Committed revision 1028984(2.9), 1028986(3.0).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when many query clases are specified in boolean or dismax query, highlighted terms are always ""yellow"" if multi-colored feature is used",LUCENE-2524,12468516,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,koji,koji,koji,04/Jul/10 15:15,01/Dec/10 14:49,30/Sep/19 08:38,29/Oct/10 17:13,2.9,,,,,,,,,,2.9.4,3.0.3,3.1,4.0-ALPHA,modules/highlighter,,,0,,,,"The problem is the following snippet:

{code}
protected String getPreTag( int num ){
  return preTags.length > num ? preTags[num] : preTags[0];
}
{code}

it should be:

{code}
protected String getPreTag( int num ){
  int n = num % preTags.length;
  return  preTags[n];
}
{code}
",,,,,,,,,,,,,,,,"04/Jul/10 15:42;koji;LUCENE-2524.patch;https://issues.apache.org/jira/secure/attachment/12448648/LUCENE-2524.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-29 13:15:52.826,,,false,,,,,,,,,,,,,,,11302,,,Fri Oct 29 17:13:53 UTC 2010,New,,,,,,,"0|i04q1z:",25486,,,,,,,,,"04/Jul/10 15:42;koji;The fix + more color tags contribution attached. I'll commit shortly.","04/Jul/10 16:11;koji;trunk: Committed revision 960349.
branch_3x: Committed revision 960353.","29/Oct/10 13:15;rcmuir;reopening for possible 2.9.4/3.0.3 backport.
","29/Oct/10 17:13;koji;Committed revision 1028850(2.9), 1028854(3.0).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove ""synchonized"" from FuzzyTermEnum#similarity(final String target)",LUCENE-2258,12455874,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,uschindler,uschindler,10/Feb/10 08:18,12/Oct/10 13:39,30/Sep/19 08:38,10/Feb/10 13:07,,,,,,,,,,,4.0-ALPHA,,,,core/search,,,0,,,,"The similarity method in FuzzyTermEnum is synchronized which is stupid because of:
- TermEnums are the iterator pattern and so are single-thread per definition
- The method is private, so nobody could ever create a fake FuzzyTermEnum just to have this method and use it multithreaded.
- The method is not static and has no static fields - so instances do not affect each other

The root of this comes from LUCENE-296, but was never reviewd and simply committed. The argument for making it synchronized is wrong.",,,,,,,,,,,,,,,,"10/Feb/10 08:20;uschindler;LUCENE-2258.patch;https://issues.apache.org/jira/secure/attachment/12435421/LUCENE-2258.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,11531,,,Wed Feb 10 13:07:27 UTC 2010,New,Patch Available,,,,,,"0|i04rwf:",25785,,,,,,,,,"10/Feb/10 08:20;uschindler;Patch.","10/Feb/10 13:07;uschindler;Committed trunk revision: 908477
Committed 3.0 revision: 908479
Committed 2.9 revision: 908481",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Developers Resources Documentation,LUCENE-1237,12391614,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Incomplete,,gsingers,gsingers,17/Mar/08 12:37,30/Aug/10 15:05,30/Sep/19 08:38,30/Aug/10 15:05,,,,,,,,,,,,,,,general/website,,,0,,,,Some of the links on the developer resources page are broken.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-07-08 09:57:35.205,,,false,,,,,,,,,,,,,,,12510,,,Thu Jul 10 12:07:17 UTC 2008,New,,,,,,,"0|i04y87:",26810,,,,,,,,,"08/Jul/08 09:57;lakhani.ajay;Hi Grant,  I've checked  the developers resource page (http://lucene.apache.org/java/docs/developer-resources.html)
Could you please comment on which link is broken","10/Jul/08 12:07;gsingers;The Clover test coverage links and the ""Download via Hudson"" links don't work for me.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Clicking on the ""More Results"" link in luceneweb.war demo results in ArrayIndexOutOfBoundsException",LUCENE-2473,12465015,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,siren,siren,20/May/10 10:49,18/Jun/10 08:04,30/Sep/19 08:38,20/May/10 13:04,2.9.2,3.0.1,,,,,,,,,2.9.3,3.0.2,3.1,4.0-ALPHA,modules/examples,,,0,,,,Summary says it all.,,,,,,,,,,,,,,,,"20/May/10 10:52;siren;LUCENE-2473.patch;https://issues.apache.org/jira/secure/attachment/12445049/LUCENE-2473.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-05-20 13:04:16.475,,,false,,,,,,,,,,,,,,,11347,,,Thu May 20 13:04:16 UTC 2010,New,,,,,,,"0|i04qdb:",25537,,,,,,,,,"20/May/10 10:52;siren;patch against trunk","20/May/10 13:04;rcmuir;Committed revision 946599.

Thanks Sami!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If you hit the ""max term prefix"" warning when indexing, it never goes away",LUCENE-2166,12443520,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,16/Dec/09 11:13,25/Feb/10 10:18,30/Sep/19 08:38,16/Dec/09 11:24,,,,,,,,,,,2.9.2,3.0.1,4.0-ALPHA,,core/index,,,0,,,,"Silly bug.

If IW's infoStream is on, we warn whenever we hit a ridiculously long term (> 16 KB in length).  The problem is, we never reset this warning, so, once one doc contains such a massive term, we then keep warning over and over about that same term for future docs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,11619,,,2009-12-16 11:13:59.0,New,,,,,,,"0|i04sgv:",25877,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the documentation of Version,LUCENE-2080,12441055,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,rcmuir,rcmuir,18/Nov/09 15:36,25/Feb/10 10:18,30/Sep/19 08:38,11/Feb/10 14:36,,,,,,,,,,,2.9.2,3.0,4.0-ALPHA,,general/javadocs,,,0,,,,"In my opinion, we should elaborate more on the effects of changing the Version parameter.
Particularly, changing this value, even if you recompile your code, likely involves reindexing your data.
I do not think this is adequately clear from the current javadocs.
",,,,,,,,,,,,,,,,"11/Feb/10 04:10;rcmuir;LUCENE-2080.patch;https://issues.apache.org/jira/secure/attachment/12435545/LUCENE-2080.patch","07/Feb/10 19:46;rcmuir;LUCENE-2080.patch;https://issues.apache.org/jira/secure/attachment/12435130/LUCENE-2080.patch","07/Feb/10 19:27;rcmuir;LUCENE-2080.patch;https://issues.apache.org/jira/secure/attachment/12435129/LUCENE-2080.patch","18/Nov/09 15:37;rcmuir;LUCENE-2080.patch;https://issues.apache.org/jira/secure/attachment/12425348/LUCENE-2080.patch",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2009-11-18 15:51:56.334,,,false,,,,,,,,,,,,,,,11699,,,Fri Feb 12 11:08:37 UTC 2010,New,Patch Available,,,,,,"0|i04szz:",25963,,,,,,,,,"18/Nov/09 15:37;rcmuir;here is an initial patch to try to improve it.","18/Nov/09 15:51;mikemccand;+1","18/Nov/09 15:58;rcmuir;Uwe, maybe we could have some very very short verbage about this in 3.0 release notes, since it won't make 3.0?","18/Nov/09 15:59;uschindler;+1, commit it to 3.0, too

Its only documentation change, no need for a new RC because of this.","18/Nov/09 16:10;rcmuir;Committed revision 881819.","17/Dec/09 15:22;rcmuir;I am reopening this, because I have noticed lots of usage of LUCENE_CURRENT on the user list, etc in examples from peoples code.
So I would like to backport this to 2.9 branch.

I think it may be the case that even with this warning, it is not enough. We should think of further improvements.
","17/Dec/09 15:58;mikemccand;+1 to fix on 2.9.x","07/Feb/10 19:15;rcmuir;It seems that LUCENE_CURRENT is still causing some confusion.

I am reopening this issue and propose removing this constant entirely.

Regardless of whether we want to remove it, it should be @deprecated, have much scarier javadocs text, and instead positive wording like 'latest and greatest' should instead be applied to LUCENE_31 (whatever the latest constant is), to encourage users to use that instead.
","07/Feb/10 19:27;rcmuir;attached is a patch that deprecates LUCENE_CURRENT, adds some scarier text about 'reindexing all your documents', and moves positive 'latest & greatest' text to LUCENE_31 instead.","07/Feb/10 19:46;rcmuir;I think this wording is even better and scarier:
* i bolded *re-index all your documents* in the previous wording.
* i added another sentence: Additionally, you may need to *re-test your entire application* to ensure it now does what you want, as some defaults may have changed and may break functionality in your application.","07/Feb/10 19:59;simonw;
I like this extension and I think it is important! Yet, I would use the following wording instead:

{quote}Additionally, you may need to re-test your entire application to ensure it behaves like expected, as some defaults may have changed and may break functionality in your application.{quote}
","08/Feb/10 01:14;mikemccand;I like the new scary javadoc Robert!  I think removing (deprecating today) LUCENE_CURRENT may make sense -- it's very dangerous.","08/Feb/10 01:34;rcmuir;I was thinking... this wording could be more concise too.  I will add simons suggestion, trying to also think of the scariest, shortest verbage we can use.","11/Feb/10 04:10;rcmuir;updated patch with simon's suggestion, some reformatting and edits to be slightly scarier.

if no one objects, will commit tomorrow (and backport, adjusting the 'latest and greatest' wordage to the appropriate versions)","11/Feb/10 14:36;rcmuir;Committed revision 908975.","12/Feb/10 11:08;uschindler;We should add a note in CHANGES.txt in 3.0 and 2.9 branch as this is an API change.

Something like: ""Deprecated Version.LUCENE_CURRENT constant..."" with the reason phrases from above",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Lucene 3.0 - Deprecated QueryParser Constructor in Demo Code [new QueryParser( ""contents"", analyzer)]",LUCENE-2253,12455570,,Task,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Duplicate,,locklevels,locklevels,07/Feb/10 13:06,07/Feb/10 13:11,30/Sep/19 08:38,07/Feb/10 13:11,2.9.1,3.0,,,,,,,,,,,,,modules/examples,,,0,deprecated,missing,QueryParser,"Found this issue when following the getting started tutorial with Lucene 3.0.  It appears the QueryParser constructor was deprecated 

The new code in results.jsp should be changed from:

new QueryParser(""contents"", analyzer)

to:

new QueryParser(Version.LUCENE_CURRENT, ""contents"", analyzer)

http://www.locklevels.com",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-02-07 13:11:00.45,,,false,,,,,,,,,,,,,,,11536,,,Sun Feb 07 13:11:14 UTC 2010,New,,,,,,,"0|i04rxj:",25790,,,,,,,,,"07/Feb/10 13:11;rcmuir;duplicate of LUCENE-2132, fixed there in both trunk and 3.0 branch.","07/Feb/10 13:11;simonw;Changed issue to Task / Trivial.

Thanks for reporting this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NUM_THREADS is a static member of RunAddIndexesThreads and should be accessed in a static way,LUCENE-1942,12437154,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,hdiwan,hdiwan,02/Oct/09 20:15,16/Dec/09 03:05,30/Sep/19 08:38,16/Dec/09 03:05,,,,,,,,,,,,,,,core/other,,,0,,,,"The summary contains the problem. No further description needed, I don't think.",Eclipse 3.4.2,,,,,,,,,,,,,,,"02/Oct/09 21:49;hdiwan;lucene.pat;https://issues.apache.org/jira/secure/attachment/12421159/lucene.pat",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-23 21:20:39.105,,,false,,,,,,,,,,,,,,,11828,,,Sun Dec 06 21:28:53 UTC 2009,New,Patch Available,,,,,,"0|i04tun:",26101,,,,,,,,,"02/Oct/09 21:49;hdiwan;Patch fixes the compile bugs outlined in the summary.","23/Oct/09 21:20;mikemccand;Can you make a simple patch for this?  (I don't use Eclipse).  Thanks.","23/Oct/09 21:42;hdiwan;Patch files are independent of the platform they were generated on. Indeed, I generated the patch using svn diff, I merely use eclipse as a glorified text editor.","23/Oct/09 21:49;rcmuir;i'm not able to read this patch file either... to me it looks malformed. (all on one line?)","23/Oct/09 21:49;mikemccand;But the attachment here is a binary file -- I can't feed it to the ""patch"" command.  If you have an svn command line client you should be able to just do ""svn diff"" > LUCENE-1924.patch and then post that.","23/Oct/09 21:52;markrmiller@gmail.com;Its not only a binary file, but it clearly says at the start that its an eclipse patch.","23/Oct/09 22:07;uschindler;I do not understand the whole problem. We have no compilation problems, so whats wrong?","23/Oct/09 22:14;mikemccand;bq. I do not understand the whole problem. We have no compilation problems, so whats wrong?

Right, I was curious what could be wrong, so, I wanted to look at the patch...","23/Oct/09 22:19;markrmiller@gmail.com;Well if the title is self explanatory as said, it just means he wants to switch access to that variable to a static way, rather than from a given instance. (ie RunIndexAccessThreads.NUM_THREADS rather than runIndexAccessThreads.NUM_THREADS). Purists ;) But its in the tests it looks ... and the only access' like that are now commented out that I see ... and the patch is way too big - but perhaps thats all binary cruft :)

Its a nice mystery for now :)","06/Dec/09 21:28;markrmiller@gmail.com;This issue is on the chopping block - you still around Hasan?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopDocs and TopFieldDocs does not implement equals and hashCode,LUCENE-774,12360459,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,karl.wettin,karl.wettin,13/Jan/07 13:03,10/Dec/09 20:01,30/Sep/19 08:38,10/Dec/09 20:01,2.0.0,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,"13/Jan/07 13:04;karl.wettin;extendsObject.diff;https://issues.apache.org/jira/secure/attachment/12348900/extendsObject.diff",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-01-15 07:27:31.098,,,false,,,,,,,,,,,,,,,12979,,,Sun Dec 06 19:48:45 UTC 2009,New,Patch Available,,,,,,"0|i0513b:",27274,,,,,,,,,"15/Jan/07 07:27;hossman;The summary refers to TopDocs and TopFieldDocs, but the diff changes FieldDoc and ScoreDoc ... was it ment to cover all four?
","15/Jan/07 09:45;karl.wettin;> The summary refers to TopDocs and TopFieldDocs, but the diff changes FieldDoc and ScoreDoc ... was it ment to cover all four? 

Oups, that was a typeo. I'm a confused young man.

It does however make sense to cover all four while I'm at it. I'll patch some more.","06/Dec/09 19:48;markrmiller@gmail.com;Still want to push forward with this issue?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add link to irc channel #lucene on the website,LUCENE-2116,12442544,,New Feature,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,simonw,simonw,05/Dec/09 10:49,06/Dec/09 15:32,30/Sep/19 08:38,06/Dec/09 15:32,,,,,,,,,,,,,,,general/website,,,0,,,,We should add a link to #lucene IRC channel on chat.freenode.org. ,,,,,,,,,,,,,,,,"06/Dec/09 14:06;simonw;LUCENE-2116.patch;https://issues.apache.org/jira/secure/attachment/12427094/LUCENE-2116.patch","05/Dec/09 13:17;simonw;LUCENE-2116.patch;https://issues.apache.org/jira/secure/attachment/12427040/LUCENE-2116.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-12-06 11:47:34.02,,,false,,,,,,,,,,,,,,,11663,,,Sun Dec 06 15:32:12 UTC 2009,New,,,,,,,"0|i04srz:",25927,,,,,,,,,"05/Dec/09 13:17;simonw;Created a patch for the website. 
As mike mentioned in the chat it would be desirable to have an archive for the IRC channel. Does anybody know how IRC archiving works and who initially created the channel?","06/Dec/09 11:47;mikemccand;Why does this talk about Hadoop?  Ie shouldn't it be Lucene?","06/Dec/09 12:02;simonw;LOL - I should not do any copy paste -- will fix later","06/Dec/09 14:06;simonw;s/hadoop/Lucene","06/Dec/09 15:28;mikemccand;Looks good, thanks Simon!  I'll commit shortly.","06/Dec/09 15:32;mikemccand;Thanks Simon!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
changes-to-html: better handling of bulleted lists in CHANGES.txt,LUCENE-2077,12440910,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,sarowe,sarowe,17/Nov/09 09:16,25/Nov/09 16:47,30/Sep/19 08:38,17/Nov/09 09:45,2.9.1,,,,,,,,,,3.0,,,,general/website,,,0,,,,"- bulleted lists
- should be rendered
- as such
- in output HTML",,,,,,,,,,,,,,,,"17/Nov/09 09:18;sarowe;LUCENE-2077.patch;https://issues.apache.org/jira/secure/attachment/12425195/LUCENE-2077.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-11-17 09:28:44.201,,,false,,,,,,,,,,,,,,,11701,,,Tue Nov 17 09:45:35 UTC 2009,New,Patch Available,,,,,,"0|i04t0n:",25966,,,,,,,,,"17/Nov/09 09:18;sarowe;Patch to handle bulleted lists in CHANGES.txt, and remove <code> tag workarounds from CHANGES.txt.","17/Nov/09 09:28;uschindler;Works as expected, thanks! I will commit this to trunk and lucene_30 before release!","17/Nov/09 09:45;uschindler;Committed:
At revision: 881213 (trunk)
At revision: 881216 (3.0)

Thanks Steven!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
benchmark pkg: specify trec_eval submission output from the command line,LUCENE-2058,12440488,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,rcmuir,rcmuir,12/Nov/09 12:00,25/Nov/09 16:47,30/Sep/19 08:38,13/Nov/09 00:42,,,,,,,,,,,3.0,,,,modules/benchmark,,,0,,,,"the QueryDriver for the trec benchmark currently requires 4 command line arguments.
the third argument is ignored (i typically populate this with ""bogus"")
Instead, allow the third argument to specify the submission.txt file for trec_eval.

while I am here, add a usage() documenting what the arguments to this driver program do.",,,,,,,,,,,,,,,,"12/Nov/09 12:01;rcmuir;LUCENE-2058.patch;https://issues.apache.org/jira/secure/attachment/12424716/LUCENE-2058.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-11-12 23:28:38.631,,,false,,,,,,,,,,,,,,,11719,,,Fri Nov 13 00:42:53 UTC 2009,New,Patch Available,,,,,,"0|i04t4v:",25985,,,,,,,,,"12/Nov/09 16:11;rcmuir;if no one objects, I would like to commit this to 3.0

I do not think it is a new feature, just a cleanup. ","12/Nov/09 23:28;uschindler;+1, the usage info is perfect, goes to stderr and exits with 1, just like any other unix tool *g*","13/Nov/09 00:42;rcmuir;I tested: same results with these two patches as my old hackish setup.

Committed revision 835674.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Systemrequirements should say 1.5 instead of 1.4,LUCENE-2038,12439994,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,simonw,simonw,06/Nov/09 01:37,25/Nov/09 16:47,30/Sep/19 08:38,11/Nov/09 10:28,,,,,,,,,,,3.0,,,,general/website,,,0,,,,The website still says Java 1.4 but it should say 1.5,,,,,,,,,,,,,,,,"06/Nov/09 01:37;simonw;LUCENE-2038.patch;https://issues.apache.org/jira/secure/attachment/12424177/LUCENE-2038.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-11-11 10:28:46.674,,,false,,,,,,,,,,,,,,,11739,,,Wed Nov 11 10:28:46 UTC 2009,New,Patch Available,,,,,,"0|i04t9b:",26005,,,,,,,,,"11/Nov/09 10:28;uschindler;Committed revision: 834833

In general, this should be fixed in a version-specific system requirements page (see LUCENE-1154)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DisjunctionMaxQuery -  Iterator code to  for ( A  a : container ) construct,LUCENE-1985,12438312,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,kaykay.unique,kaykay.unique,16/Oct/09 16:14,25/Nov/09 16:47,30/Sep/19 08:38,16/Oct/09 16:31,,,,,,,,,,,3.0,,,,,,,0,,,,For better readability  - converting the Iterable<T> to  for ( A  a : container ) constructs that is more intuitive to read. ,,,,,,,,,,,,,,,,"16/Oct/09 16:15;kaykay.unique;LUCENE-1985.patch;https://issues.apache.org/jira/secure/attachment/12422363/LUCENE-1985.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-16 16:31:55.4,,,false,,,,,,,,,,,,,,,11786,,,Fri Oct 16 18:44:06 UTC 2009,New,Patch Available,,,,,,"0|i04tl3:",26058,,,,,,,,,"16/Oct/09 16:31;uschindler;Committed revision: 825989

Thanks Kay Kay! For further Java5 fixes, just add it to LUCENE-1257.","16/Oct/09 18:44;kaykay.unique;Thanks Uwe. 

Added another patch to LUCENE-1257 to get away from some of the casting that is not necessary given that LUCENE-1984 and LUCENE-1985 are in now ( with generics ). ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index Splitter,LUCENE-1959,12437559,,New Feature,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,jasonrutherglen,jasonrutherglen,07/Oct/09 23:19,25/Nov/09 16:47,30/Sep/19 08:38,13/Oct/09 15:14,2.9,,,,,,,,,,3.0,,,,core/index,,,0,,,,"If an index has multiple segments, this tool allows splitting those segments into separate directories.  ",,,,,,,,,,,,,,,,"08/Oct/09 12:19;mikemccand;LUCENE-1959.patch;https://issues.apache.org/jira/secure/attachment/12421623/LUCENE-1959.patch","08/Oct/09 00:21;jasonrutherglen;LUCENE-1959.patch;https://issues.apache.org/jira/secure/attachment/12421593/LUCENE-1959.patch","08/Oct/09 22:01;uschindler;mp-splitter-inline.patch;https://issues.apache.org/jira/secure/attachment/12421670/mp-splitter-inline.patch","08/Oct/09 20:28;ab;mp-splitter.patch;https://issues.apache.org/jira/secure/attachment/12421658/mp-splitter.patch","09/Oct/09 00:05;ab;mp-splitter2.patch;https://issues.apache.org/jira/secure/attachment/12421688/mp-splitter2.patch","09/Oct/09 09:47;ab;mp-splitter3.patch;https://issues.apache.org/jira/secure/attachment/12421722/mp-splitter3.patch","09/Oct/09 11:16;ab;mp-splitter4.patch;https://issues.apache.org/jira/secure/attachment/12421727/mp-splitter4.patch","10/Oct/09 07:41;koji;mp-splitter5.patch;https://issues.apache.org/jira/secure/attachment/12421800/mp-splitter5.patch",,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2009-10-08 09:39:05.624,,,false,,,,,,,,,,,,,,,11812,,,Tue Oct 13 15:14:38 UTC 2009,New,,,,,,,"0|i04tqv:",26084,,,,,,,,,"08/Oct/09 00:21;jasonrutherglen;First cut of the index splitter which allows listing segments, copying segments to a new directory, and removing segments from a directory.","08/Oct/09 09:39;mikemccand;Looks great, thanks Jason!  I just tweaked the javadoc to this:

/**
 * Command-line tool that enables listing segments in an
 * index, copying specific segments to another index, and
 * deleting segments from an index.
 *
 * <p><b>NOTE</b>: The tool is experimental and might change
 * in incompatible ways in the next release.  You can easily
 * accidentally remove segments from your index so be
 * careful!
 */

My inclination would be to commit this today (ie for 3.0), since it's such an isolated change, but we have said that 3.0 would only turnaround removal of deprecated APIs, cutover to Java 1.5 features, and bug fixes, so if anyone objects to my committing this for 3.0, please speak up soon!
","08/Oct/09 10:03;ab;I'm of a split mind about this splitter ;) in the sense that I'm not sure how useful it is - if your input is an optimized index then it has just 1 segment, so this tool won't be able to split it, right?

AFAIK a similar functionality can be implemented also using two other methods that would work on indexes with any number of segments: one method is trivial, based on a ""delete/IndexWriter.addIndexes/undeletAll"" loop that requires multiple passes over input data, the other would use the same method as SegmentMerger uses, i.e. working with FieldsWriter, FormatPostings*Consumer, TermVectorsWriter, etc. for a single-pass splitting.

So I guess I'm -0 on this index splitting method, because I think we can do it better.
","08/Oct/09 10:42;uschindler;I would put it into contrib (misc next to IndexNormModifier which is also command line), as it is a utility tool. I see no real reason to have it in core. We have then all flexibility to change and optimize it, as Andrzey suggested.

One thing against this tool in its current form: To copy the files it should use the directory abstraction lay and not use java.io directly. So open IndexInput/IndexOutput to copy the files.","08/Oct/09 11:53;mikemccand;bq. I would put it into contrib

+1, I'll do that.

bq. To copy the files it should use the directory abstraction lay and not use java.io directly.

I agree, that'd be nice, but I don't think really necessary before committing... it can be another future improvement.  But, we should not the limitations of the tool; I'll add javadocs.

Jason do you want to address any of these issues now (before committing to contrib)?","08/Oct/09 12:03;markrmiller@gmail.com;bq. To copy the files it should use the directory abstraction lay and not use java.io directly.

I'd use Channels instead - generally much faster.","08/Oct/09 12:05;markrmiller@gmail.com;bq. So I guess I'm -0 on this index splitting method, because I think we can do it better.

Improvements welcome :) No reason not to start somewhere though.","08/Oct/09 12:12;mikemccand;bq. No reason not to start somewhere though.

+1

Progress not perfection!","08/Oct/09 12:19;mikemccand;New patch attached: move to contrib/misc, renamed TestFileSplitter -> TestIndexSplitter, added javadocs noting the limitations, added CHANGES entry.  I'll commit shortly.","08/Oct/09 12:46;markrmiller@gmail.com;small opt - you might switch it to reuse the buffer between files.","08/Oct/09 12:52;mikemccand;bq. small opt - you might switch it to reuse the buffer between files.

OK I just committed that!","08/Oct/09 12:52;mikemccand;Thanks Jason!","08/Oct/09 20:28;ab;Here's my submission to the index splitting race ;) This version implements the multi-pass method that uses loops of delete/addIndexes/undelete.","08/Oct/09 20:37;markrmiller@gmail.com;Nice! Lets add it to the mix - I'm guessing Jason's is quite a bit faster for splitting segs and this one nicer in that it can split indivd segs. Do we keep two tools or merge them into one with options?","08/Oct/09 20:38;uschindler;Really cool!","08/Oct/09 20:39;mikemccand;Excellent!","08/Oct/09 20:47;uschindler;Small optimization for this one: You even do not need a bitset or explicite delete/undelete operations, it can be done inline. Just put the logic into isDeleted() [e.g. modulo or range comparison] and let the TermPositions also check isDeleted().","08/Oct/09 21:28;uschindler;Test fails here (I applied the patch to contrib/misc, but that should be no difference):
{code}
    [junit]
    [junit] Testsuite: org.apache.lucene.index.TestMultiPassIndexSplitter
    [junit] Tests run: 2, Failures: 0, Errors: 2, Time elapsed: 1,11 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] Writing part 1 ...
    [junit] Writing part 1 ...
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testSplitRR(org.apache.lucene.index.TestMultiPassIndexSpli
tter):  Caused an ERROR
    [junit] null
    [junit] java.lang.AssertionError
    [junit]     at org.apache.lucene.index.SegmentMerger.mergeTermInfos(SegmentM
erger.java:600)
    [junit]     at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerge
r.java:571)
    [junit]     at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.jav
a:152)
    [junit]     at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.jav
a:128)
    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.ja
va:3367)
    [junit]     at org.apache.lucene.index.MultiPassIndexSplitter.split(MultiPas
sIndexSplitter.java:92)
    [junit]     at org.apache.lucene.index.TestMultiPassIndexSplitter.testSplitR
R(TestMultiPassIndexSplitter.java:60)
    [junit]
    [junit]
    [junit] Testcase: testSplitSeq(org.apache.lucene.index.TestMultiPassIndexSpl
itter): Caused an ERROR
    [junit] null
    [junit] java.lang.AssertionError
    [junit]     at org.apache.lucene.index.SegmentMerger.mergeTermInfos(SegmentM
erger.java:600)
    [junit]     at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerge
r.java:571)
    [junit]     at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.jav
a:152)
    [junit]     at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.jav
a:128)
    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.ja
va:3367)
    [junit]     at org.apache.lucene.index.MultiPassIndexSplitter.split(MultiPas
sIndexSplitter.java:92)
    [junit]     at org.apache.lucene.index.TestMultiPassIndexSplitter.testSplitS
eq(TestMultiPassIndexSplitter.java:102)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.index.TestMultiPassIndexSplitter FAILED
    [junit] Testsuite: org.apache.lucene.index.TestTermVectorAccessor
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 1,079 sec
{code}","08/Oct/09 22:01;uschindler;Here is my inline version without OpenBitSet. The test results are the same (does not pass at same position), but shows, what I meant.

Patch contains contrib/misc path prefix.","08/Oct/09 22:45;jasonrutherglen;I'm using the IndexSplitter to divide a 100GB index into roughly equals parts and deploying into production.  Then will clean up the patch.  

I'm not sure why we'd want to use FSDir to copy files as the input parameters are filesystem paths?","09/Oct/09 00:05;ab;Now, the mystery is why this test passed when executed in Eclipse - that assert should've tripped then as well. I remember now why I used bitsets - we actually need to know the number of deleted docs to return proper value in IR.numDocs(), and this value is not easy to calculate without actually doing this intersection. Your version looked more elegant, but it still tripped that assert (for good reason). I fixed my version so that it passes the tests when executed through ant (and it still passes in Eclipse, huh .. ;) ).","09/Oct/09 08:51;uschindler;Ah ok, I didn't look into the test failure yesterday (was too late in the evening), I only wanted to make a quick design and if it would generally work.
But you are right, the numDocs() return value is incorrect, leading to a failure in this test. But as the test pass in your test environment, the assertion in the SegmentMerger seems not important for functionality. So in general my code and your first code would work correct. I do not know how costly the initial building of the BitSet used for the input reader's deleted docs is, but one possibility would be to only build/use the additional bitset, if hasDeletions() on the original index returns true.

Thanks for clarifying.","09/Oct/09 09:28;ab;The test passed in Eclipse only - ""ant test"" ran from cmdline didn't pass without this fix, so I suspect my Eclipse is to blame for hiding the problem. Re: lazy allocation of bitset - good point, I'll make this change.","09/Oct/09 09:47;ab;As suggested by Uwe, don't allocate the old deletions bitset if there are no deletions.","09/Oct/09 10:00;mikemccand;Good progress!  Andrzej, how about you go ahead & commit yourself?","09/Oct/09 11:16;ab;I moved the files in this patch to contrib/misc and updated the contrib/CHANGES.txt. If there are no objections I'll commit it soon.","10/Oct/09 07:41;koji;I added small fix. If we have 13 docs (docid=0,1,2,...,12) and numParts=3, 12th doc is missing with -seq mode. I changed this:

{code}
// above range
for (int j = hi; j < maxDoc; j++) {
   input.deleteDocument(j);
}
{code}

to:

{code}
// above range
if( i < numParts - 1 ){
  for (int j = hi; j < maxDoc; j++) {
     input.deleteDocument(j);
  }
}
{code}
","13/Oct/09 14:44;ab;Indeed, thanks for the fix - I'll commit this.","13/Oct/09 14:57;ab;Committed revision 824798.","13/Oct/09 15:14;mikemccand;Thanks Andrzej!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make all classes that have a close() methods instanceof Closeable (Java 1.5),LUCENE-1945,12437220,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,uschindler,uschindler,04/Oct/09 07:25,25/Nov/09 16:47,30/Sep/19 08:38,18/Oct/09 22:22,2.9,,,,,,,,,,3.0,,,,,,,0,,,,This should be simple.,,,,,,,,,,,,,,,,"18/Oct/09 22:19;uschindler;LUCENE-1945.patch;https://issues.apache.org/jira/secure/attachment/12422512/LUCENE-1945.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-18 23:59:56.965,,,false,,,,,,,,,,,,,,,11825,,,Sun Oct 18 23:59:56 UTC 2009,New,,,,,,,"0|i04ttz:",26098,,,,,,,,,"18/Oct/09 22:19;uschindler;Patch that adds Closeable to all public interfaces/super-classes that define close(). Package-private classes inside oal.index are not changed (as they often only define package-private close())

Will commit soon.","18/Oct/09 22:22;uschindler;Committed revision: 826540","18/Oct/09 23:59;earwin;Package-private classes might as well implement public close(), nobody's gonna see this method from outside anyway.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
smartcn HHMM doc translation,LUCENE-1916,12435957,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,rcmuir,rcmuir,17/Sep/09 15:15,25/Nov/09 16:47,30/Sep/19 08:38,03/Oct/09 14:26,,,,,,,,,,,3.0,,,,general/javadocs,,,0,,,,"My coworker Patricia Peng translated the documentation and code comments for smartcn HHMM package.
",,,,,,,,,,,,,,,,"17/Sep/09 15:15;rcmuir;LUCENE-1916.patch;https://issues.apache.org/jira/secure/attachment/12419898/LUCENE-1916.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,11852,,,Sat Oct 03 14:26:47 UTC 2009,New,Patch Available,,,,,,"0|i04u0f:",26127,,,,,,,,,"03/Oct/09 14:26;rcmuir;Committed revision 821325.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When we move to java 1.5 in 3.0 we should replace all Interger, Long, etc construction with .valueOf",LUCENE-1833,12433624,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,markrmiller@gmail.com,markrmiller@gmail.com,20/Aug/09 22:42,25/Nov/09 16:47,30/Sep/19 08:38,02/Oct/09 22:17,,,,,,,,,,,3.0,,,,,,,0,,,,-128 to 128 are guaranteed to be cached and using valueOf in that case is 3.5 times faster than using contructor,,,,,,,,,,,,,,,,"02/Oct/09 20:02;uschindler;LUCENE-1833.patch;https://issues.apache.org/jira/secure/attachment/12421147/LUCENE-1833.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-08-20 22:46:31.736,,,false,,,,,,,,,,,,,,,11930,,,Fri Oct 02 22:24:06 UTC 2009,New,,,,,,,"0|i04ujb:",26212,,,,,,,,,"20/Aug/09 22:46;uschindler;...and also StringBuffers!","02/Oct/09 20:02;uschindler;This patch changes all ""new Number("" code parts with ""Number.valueOf("" (changed using find/sed). All tests pass.

I want to commit this as soon as possible, because it affects lots of files and I do not want to get this patch outdated. The StringBuffer from previous comment is in another issue.","02/Oct/09 22:17;uschindler;Committed revision: 821186

I hope, nobody has problems with merging caused by this!","02/Oct/09 22:19;markrmiller@gmail.com;bq. I hope, nobody has problems with merging caused by this!

Pfff - so many good merge tools out there today, lets not let that get in the way of this sweet rapid movement to java 1.5!

If anyone is annoyed, I'd be happy to merge any patch for them.","02/Oct/09 22:24;rcmuir;Uwe, I agree with what Mark said. 
my previous comment on LUCENE-1936 really was just me wanting to stay out of your way, not the other way around :)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Core Test should not have dependencies on the Demo code,LUCENE-486,12327485,,Test,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,gsingers,gsingers,11/Jan/06 06:15,25/Nov/09 16:47,30/Sep/19 08:38,06/Oct/09 06:07,1.4,,,,,,,,,,3.0,,,,general/build,,,0,,,,"The TestDoc.java Test file has a dependency on the Demo FileDocument code.  Some of us don't keep the Demo code around after downloading, so this breaks the build.

Patch will be along shortly",,,,,,,,,,,,,,,,"11/Jan/06 07:11;gsingers;FileDocument.java;https://issues.apache.org/jira/secure/attachment/12321853/FileDocument.java","06/Oct/09 00:04;michaelbusch;lucene-486.patch;https://issues.apache.org/jira/secure/attachment/12421356/lucene-486.patch","11/Jan/06 07:11;gsingers;testdoc.patch;https://issues.apache.org/jira/secure/attachment/12321852/testdoc.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2006-01-11 07:34:05.0,,,false,,,,,,,,,,,,,,,13263,,,Tue Oct 06 06:07:06 UTC 2009,,,,,,,,"0|i052uf:",27558,,,,,,,,,"11/Jan/06 07:11;gsingers;Attached is a new file FileDocument that lives under the Test structure, and changes to the appropriate Tests (TestDoc and IndexTest) that use this class.","11/Jan/06 07:34;cutting;Why should we replicate this file in the t est tree?  I don't see the need to make unit tests independent of the demo code.  Why do you remove the demo code?","11/Jan/06 08:40;gsingers;I am not happy with the replication thing, but if you can't build something in the main source tree without having the ""demo"" code then that just doesn't feel right.  What happens when someone changes the demo?  Now we have to keep this demo code exactly the same lest we also want to update the dependencies in the main source as well.  I suppose it isn't a big deal since it is only one file, but it just seems like an unnecessary dependency.  Ideally, the FileDocument would live in some common place, but I am not sure where that would be.

I don't include the demo source under my source tree in intellij, so it always complains at me (I hate those red squiggly marks),  but I suppose I can get over it for this one file.","11/Jan/06 20:34;ehatcher;I concur with Grant on this - the dependency from test to demo has caused me annoyance as well.   I'm in favor of a fix to it, but haven't looked at Grant's solution yet.","12/Jan/06 02:46;cutting;Grant, the test code is not in the ""main"" source tree, it's in the ""test"" source tree, which depends on ""core"" and ""demo"".  I still don't see what harm that dependency causes, aesthetically or otherwise.  Perhaps such dependencies are awkward in intellij?  Is that the issue?","12/Jan/06 03:49;gsingers;Doug,

The intellij thing is just a symptom.  I think the bigger issues are the consequences of the demo being changed and that it just doesn't feel right.  
  
In my mind, the Test code is for testing the core, not the demo.  The demo, in my mind is a sample application using the core and has nothing to do with testing the core.  To me the demo feels more akin to the contrib area then the core area and I think you would agree that the core tests should not be depend on anything in contrib, right?

At any rate, I am not trying to create an aesthetics war here, so at this point I will leave it to the committers to decide.
","12/Jan/06 04:05;cutting;The more stuff that's regularly compiled & tested, the better.  We don't want the demo or contrib code to become neglected.  Removing dependencies from the core is important.  But I don't see that as a priority for test code.

My primary objection to making tests dependent on contrib would be that it might substantially slow the compile & test loop, to the degree that folks would run tests less frequently.  The demo is small enough that I don't think this is a concern.
","12/Jan/06 04:48;gsingers;>The more stuff that's regularly compiled & tested, the better. 

Couldn't agree more.

>We don't want the demo or contrib code to become neglected. Removing dependencies from the 
>core is important. But I don't see that as a priority for test code. 

Hence, me giving this bug a priority of Minor

","23/Sep/09 12:26;michaelbusch;I still think we should remove the dependency that test has on demo. Looking into the code you can replace the call of FileDocument in TestDoc with just 2 lines:
{code}
Document doc = new Document();
doc.add(new Field(""contents"", new FileReader(file)));
{code}

With this change alone we could remove the dependency. I think we should do that because it makes the build cleaner and I don't think we'll pay less attention to the demo code in the future just because we remove this classpath dependency. Any objections? Otherwise I'll reopen this issue and attach a patch.","23/Sep/09 12:35;gsingers;+1","06/Oct/09 00:04;michaelbusch;Will have to commit the change in TestDoc also to the compatibility tests branch.","06/Oct/09 06:07;michaelbusch;Committed revision 822139.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TokenStream/Tokenizer/TokenFilter/Token javadoc improvements,LUCENE-2008,12439014,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,nezda,nezda,24/Oct/09 21:09,07/Nov/09 14:57,30/Sep/19 08:38,26/Oct/09 12:24,2.9,,,,,,,,,,2.9.1,3.0,,,,,,0,,,,"Some of the javadoc for the new TokenStream/Tokenizer/TokenFilter/Token APIs had javadoc errors.  To the best of my knowledge, I corrected these and refined the copy a bit.",,,,,,,,,,,,,,,,"24/Oct/09 21:10;nezda;javadoc.patch;https://issues.apache.org/jira/secure/attachment/12423129/javadoc.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-25 12:53:33.442,,,false,,,,,,,,,,,,,,,11765,,,Sat Nov 07 14:57:31 UTC 2009,New,,,,,,,"0|i04tfz:",26035,,,,,,,,,"24/Oct/09 21:10;nezda;Patch against trunk (829452)","25/Oct/09 12:53;mikemccand;Patch looks good, thanks Luke!  I'll commit to trunk shortly.

Unfortunately, the patch doesn't apply cleanly to 2.9.x -- can you post a version that does?  (Or we can commit only to trunk...).","26/Oct/09 12:24;mikemccand;OK I back-ported the javadoc improvements to 2.9.x, and just committed.  Thanks Luke!","07/Nov/09 14:57;mikemccand;Bulk close all 2.9.1 issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastVectorHighlighter: small fragCharSize can cause StringIndexOutOfBoundsException ,LUCENE-1953,12437498,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,koji,koji,07/Oct/09 12:20,07/Nov/09 14:57,30/Sep/19 08:38,08/Oct/09 14:48,2.9,,,,,,,,,,2.9.1,,,,modules/highlighter,,,0,,,,"If fragCharSize is smaller than Query string, StringIndexOutOfBoundsException is thrown.",,,,,,,,,,,,,,,,"07/Oct/09 12:25;koji;LUCENE-1953.patch;https://issues.apache.org/jira/secure/attachment/12421522/LUCENE-1953.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-07 14:57:36.065,,,false,,,,,,,,,,,,,,,11818,,,Sat Nov 07 14:57:30 UTC 2009,New,,,,,,,"0|i04ts7:",26090,,,,,,,,,"07/Oct/09 12:25;koji;Patch that includes the fix and test cases. Will commit soon.","07/Oct/09 14:57;markrmiller@gmail.com;Koji can't commit to the 2.9 branch can he? Not sure how that karma works - we can do it for him if not - lets wait to resolve until thats done though.","08/Oct/09 13:42;koji;Committed revision 823170 in trunk.

bq. Koji can't commit to the 2.9 branch can he? Not sure how that karma works - we can do it for him if not - lets wait to resolve until thats done though.

I couldn't. The error I got:

{code}
[koji@macbook COMMIT-1953-lucene_2_9]$ svn up
At revision 823174.
[koji@macbook COMMIT-1953-lucene_2_9]$ svn commit -m ""LUCENE-1953: FastVectorHighlighter: small fragCharSize can cause StringIndexOutOfBoundsException""
Sending        contrib/CHANGES.txt
svn: Commit failed (details follow):
svn: CHECKOUT of '/repos/asf/!svn/ver/818600/lucene/java/branches/lucene_2_9/contrib/CHANGES.txt': 403 Forbidden (https://svn.apache.org)
{code}

Can you commit it for me please?","08/Oct/09 14:32;markrmiller@gmail.com;just committed Koji.","08/Oct/09 14:48;koji;Thanks, Mark!

BTW, I cannot assign myself because I cannot find ""Assign"" link in Lucene JIRA. Could anyone solve this problem?","08/Oct/09 15:35;markrmiller@gmail.com;I think that means someone has to give you JIRA power and hasn't yet - can't remember who to bug on that - Hoss or Grant I think? Perhaps the right person is watching ...","07/Nov/09 14:57;mikemccand;Bulk close all 2.9.1 issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
short circuit FuzzyQuery.rewrite when input token length is small compared to minSimilarity,LUCENE-1124,12385803,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,markrmiller@gmail.com,hossman,hossman,08/Jan/08 22:41,07/Nov/09 14:57,30/Sep/19 08:38,16/Oct/09 17:38,,,,,,,,,,,2.9.1,3.0,,,core/query/scoring,,,1,,,,"I found this (unreplied to) email floating around in my Lucene folder from during the holidays...

{noformat}
From: Timo Nentwig
To: java-dev
Subject: Fuzzy makes no sense for short tokens
Date: Mon, 31 Dec 2007 16:01:11 +0100
Message-Id: <200712311601.12255.lucene@nitwit.de>

Hi!

it generally makes no sense to search fuzzy for short tokens because changing
even only a single character of course already results in a high edit
distance. So it actually only makes sense in this case:

           if( token.length() > 1f / (1f - minSimilarity) )

E.g. changing one character in a 3-letter token (foo) results in an edit
distance of 0.6. And if minSimilarity (which is by default: 0.5 :-) is higher
we can save all the expensive rewrite() logic.
{noformat}

I don't know much about FuzzyQueries, but this reasoning seems sound ... FuzzyQuery.rewrite should be able to completely skip all TermEnumeration in the event that the input token is shorter then some simple math on the minSimilarity.  (i'm not smart enough to be certain that the math above is right however ... it's been a while since i looked at Levenstein distances ... tests needed)
",,,,,,,,,,,,,,,,"16/Oct/09 16:31;mikemccand;LUCENE-1124.patch;https://issues.apache.org/jira/secure/attachment/12422367/LUCENE-1124.patch","04/Jan/09 15:35;markrmiller@gmail.com;LUCENE-1124.patch;https://issues.apache.org/jira/secure/attachment/12397082/LUCENE-1124.patch","17/Aug/08 14:44;markrmiller@gmail.com;LUCENE-1124.patch;https://issues.apache.org/jira/secure/attachment/12388383/LUCENE-1124.patch","17/Aug/08 14:35;markrmiller@gmail.com;LUCENE-1124.patch;https://issues.apache.org/jira/secure/attachment/12388381/LUCENE-1124.patch",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2008-01-09 05:57:20.13,,,false,,,,,,,,,,,,,,,12621,,,Sat Nov 07 14:57:29 UTC 2009,New,,,,,,,"0|i04yxb:",26923,,,,,,,,,"09/Jan/08 05:57;otis;This makes sense, Timo.  Could you please attach a patch + a simple unit test?
","17/Aug/08 14:35;markrmiller@gmail.com;This optimization is correct. Highlights some interesting things about fuzzy query as well i.e. if you put a minsim of 0.9, your query term *has* to be over 10 chars to have any hope of getting a match. For the default of 0.5 its 2 chars, so in the common case the optimization doesn't do much good, and you do have to pay for the check every time no matter what. For larger minsim values though, this will turn a lot of fuzz queries into no ops.

- Mark","17/Aug/08 14:44;markrmiller@gmail.com;Computing the needed term length in the constructor is probably better.","25/Aug/08 19:26;hossman;I don't deal with FuzzyQueries much, but skimming this issue it seems to touch on a lot of hte same things that spawned the creation of the ""mm"" syntax for specifying the ""minNumberShouldMatch"" value on BooleanQueries in the Solr dismax query parser...

http://lucene.apache.org/solr/api/org/apache/solr/util/doc-files/min-should-match.html

...perhaps something similar could be used to allow people to specify simpel expressions for dictating the ""fuzzyiness"" of short input vs medium length input, vs long input.","04/Jan/09 15:35;markrmiller@gmail.com;Updated to trunk.

Im going to commit in few days if no one objects.","18/Jan/09 18:33;markrmiller@gmail.com;Thanks! Committed.","16/Oct/09 16:29;mikemccand;This fix breaks the case when the exact term is present in the index.","16/Oct/09 16:31;mikemccand;Attach patch (based on 2.9) showing the bug, along with the fix.  Instead of rewriting to empty BooleanQuery when prefix term is not long enough, I rewrite to TermQuery with that prefix.  This way the exact term matches.

I'll commit shortly to trunk & 2.9.x.","07/Nov/09 14:57;mikemccand;Bulk close all 2.9.1 issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Confusing Javadoc in Searchable.java,LUCENE-1900,12435121,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,markrmiller@gmail.com,nyh,nyh,08/Sep/09 15:07,25/Sep/09 16:23,30/Sep/19 08:38,09/Sep/09 13:53,2.9,,,,,,,,,,2.9,,,,core/search,,,0,,,,"In Searchable.java, the javadoc for maxdoc() is:

  /** Expert: Returns one greater than the largest possible document number.
   * Called by search code to compute term weights.
   * @see org.apache.lucene.index.IndexReader#maxDoc()

The qualification ""expert"" and the statement ""called by search code to compute term weights"" is a bit confusing, It implies that maxdoc() somehow computes weights, which is obviously not true (what it does is explained in the other sentence). Maybe it is used as one factor of the weight, but do we really need to mention this here? ",,,,,,,,,,,,,,,,"08/Sep/09 17:56;markrmiller@gmail.com;LUCENE-1900.patch;https://issues.apache.org/jira/secure/attachment/12418944/LUCENE-1900.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-09-08 15:32:27.783,,,false,,,,,,,,,,,,,,,11867,,,Wed Sep 09 13:53:51 UTC 2009,New,Patch Available,,,,,,"0|i04u3z:",26143,,,,,,,,,"08/Sep/09 15:32;markrmiller@gmail.com;Yeah - could be more clear.

I agree that there doesn't appear to be a need to mark it as expert. And I guess we should just remove the part that mentions its involved in the weight? Or re word.

""This is used as an approximation of the number of documents in the index for scoring calculations.""

Or something to that effect (namely, better). Perhaps approximation isnt even needed - 

""Because un-merged deleted docs still contribute to index statistics, this is used as the number of documents in the index for scoring calculations.""

or ...","08/Sep/09 17:17;marvin;maxDoc() isn't just used for calculating weights.  It's also used for e.g.
figuring out how big your bit vector needs to be in order to accommodate the
largest doc in the collection.

My vote would be to just strip that extra comment about calculating term
weights.
","08/Sep/09 17:24;markrmiller@gmail.com;bq. My vote would be to just strip that extra comment about calculating term weights.

+1 - unless someone else comments, I'm just going to do that.
","08/Sep/09 17:36;doronc;hi.., I think the 'expert' is here because it reveals internal information which users should not rely on unless understanding exactly what they are doing with it. - well, at least as internal as are the unstable docids - on the other hand, IndexReader.maxDoc() is not marked 'expert', so perhaps this one also should not be marked 'expert'.

Similarity.idfExplain(Term,Searcher) explains nicely why maxDoc() is used rather than numDocs():
{noformat}
   * Note that {@link Searcher#maxDoc()} is used instead of
   * {@link org.apache.lucene.index.IndexReader#numDocs()} because it is
   * proportional to {@link Searcher#docFreq(Term)} , i.e., when one is
   * inaccurate, so is the other, and in the same direction.
{noformat}","08/Sep/09 17:41;markrmiller@gmail.com;Thanks Doron - I was going to err on leaving the expert and just stripping:

""Called by search code to compute term weights.""

Sounds like your not opposed to that?","08/Sep/09 17:47;doronc;{quote}
stripping: ""Called by search code to compute term weights.""
Sounds like your not opposed to that?
{quote}

Yes, I agree, go ahead...

While looking at this though, how about other ""Expert"" and ""called by"" javadoc comments in this class? - like the one in doc(int i) - I am not sure what's so expert about it..? Also there are 3 more 'called by' javadoc comments in that class, are they really needed?
","08/Sep/09 17:52;markrmiller@gmail.com;Good point - we should remove that expert and the called by's - one of them references a deprecated class (HitCollector), so it def needs to be changed in either case.

I'll make a quick patch.","08/Sep/09 18:11;doronc;Mark, thanks for removing the 'called by's

After applying the patch there are still a few 'expert' statements: 

* maxDoc()  - I think you wanted to remove this one?
* docFreq() - it is not marked expert in IndexReader... should it be marked so here?
* docFeqs() - not sure here

There are 3 more 'expert' sttmnts which seem okay to me.","08/Sep/09 18:16;markrmiller@gmail.com;I figured I'd leave em -

but I do agree that it makes more sense to pull the expert from them. Will add to the patch.

If it doesn't belong on docFreq, it doesn't belong on docFreqs (which was just added to reduce chatter over RMI it appears by the comment)","08/Sep/09 18:45;marvin;IMO, maxDoc(), docFreq(), and docFreqs() are all expert, because they all
require an understanding of the deletions mechanism to grasp their behavior.  

I'd vote for adding the ""expert"" tag to IndexReader.maxDoc() before stripping
it from those.","08/Sep/09 18:47;markrmiller@gmail.com;Good point. I've jumped sides.

bq. I'd vote for adding the ""expert"" tag to IndexReader.maxDoc() before stripping it from those.

+1

*edit

Actually - as I look over IndexReader, I think its just that the context changes - what is expert in Searchable is not necessarily expert in IndexReader - moving to that level already has more advanced implications, and current labeling of expert is slanted to whats more difficult in reference of IndexReader - its all expert compared to use Searchable.

I think that argues for just leaving as is (the current patch).","08/Sep/09 19:20;doronc;{quote}
what is expert in Searchable is not necessarily expert in IndexReader - moving to that level already has more advanced implications
{quote}

I agree (even though IndexReader is not marked 'expert').

{quote}
I think that argues for just leaving as is (the current patch).
{quote}

+1
","09/Sep/09 13:53;markrmiller@gmail.com;thanks all",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Demo HTMLParser compares StringBuffer to an empty String with .equals,LUCENE-1892,12434967,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,markrmiller@gmail.com,markrmiller@gmail.com,markrmiller@gmail.com,07/Sep/09 03:14,25/Sep/09 16:23,30/Sep/19 08:38,07/Sep/09 18:33,,,,,,,,,,,2.9,,,,core/other,,,0,,,,,,,,,,,,,,,,,,,,"07/Sep/09 15:51;markrmiller@gmail.com;LUCENE-1892.patch;https://issues.apache.org/jira/secure/attachment/12418823/LUCENE-1892.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,11875,,,Mon Sep 07 15:51:52 UTC 2009,New,Patch Available,,,,,,"0|i04u5r:",26151,,,,,,,,,"07/Sep/09 15:51;markrmiller@gmail.com;checks stringbuffer.length == 0 instead",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spatial uses java util logging that causes needless minor work (multiple string concat, a method call) due to not checking log level",LUCENE-1891,12434966,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,markrmiller@gmail.com,markrmiller@gmail.com,07/Sep/09 01:26,25/Sep/09 16:23,30/Sep/19 08:38,07/Sep/09 02:13,,,,,,,,,,,2.9,,,,modules/spatial,,,0,,,,"Not sure there should be logging here - just used in two spots and looks more for debug - but if its going to be there, should check for isFineEnabled.",,,,,,,,,,,,,,,,"07/Sep/09 01:27;markrmiller@gmail.com;LUCENE-1891.patch;https://issues.apache.org/jira/secure/attachment/12418770/LUCENE-1891.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,11876,,,Mon Sep 07 02:13:26 UTC 2009,New,Patch Available,,,,,,"0|i04u5z:",26152,,,,,,,,,"07/Sep/09 02:13;markrmiller@gmail.com;open the door and let em in.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Javadoc,LUCENE-1886,12434650,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,hossman,brainlounge,brainlounge,02/Sep/09 15:57,25/Sep/09 16:23,30/Sep/19 08:38,03/Sep/09 17:59,,,,,,,,,,,2.9,,,,modules/analysis,,,0,,,,,,,,,,,,,,,,,,,,"02/Sep/09 15:58;brainlounge;javadoc.patch;https://issues.apache.org/jira/secure/attachment/12418395/javadoc.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-09-02 16:55:53.421,,,false,,,,,,,,,,,,,,,11879,,,Thu Sep 03 17:59:09 UTC 2009,New,Patch Available,,,,,,"0|i04u7j:",26159,,,,,,,,,"02/Sep/09 16:55;markrmiller@gmail.com;looks nice, thanks Bernd.","03/Sep/09 17:59;hossman;Committed revision 811060.

Thanks Bernd",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typos in CHANGES.txt and contrib/CHANGES.txt prior to 2.9 release,LUCENE-1883,12434567,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,sarowe,sarowe,01/Sep/09 23:31,25/Sep/09 16:23,30/Sep/19 08:38,02/Sep/09 15:27,,,,,,,,,,,2.9,,,,core/other,,,0,,,,"I noticed a few typos in CHANGES.txt and contrib/CHANGES.txt.  (Once they make it past a release, they're set in stone...)

Will attach a patch shortly.",,,,,,,,,,,,,,,,"01/Sep/09 23:32;sarowe;LUCENE-1883.patch;https://issues.apache.org/jira/secure/attachment/12418314/LUCENE-1883.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-09-02 15:06:11.918,,,false,,,,,,,,,,,,,,,11882,,,Wed Sep 02 15:26:58 UTC 2009,New,Patch Available,,,,,,"0|i04u87:",26162,,,,,,,,,"01/Sep/09 23:32;sarowe;patch with typos corrected","02/Sep/09 15:06;markrmiller@gmail.com;bq. Once they make it past a release, they're set in stone..

Why?","02/Sep/09 15:18;sarowe;I searched just now, but couldn't find, an email thread I recall on java-dev between Doug Cutting and the RM at that point (several years ago) about modifying past releases' CHANGES.txt entries.  Doug's position, articulated both in that thread (and elsewhere, IIRC), was that people depend on being able to do a diff between CHANGES.txt versions, so once a release was cut, the release notes should never change thereafter.
","02/Sep/09 15:25;markrmiller@gmail.com;interesting - someone forgot to put up the sign ;) I've posted a back spell fix before that was applied (wasn't me ;) it was before someone was nuts enough to give me write access to Lucene) 

Thanks a lot for the patch - you've got some eagle eyes man - I wouldn't have caught those name misspellings.","02/Sep/09 15:26;markrmiller@gmail.com;hmm - got marked invalid - weird - reopen to resolve right",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make contrib/collation/(ICU)CollationKeyAnalyzer constructors public,LUCENE-1880,12434456,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,sarowe,sarowe,31/Aug/09 21:04,25/Sep/09 16:23,30/Sep/19 08:38,31/Aug/09 22:35,,,,,,,,,,,2.9,,,,modules/other,,,0,,,,"In contrib/collation, the constructors for CollationKeyAnalyzer and ICUCollationKeyAnalyzer are package private, and so are effectively unusable.",,,,,,,,,,,,,,,,"31/Aug/09 21:06;sarowe;LUCENE-1880.patch;https://issues.apache.org/jira/secure/attachment/12418177/LUCENE-1880.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-08-31 22:35:01.374,,,false,,,,,,,,,,,,,,,11885,,,Mon Aug 31 22:35:01 UTC 2009,New,Patch Available,,,,,,"0|i04u8v:",26165,,,,,,,,,"31/Aug/09 21:06;sarowe;trivial patch adding public access to currently package private constructors","31/Aug/09 22:35;mikemccand;Thanks Steve, I'll commit shortly!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some contrib packages are missing a package.html,LUCENE-1876,12434381,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,rcmuir,markrmiller@gmail.com,markrmiller@gmail.com,30/Aug/09 17:34,25/Sep/09 16:23,30/Sep/19 08:38,03/Sep/09 13:04,,,,,,,,,,,2.9,,,,modules/other,,,0,,,,"Dunno if we will get to this one this release, but a few contribs don't have a package.html (or a good overview that would work as a replacement) - I don't think this is hugely important, but I think it is important - you should be able to easily and quickly read a quick overview for each contrib I think.

So far I have identified collation and spatial.",,,,,,,,,,,,,,,,"03/Sep/09 04:13;rcmuir;LUCENE-1876.patch;https://issues.apache.org/jira/secure/attachment/12418468/LUCENE-1876.patch","02/Sep/09 15:32;rcmuir;LUCENE-1876.patch;https://issues.apache.org/jira/secure/attachment/12418393/LUCENE-1876.patch","31/Aug/09 20:24;sarowe;collation-package.html;https://issues.apache.org/jira/secure/attachment/12418173/collation-package.html",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-08-31 20:24:33.517,,,false,,,,,,,,,,,,,,,11888,,,Thu Sep 03 13:04:24 UTC 2009,New,,,,,,,"0|i04u9r:",26169,,,,,,,,,"30/Aug/09 17:37;markrmiller@gmail.com;also db and remote","31/Aug/09 20:24;sarowe;Here is {{package.html}} for contrib/collation, with content mostly stolen from class comments and test cases.

The Turkish collation example is mostly stolen from Robert Muir's TestTurkishCollation.java from LUCENE-1581.","31/Aug/09 20:47;rcmuir;Steven, thanks, I think this writeup really is hugely important for this package.

the existing javadocs assumes you know what collation does, and the changes entry really understates what you can do with this:
{noformat}
 2. LUCENE-1435: Added contrib/collation, a CollationKeyFilter
    allowing you to convert tokens into CollationKeys encoded usign
    IndexableBinaryStringTools.  This allows for faster RangQuery when
    a field needs to use a custom Collator.  (Steven Rowe via Mike
    McCandless)
{noformat}

So I think its good for someone to understand they can use this package for more than just range queries: sorting and searching!
","02/Sep/09 15:32;rcmuir;Steven Rowe's supplied docs for collation, plus some package.htmls for a couple more contribs.
","02/Sep/09 15:38;markrmiller@gmail.com;You want to take this issue Robert?","02/Sep/09 15:40;rcmuir;Mark, sure. I would absolutely love it if someone who has a good understanding of spatial could comment on what the packages do, I will create the html files and update the patch!","03/Sep/09 04:13;rcmuir;added package.htmls for the two user-level spatial pkgs (tier and geohash)

plan to commit this patch tomorrow.
","03/Sep/09 13:04;rcmuir;Committed revision 810923.

thanks Steven!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when checking tvx/fdx size mismatch, also include whether the file exists",LUCENE-1869,12434256,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,28/Aug/09 09:01,25/Sep/09 16:23,30/Sep/19 08:38,28/Aug/09 13:36,,,,,,,,,,,2.9,,,,core/index,,,0,,,,"IndexWriter checks, during flush and during merge, that the size of the index file for stored fields (*.fdx) and term vectors (*.tvx) matches how many bytes it has just written.

This originally was added for LUCENE-1282, ie, as a safety to catch the nasty ""off by 1"" JRE hotspot bug that would otherwise silently corrupt the index.

However, this check also seems to catch a different case, where the size of the file is zero.   The most recent example is LUCENE-1521.  I'd like to improve the message in the exception to include whether or not the file exists, to help understand why users are sometimes hitting this exception.  My best theory at this point is something external is removing the file out from under the IndexWriter.
",,,,,,,,,,,,,,,,"28/Aug/09 09:02;mikemccand;LUCENE-1869.patch;https://issues.apache.org/jira/secure/attachment/12417975/LUCENE-1869.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,11895,,,Fri Aug 28 09:02:53 UTC 2009,New,,,,,,,"0|i04ubb:",26176,,,,,,,,,"28/Aug/09 09:02;mikemccand;Attached patch.

The patch is trivial (only changes the String message in the already-thrown RuntimeException when there is a mismatch detected in expected vs actual size of *.fdx or *.tvx).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SnowballAnalyzer has a link to net.sf (a package that is empty and needs to be removed).,LUCENE-1863,12434176,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,markrmiller@gmail.com,markrmiller@gmail.com,markrmiller@gmail.com,27/Aug/09 14:53,25/Sep/09 16:23,30/Sep/19 08:38,27/Aug/09 16:43,,,,,,,,,,,2.9,,,,general/javadocs,,,0,,,,"need to remove net.sf and points to org.tartarus.snowball.ext. Doesn't work as a link though, so I'll also remove the @link to lose the javadoc error and broken link.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-08-27 16:43:33.014,,,false,,,,,,,,,,,,,,,11900,,,Thu Aug 27 16:43:33 UTC 2009,New,,,,,,,"0|i04ucn:",26182,,,,,,,,,"27/Aug/09 16:43;hossman;mark fixed in r808472",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"build.xml's tar task should use longfile=""gnu""",LUCENE-1854,12433980,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,markrmiller@gmail.com,markrmiller@gmail.com,markrmiller@gmail.com,25/Aug/09 20:57,25/Sep/09 16:23,30/Sep/19 08:38,25/Aug/09 22:52,,,,,,,,,,,2.9,,,,general/build,,,0,,,,"The default (used now) is the same, but we get all those nasty false warnings filling the screen.",,,,,,,,,,,,,,,,"25/Aug/09 20:58;markrmiller@gmail.com;LUCENE-1854.patch;https://issues.apache.org/jira/secure/attachment/12417657/LUCENE-1854.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,11909,,,Tue Aug 25 22:07:30 UTC 2009,New,Patch Available,,,,,,"0|i04uen:",26191,,,,,,,,,"25/Aug/09 22:07;markrmiller@gmail.com;I'll commit this shortly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide Summary Information on the Files in the Lucene index,LUCENE-1841,12433710,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,gsingers,gsingers,gsingers,21/Aug/09 17:54,25/Sep/09 16:23,30/Sep/19 08:38,23/Aug/09 05:57,,,,,,,,,,,2.9,,,,,,,0,,,,"I find myself often having to remember, by file extension, what is in a particular index file.  The information is all contained in the File Formats, but not summarized.  This patch provides a simple table that describes the extensions and provides links to the relevant section.",,,,,,,,,,,,,,,,"21/Aug/09 17:55;gsingers;LUCENE-1841.patch;https://issues.apache.org/jira/secure/attachment/12417290/LUCENE-1841.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-08-21 22:34:45.607,,,false,,,,,,,,,,,,,,,11922,,,Sun Aug 23 01:19:24 UTC 2009,New,Patch Available,,,,,,"0|i04uhj:",26204,,,,,,,,,"21/Aug/09 22:34;rcmuir;Grant this is helpful. 
nit: i noticed typo ""offest"" in the tvx section","23/Aug/09 01:19;gsingers;Committed revision 806916.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
minor/nitpick TermInfoReader bug ?,LUCENE-1832,12433623,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,markrmiller@gmail.com,markrmiller@gmail.com,20/Aug/09 22:34,25/Sep/09 16:23,30/Sep/19 08:38,21/Aug/09 16:48,,,,,,,,,,,2.9,,,,,,,0,,,,"Some code flagged by a bytecode static analyzer - I guess a nitpick, but we should just drop the null check in the if? If its null it will fall to the below code and then throw a NullPointer exception anyway. Keeping the nullpointer check implies we expect its possible - but then we don't handle it correctly.

{code}
  /** Returns the nth term in the set. */
  final Term get(int position) throws IOException {
    if (size == 0) return null;

    SegmentTermEnum enumerator = getThreadResources().termEnum;
    if (enumerator != null && enumerator.term() != null &&
        position >= enumerator.position &&
	position < (enumerator.position + totalIndexInterval))
      return scanEnum(enumerator, position);      // can avoid seek

    seekEnum(enumerator, position/totalIndexInterval); // must seek
    return scanEnum(enumerator, position);
  }

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-08-21 16:13:58.824,,,false,,,,,,,,,,,,,,,11931,,,Fri Aug 21 16:48:22 UTC 2009,New,,,,,,,"0|i04ujj:",26213,,,,,,,,,"21/Aug/09 16:13;michaelbusch;enumerator can never be null there. I'll commit the simple fix.","21/Aug/09 16:48;michaelbusch;Committed revision 806637.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
highlight-vs-vector-highlight.alg is unfair,LUCENE-1809,12433104,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,koji,koji,14/Aug/09 18:31,25/Sep/09 16:23,30/Sep/19 08:38,15/Aug/09 10:06,2.9,,,,,,,,,,2.9,,,,modules/benchmark,,,0,,,,"highlight-vs-vector-highlight.alg uses EnwikiQueryMaker which makes SpanQueries, but FastVectorHighlighter simply ignores SpanQueries.",,,,,,,,,,,,,,,,"15/Aug/09 04:05;koji;LUCENE-1809.patch;https://issues.apache.org/jira/secure/attachment/12416646/LUCENE-1809.patch","14/Aug/09 18:40;koji;LUCENE-1809.patch;https://issues.apache.org/jira/secure/attachment/12416589/LUCENE-1809.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-08-15 09:56:13.22,,,false,,,,,,,,,,,,,,,11953,,,Sat Aug 15 10:06:52 UTC 2009,New,,,,,,,"0|i04uof:",26235,,,,,,,,,"14/Aug/09 18:40;koji;The patch introduces a boolean property enwikiQueryMaker.disableSpanQueries. If it is set to true (default is false), EnwikiQueryMaker doesn't make SpanQueries.

results (elapsedSec) are:

|| ||FastVectorHL||HL||
|w/o patch|7.70|20.25|
|w/ patch|7.67|19.07|","15/Aug/09 04:05;koji;Because FastVectorHighlighter's strength is the performance when processing large docs, this patch includes:
* set max.field.length Integer.MAX_VALUE in the alg file
* introduces a new property ""highlighter.maxDocCharsToAnalyze"" and set it Integer.MAX_VALUE in the alg file
","15/Aug/09 09:56;mikemccand;Patch looks good, thanks Koji.  I'll commit shortly!","15/Aug/09 10:06;mikemccand;Thanks Koji!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add convenient constructor to PerFieldAnalyzerWrapper for Dependency Injection,LUCENE-1807,12433033,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,pcowan,pcowan,13/Aug/09 23:40,25/Sep/09 16:23,30/Sep/19 08:38,14/Aug/09 09:10,2.9,,,,,,,,,,2.9,,,,modules/analysis,,,0,,,,"It would be good if PerFieldAnalyzerWrapper had a constructor which took in an analyzer map, rather than having to repeatedly call addAnalyzer -- this would make it much easier/cleaner to use this class in e.g. Spring XML configurations.

Relatively trivial change, patch to be attached.",,,,,,,,,,,,,,,,"13/Aug/09 23:42;pcowan;LUCENE-1807.patch;https://issues.apache.org/jira/secure/attachment/12416506/LUCENE-1807.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-08-14 09:09:57.476,,,false,,,,,,,,,,,,,,,11955,,,Fri Aug 14 09:10:44 UTC 2009,New,Patch Available,,,,,,"0|i04uov:",26237,,,,,,,,,"13/Aug/09 23:42;pcowan;Patch to add extra constructor.","14/Aug/09 09:09;mikemccand;Makes sense and patch looks good... I'll commit shortly.","14/Aug/09 09:10;mikemccand;Thanks Paul!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add args to test-macro,LUCENE-1806,12433030,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,jasonrutherglen,jasonrutherglen,13/Aug/09 23:13,25/Sep/09 16:23,30/Sep/19 08:38,14/Aug/09 11:01,2.4.1,,,,,,,,,,2.9,,,,general/build,,,0,,,,Add passing args to JUnit.  (Like Solr and mainly for debugging).  ,,120,120,,0%,120,120,,,,,,,,,"14/Aug/09 00:02;jasonrutherglen;LUCENE-1806.patch;https://issues.apache.org/jira/secure/attachment/12416514/LUCENE-1806.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-08-14 11:01:08.293,,,false,,,,,,,,,,,,,,,11956,,,Fri Aug 14 11:01:08 UTC 2009,New,,,,,,,"0|i04up3:",26238,,,,,,,,,"14/Aug/09 00:02;jasonrutherglen;Adds ability to pass jvmarg(s) to JUnit task doing something like:

export ANT_ARGS=""-Dargs=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=8888""

","14/Aug/09 11:01;ehatcher;Done, thanks Jason.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup highlighter test class,LUCENE-1788,12432444,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,markrmiller@gmail.com,markrmiller@gmail.com,markrmiller@gmail.com,06/Aug/09 21:03,25/Sep/09 16:23,30/Sep/19 08:38,07/Aug/09 01:31,,,,,,,,,,,2.9,,,,modules/highlighter,,,0,,,,"cleanup highlighter test class - did some of this in another issue, but there is a bit more to do",,,,,,,,,,,,,,,,"06/Aug/09 21:12;markrmiller@gmail.com;LUCENE-1788.patch;https://issues.apache.org/jira/secure/attachment/12415778/LUCENE-1788.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,11974,,,2009-08-06 21:03:42.0,New,Patch Available,,,,,,"0|i04ut3:",26256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TokenStream API javadoc improvements,LUCENE-1760,12431416,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,markrmiller@gmail.com,michaelbusch,michaelbusch,25/Jul/09 00:12,25/Sep/09 16:23,30/Sep/19 08:38,25/Aug/09 15:20,,,,,,,,,,,2.9,,,,modules/analysis,,,0,,,,"- Change or remove experimental warnings of new TokenStream API
- Improve javadocs for deprecated Token constructors
- javadocs for TeeSinkTokenStream.SinkFilter",,,,,,,,,,,,,,,,"25/Aug/09 13:49;markrmiller@gmail.com;TokenStreamJavadoc.patch;https://issues.apache.org/jira/secure/attachment/12417615/TokenStreamJavadoc.patch","25/Jul/09 03:38;michaelbusch;lucene-1760.patch;https://issues.apache.org/jira/secure/attachment/12414493/lucene-1760.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-08-06 17:33:45.328,,,false,,,,,,,,,,,,,,,12001,,,Tue Aug 25 15:20:06 UTC 2009,New,,,,,,,"0|i04uzb:",26284,,,,,,,,,"25/Jul/09 03:38;michaelbusch;- Removes new TokenStream API warnings
- fixes a javadoc reference in analysis/package.html
- adds comments to TeeSinkTokenFilter.SinkFilter
- adds comments to deprecated Token constructors

I'm planning to commit this shortly.","25/Jul/09 03:42;michaelbusch;Committed revision 797714.","06/Aug/09 17:33;markrmiller@gmail.com;tokenstream still says token is deprecated","06/Aug/09 19:22;mikemccand;Reopening so we don't forget Mark's last comment...","10/Aug/09 23:51;markrmiller@gmail.com;I took care of it.","11/Aug/09 00:56;michaelbusch;Thanks, Mark.","20/Aug/09 21:38;markrmiller@gmail.com;Would love to have some sample code, but at a min we have to update javadocs to have full contract (end, close)

and see comment:

{quote}
And close()....
is there a way these could be consolidated?

-Yonik
{quote}","20/Aug/09 22:19;markrmiller@gmail.com;I also think it couldn't hurt to add some javadoc on AttributeSource hashCode and equals - no subclasses override them that I see (tokenfilter, tokenstream, ...) Thats a somewhat confusing situation that I think merits a comment.","24/Aug/09 13:43;markrmiller@gmail.com;Okay - there is some example code ! Its just analysis package.html - so I guess we can point to that from the TokenStream javadoc.

package.html for analysis is kind of ugly - and at times confusing. After giving an example with the new tokenstream API, there is a section saying, there is also a new API and here is stuff about it - but it was just showing an example with the new API ...","24/Aug/09 22:46;michaelbusch;Thanks for taking this, Mark. Let me know if you need help.","25/Aug/09 13:49;markrmiller@gmail.com;Okay, here is a rough draft. I'd still like to look over and polish a bit - but I think this covers the broad strokes.

One thing I would like to clear up, but I'm not exactly sure what its saying:

 * {@link TokenStream} now extends {@link AttributeSource}, which provides
 * access to all of the token {@link Attribute}s for the {@link TokenStream}.
 * Note that only one instance per {@link AttributeImpl} is created and reused
 * for every token. This approach reduces object creation and allows local
 * caching of references to the {@link AttributeImpl}s. See
 * {@link #incrementToken()} for further details.","25/Aug/09 13:58;markrmiller@gmail.com;   * @deprecated This setting will be <code>true</code> per default in Lucene 3.0,
   * when {@link #incrementToken} is abstract and must be always implemented.
   */
  public static void setOnlyUseNewAPI(boolean onlyUseNewAPI) {
    TokenStream.onlyUseNewAPI = onlyUseNewAPI;
  }


Won't that setting be removed ? Should I add the remove when api is removed reminder here? The Javadoc appears a bit misleading.

*edit

I guess my issue is, it appears ambiguous as to what method must be implemented. @depreacted gives you the hint, but I'm just going to reword to make clear.","25/Aug/09 14:04;uschindler;Yes. This setting will be removed, with about half of the whole class :( - what a pity.
It will default to true in 3.0 and one must override the abstract incrementToken() method. next() and next(Token) will be removed, too.","25/Aug/09 14:18;markrmiller@gmail.com;bq. what a pity.

Yes, I put your reflection detection/cache code into Similarity last night - that code is great - really clean, really nice design. But we will zap it soon :)","25/Aug/09 15:20;markrmiller@gmail.com;Resolving - I may tweak more, but its got what it needs overall.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add javadoc notes about ICUCollationKeyFilter's advantages over CollationKeyFilter,LUCENE-1719,12429040,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,simonw,sarowe,sarowe,28/Jun/09 04:48,25/Sep/09 16:23,30/Sep/19 08:38,01/Jul/09 16:52,2.4.1,,,,,,,,,,2.9,,,,modules/other,,,0,,,,"contrib/collation's ICUCollationKeyFilter, which uses ICU4J collation, is faster than CollationKeyFilter, the JVM-provided java.text.Collator implementation in the same package.  The javadocs of these classes should be modified to add a note to this effect.

My curiosity was piqued by [Robert Muir's comment|https://issues.apache.org/jira/browse/LUCENE-1581?focusedCommentId=12720300#action_12720300] on LUCENE-1581, in which he states that ICUCollationKeyFilter is up to 30x faster than CollationKeyFilter.

I timed the operation of these two classes, with Sun JVM versions 1.4.2/32-bit, 1.5.0/32- and 64-bit, and 1.6.0/64-bit, using 90k word lists of 4 languages (taken from the corresponding Debian wordlist packages and truncated to the first 90k words after a fixed random shuffling), using Collators at the default strength, on a Windows Vista 64-bit machine.  I used an analysis pipeline consisting of WhitespaceTokenizer chained to the collation key filter, so to isolate the time taken by the collation key filters, I also timed WhitespaceTokenizer operating alone for each combination.  The rightmost column represents the performance advantage of the ICU4J implemtation (ICU) over the java.text.Collator implementation (JVM), after discounting the WhitespaceTokenizer time (WST): (JVM-ICU) / (ICU-WST). The best times out of 5 runs for each combination, in milliseconds, are as follows:

||Sun JVM||Language||java.text||ICU4J||WhitespaceTokenizer||ICU4J Improvement||
|1.4.2_17 (32 bit)|English|522|212|13|156%|
|1.4.2_17 (32 bit)|French|716|243|14|207%|
|1.4.2_17 (32 bit)|German|669|264|16|163%|
|1.4.2_17 (32 bit)|Ukranian|931|474|25|102%|
|1.5.0_15 (32 bit)|English|604|176|16|268%|
|1.5.0_15 (32 bit)|French|817|209|17|317%|
|1.5.0_15 (32 bit)|German|799|225|20|280%|
|1.5.0_15 (32 bit)|Ukranian|1029|436|26|145%|
|1.5.0_15 (64 bit)|English|431|89|10|433%|
|1.5.0_15 (64 bit)|French|562|112|11|446%|
|1.5.0_15 (64 bit)|German|567|116|13|438%|
|1.5.0_15 (64 bit)|Ukranian|734|281|21|174%|
|1.6.0_13 (64 bit)|English|162|81|9|113%|
|1.6.0_13 (64 bit)|French|192|92|10|122%|
|1.6.0_13 (64 bit)|German|204|99|14|124%|
|1.6.0_13 (64 bit)|Ukranian|273|202|21|39%|
",,,,,,,,,,,,,,,,"29/Jun/09 03:57;sarowe;LUCENE-1719.patch;https://issues.apache.org/jira/secure/attachment/12412036/LUCENE-1719.patch","28/Jun/09 04:51;sarowe;LUCENE-1719.patch;https://issues.apache.org/jira/secure/attachment/12412009/LUCENE-1719.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-06-28 11:08:12.589,,,false,,,,,,,,,,,,,,,12040,,,Wed Jul 01 16:52:11 UTC 2009,New,Patch Available,,,,,,"0|i04v8f:",26325,,,,,,,,,"28/Jun/09 04:51;sarowe;Patch containing notes to add to collation key filter/analyzer classes' javadocs.","28/Jun/09 05:05;sarowe;I also tested ICU4J version 4.2 (released 6 weeks ago), and the timings were nearly identical to those from ICU4J version 4.0 (the one that's in contrib/collation/lib/).

The timings given in the table above were not produced with the ""-server"" option to the JVM.  I separately tested all combinations using the ""-server"" option, but there was no difference for the 32-bit JVMs, though roughly 3-4% faster for the 64-bit JVMs.  I got the impression (didn't actually calculate) that although the best times of 5 runs were better for the 64-bit JVMs when using the ""-server"" option, the average times seemed to be slightly worse.  In any case, the performance improvement of the ICU4J implementation over the java.text.Collator implementation was basically unaffected by the use of the ""-server"" JVM option.
","28/Jun/09 11:08;rcmuir;steven, you are correct. 

i should have clarified, the gain is not as much when generating keys. but there is still huge gains for runtime comparison. see recent numbers here for a few languages:

http://site.icu-project.org/charts/collation-icu4j-sun

but you should also mention that key size is smaller too!  (smaller term dictionary)","28/Jun/09 17:35;sarowe;Cool! Thanks for the link, Robert.

Key comparison under Lucene when using *CollationKeyAnalyzer will utilize neither ICU4J's nor the java.text incremental collation facilities - the base-8000h-String-encoded raw collation keys will be directly compared (and sorted) as Strings.  So key generation time and, as you point out, key length are the appropriate measures here.

I'll post a patch shortly that includes your ICU4J link, and mentions the key length aspect as well.  I'll also remove specific numbers from the javadoc notes - people can follow the ICU4J link if they're interested.","28/Jun/09 18:13;sarowe;Edited title to reflect addition of key length concerns, and switched performance improvement column to be percentage improvements rather than multipliers.","28/Jun/09 19:28;rcmuir;steven, no thank you for running the calculations!

yeah i think the sort key length is worth mentioning. in practice i wonder how much it helps lucene at runtime, maybe for things like SORT at least it would improve runtime performance by some small amount.","28/Jun/09 20:53;rcmuir;steven, another note i thought i would mention.

along these same lines i searched lucene source code for java.text.Collator and found some uses of it (the incremental facility). I wonder if in the future we could find a way to allow usage of com.ibm.icu.text.Collator in these spots.

this could give some healthy performance improvements. I found it in:

QueryParser (for localized RangeQuery)
RangeQuery/RangeFilter/RangeTermEnum/ConstantScoreRangeQuery
FieldComparator/FieldSortedHitQueue/FieldDocSortedHitQueue

","29/Jun/09 03:55;sarowe;bq. [...] i searched lucene source code for java.text.Collator and found some uses of it (the incremental facility). I wonder if in the future we could find a way to allow usage of com.ibm.icu.text.Collator in these spots.

+1

I guess the way to go would be to make the implementation pluggable.","29/Jun/09 03:57;sarowe;Updated patch including information about ICU4J's shorter key length; adding a link to the ICU4J documentation's comparison of ICU4J and java.text.Collator key generation time and key length; and removing specific performance numbers.","30/Jun/09 14:37;simonw;Steven, patch looks good to me. I will look at it again in a day or two.

simon","01/Jul/09 16:52;simonw;I committed your patch and removed the last ""NB:"" in the ICUCollationKeyFilter.java for consistency.

Thanks Steven!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogMergePolicy.findMergesToExpungeDeletes need to get deletes from the SegmentReader,LUCENE-1700,12428198,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,jasonrutherglen,jasonrutherglen,18/Jun/09 01:22,25/Sep/09 16:23,30/Sep/19 08:38,19/Jun/09 15:37,2.4.1,,,,,,,,,,2.9,,,,core/index,,,0,,,,"With LUCENE-1516, deletes are carried over in the SegmentReaders
which means implementations of
MergePolicy.findMergesToExpungeDeletes (such as LogMergePolicy)
need to obtain deletion info from the SR (instead of from the
SegmentInfo which won't have the information).",,172800,172800,,0%,172800,172800,,,,,,,,,"18/Jun/09 18:36;mikemccand;LUCENE-1700.patch;https://issues.apache.org/jira/secure/attachment/12411120/LUCENE-1700.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-06-18 18:36:27.594,,,false,,,,,,,,,,,,,,,12058,,,Fri Jun 19 15:37:13 UTC 2009,New,,,,,,,"0|i04vcn:",26344,,,,,,,,,"18/Jun/09 01:27;jasonrutherglen;Taking a step back, maybe we can solve the package protected
SegmentInfo issue here by creating a new class with the
necessary attributes?

Here's what LUCENE-1313 does:

{code} SegmentReader sr = writer.readerPool.getIfExists(info);
if (info.hasDeletions() || (sr != null && sr.hasDeletions())) {
{code}

Because SegmentInfo is package protected it seems ok to access a
package protected method (or in this case variable) in
IndexWriter.","18/Jun/09 18:36;mikemccand;Attached patch.

I added a test case showing it, then took that same approach (from LUCENE-1313) and the test passes.

I also found that with NRT, because the deletions are applied before
building the CFS after flushing, we wind up holding open both the
non-CFS and CFS files on creating the reader.  So, I changed deletions
to flush after the CFS is built.

I plan to commit in a day or two.
","19/Jun/09 15:37;mikemccand;Thanks Jason!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
use reusable collation keys in ICUCollationFilter,LUCENE-1643,12425795,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,rcmuir,rcmuir,19/May/09 06:32,25/Sep/09 16:23,30/Sep/19 08:38,19/May/09 09:50,,,,,,,,,,,2.9,,,,modules/other,,,0,,,,"ICUCollationFilter need not create a new CollationKey object for each token.
In ICU there is a mechanism to use a reusable key.
",,,,,,,,,,,,,,,,"19/May/09 06:34;rcmuir;LUCENE-1643.patch;https://issues.apache.org/jira/secure/attachment/12408445/LUCENE-1643.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-05-19 09:46:30.138,,,false,,,,,,,,,,,,,,,12115,,,Tue May 19 09:46:30 UTC 2009,New,Patch Available,,,,,,"0|i04vpj:",26402,,,,,,,,,"19/May/09 06:34;rcmuir;patch","19/May/09 09:46;mikemccand;Looks good, I'll commit shortly.  Thanks Robert!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Copy/Paste-Typo in toString() for SpanQueryFilter,LUCENE-1633,12425203,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,brainlounge,brainlounge,12/May/09 12:16,25/Sep/09 16:23,30/Sep/19 08:38,12/May/09 12:22,,,,,,,,,,,2.9,,,,,,,0,,,,"   public String toString() {
-    return ""QueryWrapperFilter("" + query + "")"";
+    return ""SpanQueryFilter("" + query + "")"";
   }

says it all.",,,,,,,,,,,,,,,,"12/May/09 12:17;brainlounge;fix_SpanQueryFilter_toString.patch;https://issues.apache.org/jira/secure/attachment/12407866/fix_SpanQueryFilter_toString.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-05-12 12:22:21.039,,,false,,,,,,,,,,,,,,,12125,,,Tue May 12 12:22:21 UTC 2009,New,Patch Available,,,,,,"0|i04vrz:",26413,,,,,,,,,"12/May/09 12:22;yseeley@gmail.com;Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TermAttribute.termLength() optimization,LUCENE-1619,12423934,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,eksdev,eksdev,27/Apr/09 20:22,25/Sep/09 16:23,30/Sep/19 08:38,28/Apr/09 08:46,,,,,,,,,,,2.9,,,,modules/analysis,,,0,,,,"
   public int termLength() {
     initTermBuffer(); // This patch removes this method call 
     return termLength;
   }

I see no reason to initTermBuffer() in termLength()... all tests pass, but I could be wrong?

",,,,,,,,,,,,,,,,"27/Apr/09 20:23;eksdev;LUCENE-1619.patch;https://issues.apache.org/jira/secure/attachment/12406567/LUCENE-1619.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-04-28 08:35:11.862,,,false,,,,,,,,,,,,,,,12138,,,Tue Apr 28 09:02:44 UTC 2009,New,Patch Available,,,,,,"0|i04vv3:",26427,,,,,,,,,"28/Apr/09 08:35;mikemccand;Indeed it seems unnecessary -- I'll commit.  Thanks Eks!","28/Apr/09 09:02;eksdev;thanks Mike",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add one setter for start and end offset to OffsetAttribute,LUCENE-1616,12423816,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,eksdev,eksdev,26/Apr/09 21:56,25/Sep/09 16:23,30/Sep/19 08:38,28/Apr/09 21:18,,,,,,,,,,,2.9,,,,modules/analysis,,,0,,,,"add OffsetAttribute. setOffset(startOffset, endOffset);

trivial change, no JUnit needed

Changed CharTokenizer to use it",,,,,,,,,,,,,,,,"27/Apr/09 19:59;eksdev;LUCENE-1616.patch;https://issues.apache.org/jira/secure/attachment/12406564/LUCENE-1616.patch","27/Apr/09 17:36;eksdev;LUCENE-1616.patch;https://issues.apache.org/jira/secure/attachment/12406545/LUCENE-1616.patch","27/Apr/09 17:24;eksdev;LUCENE-1616.patch;https://issues.apache.org/jira/secure/attachment/12406542/LUCENE-1616.patch","26/Apr/09 21:59;eksdev;LUCENE-1616.patch;https://issues.apache.org/jira/secure/attachment/12406481/LUCENE-1616.patch",,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2009-04-27 09:16:16.975,,,false,,,,,,,,,,,,,,,12141,,,Tue Apr 28 21:18:15 UTC 2009,New,Patch Available,,,,,,"0|i04vvr:",26430,,,,,,,,,"27/Apr/09 09:16;mikemccand;Should we deprecate the separate setters with this addition?","27/Apr/09 09:26;uschindler;Not really, the attributes API was added for 2.9, so it did not appear until now in official releases, it could be just removed.","27/Apr/09 09:35;mikemccand;Oh yeah :)  Good!  I'm losing track of what's not yet released...

Eks, can you update the patch with that?  Thanks.","27/Apr/09 10:34;earwin;Separate setters might have their own use? I believe I had a pair of filters that set begin and end offset in different parts of the code.","27/Apr/09 11:05;mikemccand;But surely that's a very rare case (the exception, not the rule).  Ie nearly always, one sets start & end offset together?","27/Apr/09 11:16;earwin;I have two cases.
In one case I can't access the start offset by the time I set end offset, and therefore have to introduce a field on the filter for keeping track of it (or use the next case's solution twice), if separate setters are removed.
In other case I only need to adjust end offset, so I'll have to do attr.setOffset(attr.getStartOffset(), newEndOffset).
Nothing deadly, but I don't see the point of removing methods that might be useful and don't interfere with anything.","27/Apr/09 11:49;eksdev;I am ok with both options, removing separate looks a bit better for me as it forces users to think ""attomic"" about offset <=> {start, end}. 

If you separate start and end offset too far in your code, probability that you do not see mistake somewhere is higher compared to the case where you manage start and end on your own in these cases (this is then rather ""explicit"" in you code)... 

But that is all really something we should not think too much about it :) We make no mistakes eather way
 
I can provide new patch, if needed. ","27/Apr/09 14:33;mikemccand;bq. removing separate looks a bit better for me as it forces users to think ""attomic"" about offset <=> {start, end}.

This is my thinking as well.

And in general I prefer one clear way to do something (the Python way) instead providing various different ways to do the same thing (the Perl way).","27/Apr/09 15:05;earwin;bq. removing separate looks a bit better for me as it forces users to think ""attomic"" about offset <=> {start, end}.
And if it's not atomic by design?

bq. If you separate start and end offset too far in your code, probability that you do not see mistake somewhere is higher compared to the case where you manage start and end on your own in these cases (this is then rather ""explicit"" in you code)...
Instead of having one field for Term, which you build incrementally, you now have to keep another field for startOffset. Imho, that's starting to cross into another meaning of 'explicit' :)
And while you're trying to prevent bugs of using setStartOffset and forgetting about its 'End' counterpart, you introduce another set of bugs - overwriting one end of interval, when you only need to update another.

bq. And in general I prefer one clear way to do something
And force everyone who has slightly different use-case to jump through the hoops. Span*Query api is a perfect example.

Well, whatever.","27/Apr/09 15:53;mikemccand;bq. And force everyone who has slightly different use-case to jump through the hoops.

""Simple things should be simple and complex things should be possible"" is a strong guide when I'm thinking about APIs, configuration, etc.

My feeling here is for the vast majority of the cases, people set start & end offset together, so we should shift to the API that makes that easy.  This is the ""simple"" case.

For the remaining minority (your interesting use case), you can still do what you need but yes there are some hoops to go through.  This is the ""complex"" case.

bq. Span*Query api is a perfect example.

Can you describe the limitations here in more detail?","27/Apr/09 17:24;eksdev;the same as the first patch, just with removed setStart/EndOffset(int) ","27/Apr/09 17:33;mikemccand;Thanks Eks.  You also need to fix all the places that call the old methods (things don't compile w/ the new patch).","27/Apr/09 17:36;eksdev;whoops, this time it compiles :)","27/Apr/09 18:02;mikemccand;I still get compilation errors:
{code}
    [mkdir] Created dir: /lucene/src/lucene.offsets/build/classes/java
    [javac] Compiling 372 source files to /lucene/src/lucene.offsets/build/classes/java
    [javac] /lucene/src/lucene.offsets/src/java/org/apache/lucene/analysis/KeywordTokenizer.java:62: cannot find symbol
    [javac] symbol  : method setStartOffset(int)
    [javac] location: class org.apache.lucene.analysis.tokenattributes.OffsetAttribute
    [javac]       offsetAtt.setStartOffset(0);
    [javac]                ^
    [javac] /lucene/src/lucene.offsets/src/java/org/apache/lucene/analysis/KeywordTokenizer.java:63: cannot find symbol
    [javac] symbol  : method setEndOffset(int)
    [javac] location: class org.apache.lucene.analysis.tokenattributes.OffsetAttribute
    [javac]       offsetAtt.setEndOffset(upto);
    [javac]                ^
    [javac] /lucene/src/lucene.offsets/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java:164: cannot find symbol
    [javac] symbol  : method setStartOffset(int)
    [javac] location: class org.apache.lucene.analysis.tokenattributes.OffsetAttribute
    [javac]         offsetAtt.setStartOffset(start);
    [javac]                  ^
    [javac] /lucene/src/lucene.offsets/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java:165: cannot find symbol
    [javac] symbol  : method setEndOffset(int)
    [javac] location: class org.apache.lucene.analysis.tokenattributes.OffsetAttribute
    [javac]         offsetAtt.setEndOffset(start+termAtt.termLength());
    [javac]                  ^
    [javac] /lucene/src/lucene.offsets/src/java/org/apache/lucene/index/DocInverterPerThread.java:56: cannot find symbol
    [javac] symbol  : method setStartOffset(int)
    [javac] location: class org.apache.lucene.analysis.tokenattributes.OffsetAttribute
    [javac]       offsetAttribute.setStartOffset(startOffset);
    [javac]                      ^
    [javac] /lucene/src/lucene.offsets/src/java/org/apache/lucene/index/DocInverterPerThread.java:57: cannot find symbol
    [javac] symbol  : method setEndOffset(int)
    [javac] location: class org.apache.lucene.analysis.tokenattributes.OffsetAttribute
    [javac]       offsetAttribute.setEndOffset(endOffset);
    [javac]                      ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 6 errors
{code}","27/Apr/09 18:08;eksdev;me too, sorry! 
Eclipse left me blind for some funny reason
waiting for test to complete before I commit again ... ","27/Apr/09 18:48;earwin;bq. > Span*Query api is a perfect example.
bq. Can you describe the limitations here in more detail?
Take a look at SpanNearQuery and SpanOrQuery.

1. They don't provide incremental construction (i.e. add() method, like in BooleanQuery), and they can be built only from an array of subqueries. So, if you don't know exact amount of subqueries upfront, you're busted. You have to use ArrayList, which you convert to array to feed into SpanQuery, which is converted back to ArrayList inside!!
2. They can't be edited. If you have a need to iterate over your query tree and modify it in one way or another, you need to create brand new instances of Span*Query. And here you hit #1 again, hard.
3. They can't be even inspected without creating a new array from the backing list (see getClauses).

I use patched versions of SpanNear/OrQueries, which still use backing ArrayList, but accept it in constructor, have utility 'add' method and getClauses() returns this very list, which allows for zero-cost inspection and easy modification if the need arises.","27/Apr/09 19:59;eksdev;ok, maybe this time it will work, I hope I managed to clean it up (core build and test pass). 

The only thing that fails is contrib, but I guess this has nothing to do with it? 


[javac] D:\Repository\SerachAndMatch\Lucene\lucene\java\trunk\contrib\highlighter\src\java\org\apache\lucene\search\highlight\WeightedSpanTermExtractor.java:306: cannot find symbol
    [javac]       MemoryIndex indexer = new MemoryIndex();
    [javac]       ^
    [javac]   symbol:   class MemoryIndex
    [javac]   location: class org.apache.lucene.search.highlight.WeightedSpanTermExtractor
    [javac] D:\Repository\SerachAndMatch\Lucene\lucene\java\trunk\contrib\highlighter\src\java\org\apache\lucene\search\highlight\WeightedSpanTermExtractor.java:306: cannot find symbol
    [javac]       MemoryIndex indexer = new MemoryIndex();
    [javac]                                 ^
    [javac]   symbol:   class MemoryIndex
    [javac]   location: class org.apache.lucene.search.highlight.WeightedSpanTermExtractor
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 3 errors","27/Apr/09 20:10;markrmiller@gmail.com;bq. The only thing that fails is contrib, but I guess this has nothing to do with it?

looks like an issue with highlighters dependency on memory index. what target produces the problem? We have seen something like it in the past.","27/Apr/09 20:13;eksdev;ant build-contrib ","27/Apr/09 20:15;mikemccand;bq. I use patched versions of SpanNear/OrQueries, which still use backing ArrayList, but accept it in constructor, have utility 'add' method and getClauses() returns this very list, which allows for zero-cost inspection and easy modification if the need arises.

That sounds useful -- is it something you can share?","27/Apr/09 21:07;mikemccand;OK all tests pass.  I had to fix a few back-compat tests (that were using the new TokenStream API, I think because we created the back-compat branch from trunk after the new TokenStream API landed).

I'll commit in a day or two.  Thanks Eks!","28/Apr/09 21:18;mikemccand;Thanks Eks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Preserve whitespace in <code> sections in the Changes.html generated from CHANGES.txt by changes2html.pl,LUCENE-1610,12423742,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,sarowe,sarowe,24/Apr/09 21:16,25/Sep/09 16:23,30/Sep/19 08:38,24/Apr/09 22:21,2.9,,,,,,,,,,2.9,,,,general/website,,,0,,,,"The Trunk section of CHANGES.txt sports use of a new feature: <code> sections, for the two mentions of LUCENE-1575.

This looks fine in the text rendering, but looks crappy in the HTML version, since changes2html.pl escapes HTML metacharacters to appear as-is in the HTML rendering, but the newlines in the code are converted to a single space. 

I think this should be fixed by modifying changes2html.pl to convert <code> and </code> into (unescaped) <code><pre> and </pre></code>, respectively, since just passing through <code> and </code>, without </?pre>, while changing the font to monospaced (nice), still collapses whitespace (not nice). 

See the java-dev thread that spawned this issue here: http://www.nabble.com/CHANGES.txt-td23102627.html",,,,,,,,,,,,,,,,"24/Apr/09 21:21;sarowe;LUCENE-1610.patch;https://issues.apache.org/jira/secure/attachment/12406400/LUCENE-1610.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-04-24 22:21:56.941,,,false,,,,,,,,,,,,,,,12145,,,Fri Apr 24 22:21:56 UTC 2009,New,,,,,,,"0|i04vx3:",26436,,,,,,,,,"24/Apr/09 21:21;sarowe;Implements the suggested fix: <code> is converted to <code><pre> (instead of to &amp;lt;code&amp;gt; ) and </code> is converted to </pre></code> (instead of to &amp;lt;/code&amp;gt; )","24/Apr/09 22:21;mikemccand;Thanks Steve!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add subset method to BitVector,LUCENE-1605,12422841,,New Feature,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,jvolkman,jvolkman,15/Apr/09 03:48,25/Sep/09 16:23,30/Sep/19 08:38,16/Apr/09 14:29,2.9,,,,,,,,,,2.9,,,,core/other,,,0,,,,"Recently I needed the ability to efficiently compute subsets of a BitVector. The method is:
  public BitVector subset(int start, int end)
where ""start"" is the starting index, inclusive and ""end"" is the ending index, exclusive.

Attached is a patch including the subset method as well as relevant unit tests.",,,,,,,,,,,,,,,,"15/Apr/09 03:49;jvolkman;LUCENE-1605.txt;https://issues.apache.org/jira/secure/attachment/12405490/LUCENE-1605.txt",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-04-16 14:28:49.784,,,false,,,,,,,,,,,,,,,12150,,,Thu Apr 16 14:28:49 UTC 2009,New,Patch Available,,,,,,"0|i04vy7:",26441,,,,,,,,,"16/Apr/09 14:28;mikemccand;Patch looks good; I'll commit shortly.  Thanks Jeremy!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Spatial Lucene sort to use FieldComparatorSource,LUCENE-1588,12422131,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,pjaol,pjaol,pjaol,06/Apr/09 15:22,25/Sep/09 16:23,30/Sep/19 08:38,15/Apr/09 18:51,2.9,,,,,,,,,,2.9,,,,modules/spatial,,06/Apr/09 00:00,0,,,,Update distance sorting to use FieldComparator sorting as opposed to SortComparator,,,,,,,,,,,,,,,,"06/Apr/09 15:27;pjaol;LUCENE-1588.patch;https://issues.apache.org/jira/secure/attachment/12404738/LUCENE-1588.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-04-10 02:41:37.798,,,false,,,,,,,,,,,,,,,12167,,,Wed Apr 15 18:51:07 UTC 2009,New,Patch Available,,,,,,"0|i04w27:",26459,,,,,,,,,"06/Apr/09 15:27;pjaol;Deprecate DistanceSortSource and Add DistanceFieldComparator
updated Test case to use DistanceFieldComparator

Usage
{code}
// Create a distance sort
// As the radius filter has performed the distance calculations
// already, pass in the filter to reuse the results.
// 
DistanceFieldComparatorSource dsort = new 
            DistanceFieldComparatorSource(dq.distanceFilter);
Sort sort = new Sort(new SortField(""foo"", dsort,false));
    
// Perform the search, using the term query, the serial chain filter, and the
// distance sort
Hits hits = searcher.search(customScore, dq.getFilter(),sort);
{code}

If nobody objects I'll apply this later today","10/Apr/09 02:41;markrmiller@gmail.com;hmm - looks like we didnt make FieldComparatorSource serializable like SortComparatorSource -- so no need for the serialVersionUID.

Couple other picky comments:

return void at end of the constructor could go.

might remove the nextReader comment or change it ie this space intentionally left blank.","15/Apr/09 18:51;gsingers;This was committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add IndexReader.flush(commitUserData),LUCENE-1546,12415248,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,jasonrutherglen,jasonrutherglen,20/Feb/09 17:36,25/Sep/09 16:23,30/Sep/19 08:38,27/Feb/09 12:20,2.4,,,,,,,,,,2.9,,,,core/index,,,0,,,,"IndexWriter offers a commit(String commitUserData) method.
IndexReader can commit as well using the flush/close methods and so
needs an analogous method that accepts commitUserData.",,86400,86400,,0%,86400,86400,,,,,,,,,"07/Apr/09 21:46;uschindler;LUCENE-1546-deprecation.patch;https://issues.apache.org/jira/secure/attachment/12404885/LUCENE-1546-deprecation.patch","24/Feb/09 13:00;mikemccand;LUCENE-1546.patch;https://issues.apache.org/jira/secure/attachment/12400854/LUCENE-1546.patch","20/Feb/09 18:50;jasonrutherglen;LUCENE-1546.patch;https://issues.apache.org/jira/secure/attachment/12400620/LUCENE-1546.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-02-24 13:00:09.426,,,false,,,,,,,,,,,,,,,12208,,,Tue Apr 07 22:52:55 UTC 2009,New,,,,,,,"0|i04wbj:",26501,,,,,,,,,"20/Feb/09 18:50;jasonrutherglen;All tests pass.

- Added IndexReader.flush(userCommitData).  I'm hesitant about adding
IR.close(userCommitData) as IndexWriter.close doesn't have a similar
method.","24/Feb/09 13:00;mikemccand;I tweaked the patch to keep back compat (we can't just change IndexReader's protected doCommit() method).  I think it's ready to commit... I'll wait a day or two.","27/Feb/09 12:20;mikemccand;Committed revision 748493.  Thanks Jason!","07/Apr/09 21:46;uschindler;This patch fixes deprecation errors:
I wrote a class extends FilterIndexReader. This class produced on compilation an deprecation warning on doCommit() without any hint to this method in my code (I did not implement doCommit nor used the method).
It seems, that javac from 1.5 needs the deprecation also in the subclasses. With this patch applied, the own subclass did not produce the warning anymore.","07/Apr/09 22:52;mikemccand;OK I just committed that, thanks Uwe.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Benchmark deletes.alg fails,LUCENE-1527,12413107,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,jasonrutherglen,jasonrutherglen,23/Jan/09 16:41,25/Sep/09 16:23,30/Sep/19 08:38,23/Jan/09 20:34,2.4,,,,,,,,,,2.9,,,,modules/benchmark,,,0,,,,Benchmark deletes.alg fails because the index reader defaults to open readonly.  ,,3600,3600,,0%,3600,3600,,,,,,,,,"23/Jan/09 16:44;jasonrutherglen;LUCENE-1527.patch;https://issues.apache.org/jira/secure/attachment/12398587/LUCENE-1527.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-01-23 20:34:03.007,,,false,,,,,,,,,,,,,,,12226,,,Fri Jan 23 20:34:03 UTC 2009,New,,,,,,,"0|i04wfr:",26520,,,,,,,,,"23/Jan/09 16:44;jasonrutherglen;Adds false parameter to OpenReader in deletes.alg","23/Jan/09 20:34;mikemccand;Committed revision 737175.  Thanks Jason!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CartesianTierPlotter fieldPrefix should be configurable,LUCENE-1508,12411559,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,ryantxu,ryantxu,ryantxu,02/Jan/09 02:50,25/Sep/09 16:23,30/Sep/19 08:38,02/Jan/09 02:52,,,,,,,,,,,2.9,,,,modules/spatial,,,0,,,,"CartesianTierPlotter field prefix is currrently hardcoded to ""_localTier"" -- this should be configurable",,,,,,,,,,,,,,,,"02/Jan/09 02:51;ryantxu;LUCENE-1508-tier-plotter-prefix.patch;https://issues.apache.org/jira/secure/attachment/12397005/LUCENE-1508-tier-plotter-prefix.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12243,,,Fri Jan 02 02:51:39 UTC 2009,New,,,,,,,"0|i04wjj:",26537,,,,,,,,,"02/Jan/09 02:51;ryantxu;simple patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move solr NumberUtils to lucene,LUCENE-1496,12411053,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,ryantxu,ryantxu,19/Dec/08 14:32,25/Sep/09 16:23,30/Sep/19 08:38,11/Jun/09 14:53,,,,,,,,,,,2.9,,,,,,,0,,,,"solr includes a NumberUtils class with some general utilities for dealing with tokens and numbers.

This should be in lucene rather then solr.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-12-27 10:06:27.853,,,false,,,,,,,,,,,,,,,12255,,,Thu Jun 11 14:53:56 UTC 2009,New,,,,,,,"0|i04wm7:",26549,,,,,,,,,"27/Dec/08 10:06;uschindler;I looked into the code of NumberUtils:

The encoding is very similar to the one of TrieUtils (used in TrieRangeQuery, see LUCENE-1470, http://hudson.zones.apache.org/hudson/job/Lucene-trunk/javadoc//org/apache/lucene/search/trie/TrieUtils.html). The only difference between TrieUtils and NumberUtils is the more compact encoding in NumberUtils (because in TrieUtils.VARIANT_8BIT uses one character per byte, NumberUtils uses 14 bits per character). TrieUtils works also correct with String.compareTo() (it was the intention behind TrieUtils).

In my opinion, TrieUtils has some more advantages:
- Doubles are encoded in a correctly sortable way (even Double.XXX_INFINITY!), using the IEEE binary representation of doubles with some bit alignments.
- Direct support for Dates and longs
- Builtin comparator for the new SortField constructor (LUCENE-1478)  and a nice SortField factory. This maps all encoded values to a FieldCache with long values (even for dates or doubles because there is no difference, longs have the fastest encoding/decoding speed - for sorting, the real values are not interesting).

The only problem is, that indexes, encoded with the old NumberUtils are not readable by TrieUtils. But if we include such things into Lucene, we should not duplicate code and create again new encodings.

For the more compact encoding, TrieUtils could be extended, to also support a ""14bit"" Trie variant (which would not work for real trie encoding), but may be used for simply store longs very compact. On the other hand, if somebody uses NumberUtils, he may be also interested in TrieRangeQuery, so he should use TrieUtils.VARIANT_8BIT.

So I think, we should perhaps leave NumberUtils at solr and use TrieUtils in Lucene. LocalLucene should then also use TrieUtils. And solr may in future switch to Trie encoding with the next major version, too.","27/Dec/08 17:02;ryantxu;should the number functions from TrieUtils be moved to a lucene NumberUtils?

API wise, if i were looking for ways to encode numbers, i doubt i would look at ""TrieUtils.java""

what about the non-long/double functions in NumberUtils?","28/Dec/08 16:47;uschindler;I am thinking about extending TrieUtils and TrieRangeQuery for 32bit values (ints and floats). Doing this, the other methods in NumberUtils would be obsolete, too. This was just a suggestion, maybe we should talk a little bit more about this.

On my first look through the code, I had not seen, that NumberUtils also supports doubles like TrieUtils, the only difference is the use of doubleToLongBits() vs. doubleToRawLongBits(). I am not sure what is better :-(, does anybody know more about this? Im my opinion the version with/without raw also normalizes doubles, so NaN compares with ==, anything other?

To my changes in TrieUtils for support of 32bit: I am currently not sure how to do this elegant. Esp. the auto detection of trie encoding is not so happy on changing this :-( As 2.9 is not yet released, I have time to change the classes and signatures without worry about deprecation and format changes. So a good point to unify TrieUtils and NumberUtils. Maybe TrieRangeQuery will make it into the core, when flexible indexing is available.

So my questions: Is anybody interested in TrieUtils also support 32bit? Why not unify NumberUtils and TrieUtils? Any ideas?","18/Mar/09 20:20;mikemccand;If we move trie/* into core, what do we need/want to fold in from Solr's NumberUtils?","11/Jun/09 14:25;mikemccand;Uwe, with trie now handling 32 bit values, and coming into core, does Lucene need Solr's NumberUtils?  Are there compelling things that Solr's NumberUtils do over TrieUtil?","11/Jun/09 14:37;uschindler;I think we can close this now. I originally wanted to make this issue dependent on the move-to-core issue and close it together with trie.

There is now a private copy of number utils inside LocalLucene, but this should be removed soonly and replaced by TrieUtils.

But yonik said: NumberUtils is also only for compatibility reasons in solr. The binary format used by NumberUtils is not very index-friendly (because of using the full UTF-16 range and so it has the UTF-8 decoding overhead), so it should not be used for new deleopments. I suggest to use TrieUtils (or NumberUtils to do the same). TrieUtils only uses 7 bits per char and so don't touch the UTF-8 encoding (it is simply ASCII-only).","11/Jun/09 14:53;mikemccand;OK, I'm resolving as ""won't fix"".  It sounds like Lucene only needs TrieUtils...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There are a few binary search implmentations in lucene that suffer from a now well known overflow bug,LUCENE-1457,12408618,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,markrmiller@gmail.com,markrmiller@gmail.com,17/Nov/08 02:17,25/Sep/09 16:23,30/Sep/19 08:38,19/Feb/09 09:59,,,,,,,,,,,2.4.1,2.9,,,,,,0,,,,"http://googleresearch.blogspot.com/2006/06/extra-extra-read-all-about-it-nearly.html

The places I see it are:

MultiSearcher.subSearcher(int)
TermInfosReader.getIndexOffset(Term)
MultiSegmentReader.readerIndex(int, int[], int)
MergeDocIDRemapper.remap(int)

I havn't taken much time to consider how likely any of these are to overflow. The values being averaged would have to be very large. That would rule out possible problems for at least a couple of these, but how about something like the MergeDocIDRemapper? Is there a document number that could be reached that has a chance of triggering this bug? If not we can close this and have a record of looking into it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-12-03 23:47:30.28,,,false,,,,,,,,,,,,,,,12294,,,Thu Feb 19 09:59:55 UTC 2009,New,,,,,,,"0|i04wuv:",26588,,,,,,,,,"03/Dec/08 23:47;mikemccand;Committed revision 723149.

I fixed these cases, plus one more in FieldCache.  Thanks Mark!","19/Feb/09 01:37;mikemccand;Reopening for backport to 2.4.1.","19/Feb/09 09:59;mikemccand;Committed revision 745798 on 2.4 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve payload error handling/reporting,LUCENE-1447,12408126,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,markrmiller@gmail.com,markrmiller@gmail.com,10/Nov/08 03:04,25/Sep/09 16:23,30/Sep/19 08:38,15/Nov/08 10:51,,,,,,,,,,,2.9,,,,,,,0,,,,"If you try to load a payload more than once you get the exception:  IOException(""Payload cannot be loaded more than once for the same term position."");

You also get this exception if their is no payload to load, and its a bit confusing, as the message doesn't relate to the actual problem.",,,,,,,,,,,,,,,,"14/Nov/08 03:11;markrmiller@gmail.com;LUCENE-1447.patch;https://issues.apache.org/jira/secure/attachment/12393914/LUCENE-1447.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-11-15 10:51:11.066,,,false,,,,,,,,,,,,,,,12304,,,Sat Nov 15 10:51:11 UTC 2008,New,,,,,,,"0|i04wx3:",26598,,,,,,,,,"14/Nov/08 03:11;markrmiller@gmail.com;Not sure whats best here, but I don't like the idea of adding another variable just to know which case the error is. So how about just changing the message to alert you that either case could be the problem?","15/Nov/08 10:51;mikemccand;Committed revision 714231.

Thanks Mark!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run 'test-tag' in nightly build,LUCENE-1446,12408124,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,10/Nov/08 02:09,25/Sep/09 16:23,30/Sep/19 08:38,11/Nov/08 02:32,,,,,,,,,,,2.9,,,,,,,0,,,,"Changes in this trivial patch:
- ant target 'nightly' now also depends on 'test-tag'
- adds property 'compatibility.tag' to common-build.xml that should always point to the last tagged release; its unit tests will be downloaded unless -Dtag="""" is used to override
- 'download-tag' does not fail if the svn checkout wasn't successful; instead 'test-tag' checks if the specified tag is checked-out and available, if not it fails ",,,,,,,,,,,,,,,,"11/Nov/08 02:27;michaelbusch;lucene-1446.patch;https://issues.apache.org/jira/secure/attachment/12393675/lucene-1446.patch","10/Nov/08 02:11;michaelbusch;lucene-1446.patch;https://issues.apache.org/jira/secure/attachment/12393603/lucene-1446.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12305,,,Tue Nov 11 02:32:29 UTC 2008,New,Patch Available,,,,,,"0|i04wxb:",26599,,,,,,,,,"11/Nov/08 02:27;michaelbusch;Small update: the availability check of a tag must be done in the sequential block of target 'test-tag'. I'll commit this soon.","11/Nov/08 02:32;michaelbusch;Committed revision 712920.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Make ReqExclScorer package private, and use DocIdSetIterator for excluded part.",LUCENE-1436,12407649,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,paul.elschot@xs4all.nl,paul.elschot@xs4all.nl,01/Nov/08 11:56,25/Sep/09 16:23,30/Sep/19 08:38,26/May/09 20:42,,,,,,,,,,,2.9,,,,core/search,,,0,,,,,,,,,,,,,,,,,,,,"01/Nov/08 12:00;paul.elschot@xs4all.nl;LUCENE-1436.patch;https://issues.apache.org/jira/secure/attachment/12393194/LUCENE-1436.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-05-21 18:58:51.261,,,false,,,,,,,,,,,,,,,12315,,,Tue May 26 20:03:25 UTC 2009,New,Patch Available,,,,,,"0|i04wzr:",26610,,,,,,,,,"01/Nov/08 12:00;paul.elschot@xs4all.nl;This also prepares for using Filters as clauses in BooleanQuery","21/May/09 18:58;mikemccand;Paul, this is technically a change in back-compat (since it's a public class, that this patch makes package private).  Are you thinking it's just very unlikely this is used by anything except BooleanScorer2?

We also have at least ReqOptSumScorer (and maybe others?) which is public.","21/May/09 20:20;shaie;I just hope this does not collide with LUCENE-1614. Note that in the follow-up issue to 1614 (the number is yet to be generated) I intend to change the logic in ReqExclScorer so that it conforms to the new semantics of DISI.

Why do we need to make this class package-private? Any particular reason besides not needing to worry about changes to back-compat in the future?","21/May/09 20:45;paul.elschot@xs4all.nl;This should only affect external code that uses the ReqExclScorer class.
From the amount of reactions on that so far I think such code probably does not exist.

The move to DocIdSetIterator (superclass of Scorer) can be done because no score values are used on the excluded part. This is only a consequence of the extraction of the DocIdSetIterator superclass from Scorer.

So I don't expect LUCENE-1614 or new semantics of DISI to collide with this.
Also, I don't expect LUCENE-1614 to collide with LUCENE-1345 that allows filter as clause to boolean query.

Some time ago DisjunctionSumScorer was made private in a similar way.

ReqOptSumScorer could also be made package private, perhaps even at the same time as ReqExclScorer.


","21/May/09 20:48;paul.elschot@xs4all.nl;The reason to make things package private is to have more freedom to change things lateron that are not really part of the public api now.","21/May/09 21:13;mikemccand;OK why don't we make both package private?  Are there other classes BooleanScorer/2 are using that should be made package private?","26/May/09 20:03;mikemccand;I'm assuming the answer to my last question was ""no"", so I'll go ahead and make these 2 classes package private.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expert API to specify indexing chain,LUCENE-1419,12406278,,New Feature,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,13/Oct/08 06:23,25/Sep/09 16:23,30/Sep/19 08:38,13/Oct/08 18:14,,,,,,,,,,,2.9,,,,core/index,,,0,,,,"It would be nice to add an expert API to specify an indexing chain, so that
we can make use of Mike's nice LUCENE-1301 feature.

This patch simply adds a package-protected expert API to IndexWriter and 
DocumentsWriter. It adds a inner, abstract class to DocumentsWriter called 
IndexingChain, and a default implementation that is the currently used one.

This might not be the final solution, but a nice way to play with different
modules in the indexing chain.

Could you take a look at the patch, Mike? ",,,,,,,,,,,,,,,,"13/Oct/08 06:24;michaelbusch;lucene-1419.patch;https://issues.apache.org/jira/secure/attachment/12391977/lucene-1419.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-10-13 08:49:12.008,,,false,,,,,,,,,,,,,,,12332,,,Mon Oct 13 18:14:56 UTC 2008,New,Patch Available,,,,,,"0|i04x3j:",26627,,,,,,,,,"13/Oct/08 08:49;mikemccand;This looks great Michael!","13/Oct/08 18:14;michaelbusch;Committed rev. 704193.
Thanks for reviewing, Mike!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ant contrib test can fail if there is a space in path to lucene project,LUCENE-1416,12406100,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,markrmiller@gmail.com,markrmiller@gmail.com,09/Oct/08 17:36,25/Sep/09 16:23,30/Sep/19 08:38,20/Oct/08 09:13,,,,,,,,,,,2.9,,,,modules/other,,,0,,,,"A couple contrib ant tests get the path to test files through a URL object, and so the path is URL encoded. Normally fine, but if you have a space in your path (/svn stuff/lucene/contrib/ant) then it will have %20 for the space and (at least on my Ubuntu system) the test will fail with filenotfound. This patch simply replaces all %20 with "" "". Not sure if we want/need to take it any further.",,,,,,,,,,,,,,,,"19/Oct/08 21:05;markrmiller@gmail.com;LUCENE-1416.patch;https://issues.apache.org/jira/secure/attachment/12392445/LUCENE-1416.patch","09/Oct/08 17:36;markrmiller@gmail.com;LUCENE-1416.patch;https://issues.apache.org/jira/secure/attachment/12391831/LUCENE-1416.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-10-15 09:27:42.684,,,false,,,,,,,,,,,,,,,12335,,,Mon Oct 20 09:13:07 UTC 2008,New,Patch Available,,,,,,"0|i04x47:",26630,,,,,,,,,"15/Oct/08 09:27;mikemccand;Is there a more generic function that can un-escape a URL-encoded string?  Space/%20 seems like the most common problem, but other characters are escaped in URLs too.","15/Oct/08 10:34;markrmiller@gmail.com;Yeah, thats why I mention we might want to take it further. Just wasn't 
sure if another char was actually a problem. We can use URLDecoder to 
hit everything though right? I'll put up a new patch when I get a chance.
","19/Oct/08 21:05;markrmiller@gmail.com;Patch using URLDecoder","20/Oct/08 09:13;mikemccand;Thank Mark, I just committed this!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ReverseStringFilter,LUCENE-1398,12404838,,New Feature,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,koji,koji,22/Sep/08 06:27,25/Sep/09 16:23,30/Sep/19 08:38,25/Feb/09 20:44,,,,,,,,,,,2.9,,,,modules/analysis,,,0,,,,"add ReverseStringFilter and ReverseStringAnalyzer that can be used for backword much. For Example, ""*ry"", ""*ing"", ""*ber"".",,,,,,,,,,,,,,,,"25/Feb/09 13:08;koji;LUCENE-1398.patch;https://issues.apache.org/jira/secure/attachment/12400940/LUCENE-1398.patch","19/Feb/09 03:17;koji;LUCENE-1398.patch;https://issues.apache.org/jira/secure/attachment/12400470/LUCENE-1398.patch","22/Sep/08 06:35;koji;LUCENE-1398.patch;https://issues.apache.org/jira/secure/attachment/12390607/LUCENE-1398.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-02-03 19:48:13.222,,,false,,,,,,,,,,,,,,,12353,,,Wed Feb 25 20:44:42 UTC 2009,New,,,,,,,"0|i04x87:",26648,,,,,,,,,"22/Sep/08 06:35;koji;Patch attached. The patch includes a new constructor of PrefixQuery:

{code:java}
public PrefixQuery(Term prefix, boolean reverse) {
  if(reverse){
    String s = prefix.text();
    int len = s.length();
    if( len <= 1 ){
      this.prefix = prefix;
      return;
    }
    StringBuffer sb = new StringBuffer(len);
    for( int i = 0; i < len; i++ ){
      sb.append( s.charAt( len - i - 1 ) );
    }
    this.prefix = new Term(prefix.field(),sb.toString());
  }
  else
    this.prefix = prefix;
  }
{code}

so that you can extend PrefixQuery to have SuffixQuery:

{code:java}
class SuffixQuery extends PrefixQuery {
  public SuffixQuery( Term term ){
    super( term, true );
  }
}
{code}
","03/Feb/09 19:48;markrmiller@gmail.com;I like the idea of adding this, but I almost think it might best live in contrib. Thats slightly at odds with the prefix query addition - but I'm not sure that really helps you to integrate the solution generally into the query language anyway - it just reverses the term. I think it might be better to leave that to the user, as its likely to remain a special case anyway. I'd rather even do something like a util reverse method.

So I'll hold out for a wiser opinion, but my initial thought would be to not add to prefixquery, and put the the analyzer in contrib","19/Feb/09 03:17;koji;Thanks Mark for the comment and sorry for late reply. Move it to contrib.","19/Feb/09 16:03;yseeley@gmail.com;I don't know how others feel, but I'd personally like to stop the practice of making more Analyzer classes whenever a new TokenFilter is added.

This patch looks like it could/should be a single class (other than test of course): ReverseTokenFilter.  The static reverse() methods could be put directly on that filter.
","25/Feb/09 13:08;koji;{quote}
I don't know how others feel, but I'd personally like to stop the practice of making more Analyzer classes whenever a new TokenFilter is added.

This patch looks like it could/should be a single class (other than test of course): ReverseTokenFilter. The static reverse() methods could be put directly on that filter.
{quote}

No problem for me. :) Done.","25/Feb/09 20:44;yseeley@gmail.com;Thanks, I just committed this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DisjunctionSumScorer small tweak,LUCENE-1145,12386789,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,eksdev,eksdev,21/Jan/08 16:34,25/Sep/09 16:23,30/Sep/19 08:38,18/Mar/09 16:45,,,,,,,,,,,2.9,,,,core/search,,,0,,,,"Move ScorerDocQueue initialization from next() and skipTo() methods to the Constructor. Makes DisjunctionSumScorer a bit faster (less than 1% on my tests). 

Downside (if this is one, I cannot judge) would be throwing IOException from DisjunctionSumScorer constructors as we touch HardDisk there. I see no problem as this IOException does not propagate too far (the only modification I made is in BooleanScorer2)

if (scorerDocQueue == null) {
      initScorerDocQueue();
}
 

Attached test is just quick & dirty rip of  TestScorerPerf from standard Lucene test package. Not included as patch as I do not like it.


All test pass, patch made on trunk revision 613923
",all,,,,,,,,,,,,,,,"23/Jan/08 15:51;eksdev;DSSQueueSizeOptimization.patch;https://issues.apache.org/jira/secure/attachment/12373839/DSSQueueSizeOptimization.patch","21/Jan/08 16:35;eksdev;DisjunctionSumScorerOptimization.patch;https://issues.apache.org/jira/secure/attachment/12373687/DisjunctionSumScorerOptimization.patch","21/Jan/08 16:37;eksdev;TestScorerPerformance.java;https://issues.apache.org/jira/secure/attachment/12373688/TestScorerPerformance.java",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-01-23 18:11:21.799,,,false,,,,,,,,,,,,,,,12600,,,Wed Mar 18 16:45:33 UTC 2009,New,Patch Available,,,,,,"0|i04ysn:",26902,,,,,,,,,"23/Jan/08 15:51;eksdev;Simplification of the DisjunctionSumScorer. 
- removed cached field ""private int queueSize"" which mirrored ScorerDocQueue.size()  and 
replaced it with method call.

It is faster with this patch, but hardly measurable (test made with attached TestScorerPerformance) 585660ms vs 586090ms. Test on WIN XP  Prof. Dual Core Intel T7300 2GHz with 6.0 java -server -Xbatch

At a moment, I have no other configurations to test it, it would be good to see what happens on jvm 1.4   

It makes sense to commit this as it simplifies (pff, ok, simpifies it a bit :) already complex code in DSScorer and is not slower. ","23/Jan/08 18:11;paul.elschot@xs4all.nl;When I wrote it, using the queueSize variable did make a minor difference in performance.
But with the result you have, I think it's better use the size() call only.
","23/Jan/08 22:18;eksdev;Well, I do not know how it behaves on earlier jvm-s and what would be the ""jvm we optimize"", I would not be surprised if jvm 6+ evolved optimization methods. 
These patches are just side effects of trying to get familiar with scorer family inner working in light of LUCENE-584. Boolean arithmetic on multiple skipping  iterators in Scorers can hardly be beaten and can be recycled for cases like BooleanFilter... and maybe one day merged to avoid code duplication :)

Anyhow, if it proves that performance on 1.4 behaves similarly, I would opt for size(), makes code slightly cleaner. If not, I would suggest to replace the only size() usage  in next() with cached queueSize","08/Feb/08 08:54;eksdev;test using Sun 1.4 jvm on the same hardware showed the same ""a bit faster"" behavior, so this is in my opinion OK to be committed.   ","18/Mar/09 16:24;mikemccand;I plan to commit shortly.","18/Mar/09 16:45;mikemccand;Thanks Eks and Paul!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In a benchmark alg, if you use a log.step thats higher than the number of docs you add, whacky high numbers are logged for doc adds",LUCENE-1774,12431970,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Invalid,,markrmiller@gmail.com,markrmiller@gmail.com,01/Aug/09 20:23,03/Aug/09 14:02,30/Sep/19 08:38,02/Aug/09 17:23,,,,,,,,,,,,,,,modules/benchmark,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-08-02 10:06:44.128,,,false,,,,,,,,,,,,,,,11988,,,Mon Aug 03 14:02:43 UTC 2009,New,,,,,,,"0|i04uw7:",26270,,,,,,,,,"02/Aug/09 10:06;shaie;Mark - can you perhaps paste here an .alg file and the output? I executed readContentSource.alg like this, but didn't see anything ""whacky"" printed. Perhaps I'm not running the exact algorithm? Does it have to do w/ indexing?

{code}
content.source=org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource
docs.file=temp/enwiki-20070527-pages-articles.xml.bz2

# Stop after processing the document feed once:
content.source.forever=false

# Log messages every:
log.step=100000

# -------------------------------------------------------------------------------------

{ ConsumeContentSource } : 10

RepSumByPref ConsumeContentSource
{code}

I also tried indexLineFile.alg, but again I didn't see anything weird.","02/Aug/09 13:22;markrmiller@gmail.com;Yeah, I'll post some further info when I get a moment - it may have been some aberration or something? I was seeing it while working with some enwiki benchmark stuff yesterday - if I put a step of 10,000 and then added 500 docs, it would output, 10,000, 20,000, 30,000 - 50,000 for every 100. I'll try and duplicate it again when I get a chance.","02/Aug/09 15:45;shaie;strange. I've tried to reproduce but failed, so if you succeed, I'd like to take a look too.","02/Aug/09 17:23;markrmiller@gmail.com;My fault - had 50k search task that was doing the logging. I don't know if thats new or not, but I was used to just seeing that with adding docs - so when it still jumped by 10,000 when i added 500 docs it appeared a bit weird - when I lowered the step to 100, I saw what I expected - but I didn't realize that the odd numbers I was seeing was the searching task logging by log.step.

","02/Aug/09 18:26;shaie;That is indeed new (I added it in LUCENE-1595 as part of the general refactoring). Now PerfTask includes logging logic, determined by log.step. It means that one log.step controls all tasks, which I'm not sure if it's good or not (doc.delete.log.step controls just deletes because it seemed to be used by several algs).

Maybe, as part of this issue or another issue, we can have PerfTask support a parameter, which will be the log.step? That way you can choose to AddDocTask(100), and SearchTask(-1) to enable logging for adding docs and disable for search. Though that will conflict w/ the other parameter AddDocTask supports (document size). So perhaps we can enhance the parameters to accept a list? Just a thought.","03/Aug/09 13:14;markrmiller@gmail.com;Good idea!

I'm not so keen on adding it as a param though.

What about using log.step as a default that works for all tasks, and also having log.step.PerfTask that can control by task?

Then delete would be log.step.DeleteDocTask=100 and you could individually set any of the others if you choose.

Or Something along those lines? I almost think the setting should stay out of the alg logic though - but it would be nice to have finer control - to do things like match the old behavior:

log.step.AddDocTask=1000

Def deserves a new issue if we do something here I think.","03/Aug/09 13:55;shaie;I like that idea. DeleteDocTask today defines a ""delete.log.step"", but it's defined only for that task. There is no ""search.log.step"" for example. 

Having a pattern like log.step.<Task Class name> will allow us to support it in PerfTask by doing something like:
{code}
    logStep = config.get(""log.step"", DEFAULT_LOG_STEP);
    if (logStep <= 0) {
      logStep = Integer.MAX_VALUE;
    }

    // Check if a log step for the current task is defined
    logStep = config.get(""log.step."" + getClass().getSimpleName(), DEFAULT_LOG_STEP);
    if (logStep <= 0) {
      logStep = Integer.MAX_VALUE;
    }

{code}

And so we can support a log.step for any task, irregardless of whether that task included support for it or not. And we can remove the delete.log.step param (new in 2.9 so no problem). Note that the above code is 1.5 dependent, but I can make it 1.4 compliant w/ a TODO to switch to getSimpleName() in 1.5.

What do you think? Shall I open an issue for that? Or can you reopen that issue and I'll prepare a patch?","03/Aug/09 14:02;markrmiller@gmail.com;bq. What do you think? Shall I open an issue for that? Or can you reopen that issue and I'll prepare a patch?

+1 - I vote new issue for it. If you make the patch, I'll commit it for 2.9.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slightly more readable code in Token/TermAttributeImpl,LUCENE-1762,12431427,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,eksdev,eksdev,25/Jul/09 13:21,29/Jul/09 19:15,30/Sep/19 08:38,29/Jul/09 19:15,2.9,,,,,,,,,,2.9,,,,modules/analysis,,,0,,,,"No big deal. 

growTermBuffer(int newSize) was using correct, but slightly hard to follow code. 

the method was returning null as a hint that the current termBuffer has enough space to the upstream code or reallocated buffer.

this patch simplifies logic   making this method to only reallocate buffer, nothing more.  
It reduces number of if(null) checks in a few methods and reduces amount of code. 
all tests pass.

This also adds tests for the new basic attribute impls (copies of the Token tests).",,,,,,,,,,,,,,,,"28/Jul/09 07:56;uschindler;LUCENE-1762-bw.patch;https://issues.apache.org/jira/secure/attachment/12414722/LUCENE-1762-bw.patch","28/Jul/09 10:53;uschindler;LUCENE-1762.patch;https://issues.apache.org/jira/secure/attachment/12414736/LUCENE-1762.patch","27/Jul/09 21:51;uschindler;LUCENE-1762.patch;https://issues.apache.org/jira/secure/attachment/12414669/LUCENE-1762.patch","25/Jul/09 23:12;eksdev;LUCENE-1762.patch;https://issues.apache.org/jira/secure/attachment/12414524/LUCENE-1762.patch","25/Jul/09 22:51;eksdev;LUCENE-1762.patch;https://issues.apache.org/jira/secure/attachment/12414523/LUCENE-1762.patch","25/Jul/09 13:23;eksdev;LUCENE-1762.patch;https://issues.apache.org/jira/secure/attachment/12414514/LUCENE-1762.patch",,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2009-07-25 13:57:39.579,,,false,,,,,,,,,,,,,,,11999,,,Wed Jul 29 19:15:38 UTC 2009,New,Patch Available,,,,,,"0|i04uyv:",26282,,,,,,,,,"25/Jul/09 13:57;uschindler;As Token is not yet deprecated, I think, this patch should also apply to Token.java?
Can you prepare that, too?

(This is important, because if the backwards-compatibility layer is enabled with setOnlyUseNewAPI(false)), the TermAttributeImpl is never used and a Token instance is used instead - if no tests fail, this may also be the case :-] )","25/Jul/09 22:51;eksdev;made the changes in Token along the same lines, 

- had to change one constant in TokenTest as I have changed initial allocation policy of termBuffer to be consistent with Arayutils.getnextSize()

if(termBuffer==null)

NEW:
 termBuffer = new char[ArrayUtil.getNextSize(newSize < MIN_BUFFER_SIZE ? MIN_BUFFER_SIZE : newSize)]; 

OLD:
termBuffer = new char[newSize < MIN_BUFFER_SIZE ? MIN_BUFFER_SIZE : newSize]; 

not sure if this is better, but looks more consistent to me (buffer size is always determined via getNewSize())

Uwe, 
setOnlyUseNewAPI(false) does not exist, it was removed with some of the patches lately. It gets automatically detected via reflection?

","25/Jul/09 23:12;eksdev;- made allocation in initTermBuffer() consistent with ArrayUtil.getNextSize(int) -  this is ok not to start with MIN_BUFFER_SIZE, but rather with ArrayUtil.getNextSize(MIN_BUFFER_SIZE)... e.g. if getNextSize gets very sensitive to initial conditions one day...
 
- null-ed  termText on switch to termBuffer in resizeTermBuffer (as it was before!) . This was a bug in previous patch  ","26/Jul/09 07:35;uschindler;bq. setOnlyUseNewAPI(false) does not exist, it was removed with some of the patches lately. It gets automatically detected via reflection?

No, this is a static global switch in TokenStream. If you switch it on, TokenStreams and Filters use only the new API forcefully and therefore use the separate Attribute implementations from o.a.l.analysis.tokenattributes. If it is switched off, a old Token instance is used instead, see [http://hudson.zones.apache.org/hudson/job/Lucene-trunk/javadoc/core/org/apache/lucene/analysis/TokenStream.html#setOnlyUseNewAPI(boolean)]. The red color bug is fixed in trunk now :)

There is one problem with the 6 new single attribute instances: They are code duplicates from Token but have no Test. I also think, I should add a missing test similar to TestToken.java and do the same test with 6 Attribute instances.

I will review the other changes later, I have no time today.","27/Jul/09 21:51;uschindler;Extended patch. It adds also tests for the basic attributes (in trunk only token was tested). The new tests check every impls basic functionality and clone/copyTo. It also contains the buffer tests of Token for TermAttribute.

To apply the patch you must first do a:
{code}
svn copy src/test/org/apache/lucene/analysis/TestToken.java src/test/org/apache/lucene/analysis/tokenattributes/TestTermAttributeImpl.java
{code}

It also modified the AttributeImpls default toString() to only print instance fields, not static fields.","27/Jul/09 21:53;uschindler;Eks, the original patch looked good, I did not change anything in your code, I only added the above mentioned tests and other changes.

One thing: As TestToken checks the buffer sizes, the test in the backwards-branch for Token must also be changed. I will do this before commit.","27/Jul/09 22:20;eksdev;cool, thanks for the review.   ","28/Jul/09 07:56;uschindler;Here the modified BW-test to apply on backwards-branch. It respects the new default buffer size.","28/Jul/09 10:53;uschindler;Slightly modified patch (some small refactoring). I also fixed a bug in Token, because the Payload was not deep cloned in copyTo(), which is done by the simple attributes and needed for restoring the state in AttributeSource. Also added tests for this.

I think it is ready for commit. I will do this in a day or two.","29/Jul/09 19:15;uschindler;Committed revision: 799025
This is without CHANGES.txt updates, because nothing was changed that is visible to the outside :-)

Thanks Eks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.lucene.ant.HtmlDocument added Tidy config file passthrough availability,LUCENE-1704,12428394,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,ksprochi,ksprochi,19/Jun/09 18:52,06/Jul/09 19:55,30/Sep/19 08:38,06/Jul/09 19:55,2.4.1,,,,,,,,,,,,,,modules/other,,,0,,,,"Parsing HTML documents using the org.apache.lucene.ant.HtmlDocument.Document method resulted in many error messages such as this:

    line 152 column 725 - Error: <as-html> is not recognized!
    This document has errors that must be fixed before
    using HTML Tidy to generate a tidied up version.

The solution is to configure Tidy to accept these abnormal tags by adding the tag name to the ""new-inline-tags"" option in the Tidy config file (or the command line which does not make sense in this context), like so:

    new-inline-tags: as-html

Tidy needs to know where the configuration file is, so a new constructor and Document method can be added.  Here is the code:

{code}
    /**                                                                                                                                                                                            
     *  Constructs an <code>HtmlDocument</code> from a {@link                                                                                                                                      
     *  java.io.File}.                                                                                                                                                                             
     *                                                                                                                                                                                             
     *@param  file             the <code>File</code> containing the                                                                                                                                
     *      HTML to parse                                                                                                                                                                          
     *@param  tidyConfigFile   the <code>String</code> containing                                                                                                                                  
     *      the full path to the Tidy config file                                                                                                                                                  
     *@exception  IOException  if an I/O exception occurs                                                                                                                                          
     */
    public HtmlDocument(File file, String tidyConfigFile) throws IOException {
        Tidy tidy = new Tidy();
        tidy.setConfigurationFromFile(tidyConfigFile);
        tidy.setQuiet(true);
        tidy.setShowWarnings(false);
        org.w3c.dom.Document root =
                tidy.parseDOM(new FileInputStream(file), null);
        rawDoc = root.getDocumentElement();
    }

    /**                                                                                                                                                                                            
     *  Creates a Lucene <code>Document</code> from a {@link                                                                                                                                       
     *  java.io.File}.                                                                                                                                                                             
     *                                                                                                                                                                                             
     *@param  file                                                                                                                                                                                 
     *@param  tidyConfigFile the full path to the Tidy config file                                                                                                                                 
     *@exception  IOException                                                                                                                                                                      
     */
    public static org.apache.lucene.document.Document
        Document(File file, String tidyConfigFile) throws IOException {

        HtmlDocument htmlDoc = new HtmlDocument(file, tidyConfigFile);

        org.apache.lucene.document.Document luceneDoc = new org.apache.lucene.document.Document();

        luceneDoc.add(new Field(""title"", htmlDoc.getTitle(), Field.Store.YES, Field.Index.ANALYZED));
        luceneDoc.add(new Field(""contents"", htmlDoc.getBody(), Field.Store.YES, Field.Index.ANALYZED));

        String contents = null;
        BufferedReader br =
            new BufferedReader(new FileReader(file));
        StringWriter sw = new StringWriter();
        String line = br.readLine();
        while (line != null) {
            sw.write(line);
            line = br.readLine();
        }
        br.close();
        contents = sw.toString();
        sw.close();

        luceneDoc.add(new Field(""rawcontents"", contents, Field.Store.YES, Field.Index.NO));

        return luceneDoc;
    }
{code}

I am using this now and it is working fine.  The configuration file is being passed to Tidy and now I am able to index thousands of HTML pages with no more Tidy tag errors.

",,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-07-06 19:35:03.35,,,false,,,,,,,,,,,,,,,12054,,,Mon Jul 06 19:55:12 UTC 2009,New,,,,,,,"0|i04vbr:",26340,,,,,,,,,"19/Jun/09 18:56;ksprochi;The code got mangled when converted to HTML, so I tried adding <pre> tags to prevent formatting. Their is no preview option, so I won't know if it worked until I hit the update button.
","19/Jun/09 18:58;ksprochi;Ok, pre tags did not work, so I took them out.  I have no remedy that will make the code format properly.","06/Jul/09 19:35;mikemccand;There is a preview button (that small blue computer-like icon to the left of the yellow question mark help icon), and, you can use \{code} to wrap code so Jira doesn't format it.","06/Jul/09 19:42;ksprochi;Much better, thanks.  I guess I should have RTFM.
","06/Jul/09 19:53;mikemccand;OK the patch looks good -- I'll commit shortly.  Thanks Keith!","06/Jul/09 19:55;mikemccand;Thanks Keith!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Unnecessary NULL check in FindSegmentsFile - cleanup,LUCENE-1686,12427732,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,uschindler,simonw,simonw,12/Jun/09 11:35,12/Jun/09 12:29,30/Sep/19 08:38,12/Jun/09 12:29,,,,,,,,,,,2.9,3.0,,,,,,0,,,,"FindSegmentsFile accesses the member ""directory"" in line 579 while performing a null check in 592. The null check is unnecessary as if directory is null line 579 would throw a NPE.
I removed the null check and made the member ""directory"" final. In addition I added a null check in the constructor as If the value is null we should catch it asap. 

",,,,,,,,,,,,,,,,"12/Jun/09 11:36;simonw;segments_info.patch;https://issues.apache.org/jira/secure/attachment/12410479/segments_info.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-06-12 12:16:52.145,,,false,,,,,,,,,,,,,,,12072,,,Fri Jun 12 12:29:11 UTC 2009,New,Patch Available,,,,,,"0|i04vfz:",26359,,,,,,,,,"12/Jun/09 12:16;uschindler;Thanks, I was working on this class some days ago, I will apply this!

Thanks!","12/Jun/09 12:29;uschindler;Comitted revision 784094.

This was my fault, because I refactored the class and missed to remove this unnecessary check. Before there was also support for java.io.File instead of o.a.l.store.Directory in this class for which this null-check was.
I removed the initial null check in the ctor, as it is not necessary (this class is internal only and never called with NULL directory).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
contrib/javascript is not packaged into releases,LUCENE-898,12370623,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,otis,hossman,hossman,31/May/07 18:08,02/Jun/09 15:19,30/Sep/19 08:38,02/Jun/09 15:19,,,,,,,,,,,,,,,general/build,,,0,,,,"the contrib/javascript directory is (apparently) a collection of javascript utilities for lucene .. but it has not build files or any mechanism to package it, so it is excluded form releases.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-05-31 19:12:46.305,,,false,,,,,,,,,,,,,,,12844,,,Tue Jun 02 15:19:20 UTC 2009,New,,,,,,,"0|i050bj:",27149,,,,,,,,,"31/May/07 19:12;ehatcher;My vote is to remove the javascript contrib area entirely.  It doesn't really do all that much useful.  I'd be surprised if anyone really uses it.","01/Jun/07 04:17;michaelbusch;> My vote is to remove the javascript contrib area entirely. 

+1. It also seems that this package is unmaintained. No files have
been changed since February 2005, when it was moved from the 
sandbox to contrib.","01/Jun/07 05:31;otis;I think the files have not changed in a while because they work.  I believe Kelvin Tan (the author) used/uses this stuff somewhere.  I'm typically for cleaning things up, but somehow I feel that this javascript stuff should be left alone (it ain't broken, is it?).","01/Jun/07 14:00;ehatcher;It may still work ok, but my hunch is that changes to the QueryParser have made this javascript code more deprecated than anything.  

Even if we removed it from svn, it historically would still be there in case anyone really needed it.   

Again, I am +1 for removing it entirely after running it by the java-user list to see if anyone desires it.","26/Jul/07 23:30;otis;I've emailed Kelvin and asked for his thoughts.  Maybe he'll update this code, who knows.","31/Jul/07 15:20;otis;Kelvin says:

""Hey otis, thanks for forwarding this to me.

Fine with me to have it removed..""
","22/May/08 06:49;otis;I'll take care of this in a few days...it looks like nobody will miss it.
","02/Jun/09 15:19;otis;Done.

D         javascript/queryEscaper/luceneQueryEscaper.js
D         javascript/queryEscaper/testQueryEscaper.html
D         javascript/queryEscaper
D         javascript/queryConstructor/luceneQueryConstructor.js
D         javascript/queryConstructor/luceneQueryConstructor.html
D         javascript/queryConstructor/testQueryConstructor.html
D         javascript/queryConstructor
D         javascript/queryValidator/luceneQueryValidator.js
D         javascript/queryValidator/testQueryValidator.html
D         javascript/queryValidator
D         javascript

Committed revision 781057.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deprecated method used in fieldsReader / setOmitTf(),LUCENE-1615,12423799,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,eksdev,eksdev,26/Apr/09 15:06,27/Apr/09 09:34,30/Sep/19 08:38,27/Apr/09 09:34,,,,,,,,,,,,,,,core/index,,,0,,,,"setOmitTf(boolean) is deprecated and should not be used by core classes. One place where it appears is FieldsReader , this patch fixes it. It was necessary to change Fieldable to AbstractField at two places, only local variables.   ",,,,,,,,,,,,,,,,"26/Apr/09 15:07;eksdev;LUCENE-1615.patch;https://issues.apache.org/jira/secure/attachment/12406470/LUCENE-1615.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-04-26 15:31:04.629,,,false,,,,,,,,,,,,,,,12142,,,Mon Apr 27 09:34:23 UTC 2009,New,Patch Available,,,,,,"0|i04vvz:",26431,,,,,,,,,"26/Apr/09 15:31;uschindler;We know this problem, your fix seems ok (LUCENE-1561).
We did not want to change the Fieldable interface again, so we left omitTf in the interface but deprecated the methods in AbstractField & Co. In future, the Fieldable interface should be completely removed for 3.0 and this is a first step towards it! All references to Fieldable should be replaced by AbstractField or a better alternative that also has the type in it (see LUCENE-1597)","26/Apr/09 16:02;eksdev;sure, replacing Fieldable is good,  just noticed quick win when cleaning-up deprecations from our code base... one step in a time ","27/Apr/09 09:34;mikemccand;OK I just committed this -- thanks Eks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changes.html formatting improvements,LUCENE-1256,12392792,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,doronc,sarowe,sarowe,01/Apr/08 00:03,30/Oct/08 23:20,30/Sep/19 08:38,20/Apr/08 18:32,2.4,,,,,,,,,,2.4,,,,general/website,,,0,,,,"Some improvements to the Changes.html generated by the changes2html.pl script via the 'changes-to-html' ant task:

# Simplified the Simple stylesheet (removed monospace font specification) and made it the default.  The Fancy stylesheet is really hard for me to look at (yellow text on light blue background may provide high contrast with low eye strain, but IMHO it's ugly).
# Moved the monospace style from the Simple stylesheet to a new stylesheet named ""Fixed Width""
# Fixed syntax errors in the Fancy stylesheet, so that it displays as intended.
# Added <span style=""attrib"">  to change attributions.
# In the Fancy and Simple stylesheets, change attributions are colored dark green.
# Now properly handling change attributions in CHANGES.txt that have trailing periods.
# Clicking on an anchor to expand its children now changes the document location to show the children.
# Hovering over anchors now causes a tooltip to be displayed - either ""Click to expand"" or ""Click to collapse"" - the tooltip changes appropriately after a collapse or expansion.",,,,,,,,,,,,LUCENE-1433,,,,"18/Apr/08 20:29;sarowe;LUCENE-1256.patch;https://issues.apache.org/jira/secure/attachment/12380539/LUCENE-1256.patch","18/Apr/08 19:36;doronc;LUCENE-1256.patch;https://issues.apache.org/jira/secure/attachment/12380536/LUCENE-1256.patch","02/Apr/08 20:02;sarowe;LUCENE-1256.patch;https://issues.apache.org/jira/secure/attachment/12379178/LUCENE-1256.patch","02/Apr/08 18:37;sarowe;LUCENE-1256.patch;https://issues.apache.org/jira/secure/attachment/12379168/LUCENE-1256.patch","02/Apr/08 14:56;sarowe;LUCENE-1256.patch;https://issues.apache.org/jira/secure/attachment/12379141/LUCENE-1256.patch","01/Apr/08 00:04;sarowe;LUCENE-1256.patch;https://issues.apache.org/jira/secure/attachment/12378990/LUCENE-1256.patch",,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2008-04-02 16:24:57.781,,,false,,,,,,,,,,,,,,,12492,,,Sun Apr 20 18:32:36 UTC 2008,Patch Available,,,,,,,"0|i04y3r:",26790,,,,,,,,,"01/Apr/08 00:04;sarowe;Patch implementing above-described changes.","01/Apr/08 00:10;sarowe;The issue that introduced generation of Changes.html","02/Apr/08 14:56;sarowe;Since the Lucene Bugzilla bugs mentioned in CHANGES.txt are no longer present in the Apache Bugzilla database, it doesn't make sense to auto-linkify these any longer.

Instead, this version of the patch links from each mentioned Bugzilla bug number to the JIRA issue that was cloned from it, and includes the JIRA issue name.  For example, the following text:

bq. ArrayIndexOutOfBoundsExceptions (patch #9110); 

becomes:

bq. ArrayIndexOutOfBoundsExceptions ([patch #9110 \[LUCENE-35\]|http://issues.apache.org/jira/browse/LUCENE-35]); 
","02/Apr/08 16:24;doronc;I didn't try it but from the description it sounds great.
I'll try it over the weekend.
For the fancy stylesheet - I put those colors but no sentiments for them here - please feel free to modify them to your taste..:-)","02/Apr/08 18:37;sarowe;One last bugfix: item #3.1 under 2.3.0's ""New features"" section is intended to be understood as a component of item #3.  This version of the patch renumbers this item to #4, like all previous versions, but it no longer includes a '1' at the beginning of the item's text, as all previous versions did.

Should hierarchical change item formatting be supported?  AFAICT, the above-described item #3.1 is the only occurrence of this phenomenon currently in CHANGES.txt.","02/Apr/08 20:02;sarowe;One last feature: added ""Expand All"" and ""Collapse All"" buttons at the top of the page.  

These buttons are added via Javascript, so they don't appear when Javascript disabled and everything is always expanded.

This version also cleans up some mis-identified change attributions, e.g. ""(see #3 above)"".","18/Apr/08 19:36;doronc;Patch looks great! 
One minor issue the dynamically inserted buttons do not work on IE 6 for me. 
I changed that to create them in the static HTML lines and now it works in both IE and FF. 
Also modified the colors of the  fancy style to be more pleasant and made it default again - for visible separation between the three parts (trunk, last release, older releases).
Attaching the update - will commit soon. 
Doron","18/Apr/08 20:29;sarowe;Hi Doron, I like the Fancy stylesheet now - thanks.  Attaching a new patch that contains a couple of small changes from your version:

# I found a way to fix the bad IE6 behavior with the dynamic Expand All/Collapse All buttons, so I removed your static <input> tags and put the button creation back in the <head><SCRIPT> tag.  I prefer this way, because the buttons will not appear at all if javascript is disabled.
# Removed an extra copy of styles in ChangesFixedWidthStyle.css
# Removed the alternative stylesheets <link> to the Fancy stylesheet (not necessary since it's the default stylesheet)
","18/Apr/08 20:31;doronc;I just realized that adding the buttons statically means they are shown also when javascript is disabled and then they do nothing.
I would like to have both - buttons that work also in IE6 and view that is consistent also with javascript disabled.
If this is not possible I think it is more important for IE6 to work properly. Steven?
","18/Apr/08 20:38;doronc;cool, thanks, that was fast!","18/Apr/08 20:48;sarowe;I just realized that I didn't save the editor buffer where I removed the extra styles from ChangesFixedWidthStyle.css -- somehow, there are *three* copies of each style in there.  Not attaching (yet another) patch - can you manually remove the extras, Doron?","18/Apr/08 20:51;doronc;Actually there are now 3 duplicates in ChangesFixedWidthStyle.css - never mind I'll remove that. 
Yes now without the setAttribute() it works for me too in IE6 as well. 
So I am committing this.
","18/Apr/08 20:52;doronc;Second time in a row(e) :-) that I am too slow for your fast response... ","18/Apr/08 21:14;sarowe;One tiny display issue I noticed: the Bugzilla issue autolinkification places the </a> tag before the closing right square bracket (']') around the LUCENE-XXXX issue, so the closing bracket is not part of the link text, but it should be. 

This occurs on line #389 on changes2html.pl:

{code}
   . qq!$issue&nbsp;[LUCENE-$jira_issue_num</a>]!
{code}

should be

{code}
   . qq!$issue&nbsp;[LUCENE-$jira_issue_num]</a>!
{code}

Again, not including another patch - would you mind fixing this for me, Doron?","19/Apr/08 16:11;doronc;sure np","20/Apr/08 18:32;doronc;Committed, Thanks Steve!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Deprecation of autoCommit in 2.4 leads to compile problems, when autoCommit should be false",LUCENE-1401,12404936,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,uschindler,uschindler,23/Sep/08 09:08,11/Oct/08 12:49,30/Sep/19 08:38,25/Sep/08 12:07,2.4,2.9,,,,,,,,,2.4,2.9,,,core/index,,,0,,,,"I am currently changing my code to be most compatible with 2.4. I switched on deprecation warnings and got a warning about the autoCommit parameter in IndexWriter constructors.

My code *should* use autoCommit=false, so I want to use the new semantics. The default of IndexWriter is still autoCommit=true. My problem now: How to disable autoCommit whithout deprecation warnings?

Maybe, the ""old"" constructors, that are deprecated should use autoCommit=true. But there are new constructors with this ""IndexWriter.MaxFieldLength mfl"" in it, that appear new in 2.4 but are deprecated:

IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create, IndexDeletionPolicy deletionPolicy, IndexWriter.MaxFieldLength mfl) 
          Deprecated. This will be removed in 3.0, when autoCommit will be hardwired to false. Use IndexWriter(Directory,Analyzer,boolean,IndexDeletionPolicy,MaxFieldLength) instead, and call commit() when needed.

What the hell is meant by this, a new constructor that is deprecated? And the hint is wrong. If I use the other constructor in the warning, I get autoCommit=true.

There is something completely wrong.

It should be clear, which constructors set autoCommit=true, which set it per default to false (perhaps new ones), and the Deprecated text is wrong, if autoCommit does not default to false.",,,,,,,,,,,,,,,,"23/Sep/08 22:02;mikemccand;LUCENE-1401.patch;https://issues.apache.org/jira/secure/attachment/12390789/LUCENE-1401.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-09-23 09:40:09.876,,,false,,,,,,,,,,,,,,,12350,,,Thu Sep 25 12:07:46 UTC 2008,New,,,,,,,"0|i04x7j:",26645,,,,,,,,,"23/Sep/08 09:40;mikemccand;
Achieving the migration from autoCommit=true to autoCommit=false is somewhat tricky.  As things stand now, all ctors that don't take autoCommit param still default autoCommit to true.

Maybe we could  flip the autoCommit default to false, now, with the new ctors (the ones that take a MaxFieldLength).  This may be better since you have to explicitly update your code, anyway, to switch to IndexWriter's new ctors and so if we call this change out in the javadocs, users are more likely to catch it.  Whereas if we suddenly flip the default for autoCommit to false in 3.0, since there's no API signature change, users may not realize this had happened.

OK I like that approach better.  Are there any objections?

bq. But there are new constructors with this ""IndexWriter.MaxFieldLength mfl"" in it, that appear new in 2.4 but are deprecated

I agree: we should not introduce new deprecated ctors.  I'll eliminate these.  This happened because there were two separate changes (addition of MaxFieldLength, and, deprecation of autoCommit).

","23/Sep/08 22:02;mikemccand;Attached patch that removes the new deprecated ctors, and sets autoCommit=false for the new ctors (that take MaxFieldLength).

The bulk of the patch is fixing all places where we were calling the new deprecated ctors.","24/Sep/08 08:14;uschindler;This patch seems to work, the IndexWriters created by the MaxFieldLength ctors are with autocommit=false, I have seen this, because the segment file does not change during indexing.

There is on small thing (was also there before your patch):
I use writer.setUseCompoundFile(true) to use compound files (which is also the default). It generates normally always only CFS files (on index creation, when optimizing,...). There is only one use case, when cfs and cfx files are generated:

- Use IndexWriter with create=true
- add documents to the index
- optimize the index (without closing in between)

After that the optimized index contains of one cfs and one cfx. During indexing (before optimization), I always see only cfs files for new segments (and for short times as usual the contents tfx,...).

When optimizing the index after closing it or later after adding documents, i got only one cfs file.

Two questions:
- Is this a small bug, which would be not release critical - but it is strange?
- How can I enable creation of doc store (cfx) and cfs always, I found nothing in the docs. In my opinion the separate cfs/cfx files are good for search performance (right?).","24/Sep/08 10:02;mikemccand;
That (cfx/cfs file creation) is actually ""normal"" behavior for
Lucene.

With autoCommit=false, in a single session of IndexWriter, Lucene
will share the doc store files (stored fields, term vectors) across
multiple segments.  This saves alot of merge time because those files
don't need to be merged if we are merging segments that all share the
same doc store files.  When building up a large index anew this saves
alot of time.

A cfx file is the compound-file format of the doc store files.

However, when segments spanning multiple doc stores are merged, then
the doc store files are in fact merged, and written privately for that
one segment, and then folded into that segment's cfs file.  When all
such segments reference a given doc store segment are merged away,
then that doc store segment is deleted.

So it's currently only the ""level 0"" segments that may share a cfx
file.  As a future optimization we could consider extending Lucene's
index format so that a single segment could reference multiple doc
stores.  This would require logic in FieldsReader and
TermVectorsReader to do a binary search when locating which doc store
segment holds a given document, but, would enable merging non level 0
segments to skip having to merge the doc store.  This is an invasive
optimization.

So you can't separately control when Lucene uses cfx file; it's the
merge policy that indirectly controls this.","24/Sep/08 14:31;uschindler;Thanks for the info, it did not know this!","25/Sep/08 12:07;mikemccand;Committed revision 698932 on trunk (2.9) and 698933 on 2.4.  Thanks Uwe!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some small javadocs/extra import fixes,LUCENE-1392,12404695,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,18/Sep/08 23:43,11/Oct/08 12:49,30/Sep/19 08:38,19/Sep/08 14:53,2.4,,,,,,,,,,2.4,2.9,,,,,,0,,,,"Two things that Uwe Schindler caught, plus fixes for javadoc warnings in core.  I plan to commit to trunk & 2.4.",,,,,,,,,,,,,,,,"25/Sep/08 11:05;uschindler;LUCENE-1392-more-imports.patch;https://issues.apache.org/jira/secure/attachment/12390916/LUCENE-1392-more-imports.patch","18/Sep/08 23:44;mikemccand;LUCENE-1392.patch;https://issues.apache.org/jira/secure/attachment/12390438/LUCENE-1392.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-09-25 11:05:40.362,,,false,,,,,,,,,,,,,,,12359,,,Thu Sep 25 11:05:40 UTC 2008,New,,,,,,,"0|i04x9j:",26654,,,,,,,,,"19/Sep/08 14:53;mikemccand;Committed revision 697119 (2.4) and 697115 (trunk).","25/Sep/08 11:05;uschindler;During my code analysis, I found two more unneeded imports of BitSet. See attached patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add init method to CloseableThreadLocal,LUCENE-1388,12404392,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,jasonrutherglen,jasonrutherglen,15/Sep/08 21:30,11/Oct/08 12:49,30/Sep/19 08:38,16/Sep/08 14:24,2.3.2,,,,,,,,,,2.4,,,,core/index,,,0,,,,Java ThreadLocal has an init method that allows subclasses to easily instantiate an initial value.  ,,,,,,,,,,,,,,,,"16/Sep/08 13:25;jasonrutherglen;LUCENE-1388.patch;https://issues.apache.org/jira/secure/attachment/12390195/LUCENE-1388.patch","15/Sep/08 21:48;jasonrutherglen;LUCENE-1388.patch;https://issues.apache.org/jira/secure/attachment/12390136/LUCENE-1388.patch","15/Sep/08 21:47;jasonrutherglen;LUCENE-1388.patch;https://issues.apache.org/jira/secure/attachment/12390135/LUCENE-1388.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-09-15 21:58:55.21,,,false,,,,,,,,,,,,,,,12363,,,Tue Sep 16 16:48:50 UTC 2008,New,,,,,,,"0|i04xb3:",26661,,,,,,,,,"15/Sep/08 21:47;jasonrutherglen;LUCENE-1388.patch

Added the initialValue method, and a test case.","15/Sep/08 21:48;jasonrutherglen;Wrong test case in previous patch.","15/Sep/08 21:58;mikemccand;Normally initialValue is called from get() the first time get() is invoked (if set wasn't called).  But this patch calls it in the ctor, in which case I think we should simply pass in the initial value as an arg to the ctor?","15/Sep/08 22:01;rengels@ix.netcom.com;The code is incorrect.

It needs to be in the get method, as each thread needs to have initialValue called on the first get().","16/Sep/08 13:25;jasonrutherglen;LUCENE-1388.patch

Initvalue is summoned in the get method.  Includes a extraordinarily simple test case.","16/Sep/08 14:20;mikemccand;Patch looks good; I'll commit shortly.  Thanks Jason!","16/Sep/08 14:24;mikemccand;Sending        src/java/org/apache/lucene/util/CloseableThreadLocal.java
Adding         src/test/org/apache/lucene/index/TestCloseableThreadLocal.java
Transmitting file data ..
Committed revision 695899.
","16/Sep/08 14:37;jasonrutherglen;Should the null check in the get method by synchronized?  Or does it matter?","16/Sep/08 16:48;mikemccand;bq. Should the null check in the get method by synchronized?

I don't think that's necessary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sometimes if a BG merge hits an exception, optimize() will fail to forward the exception",LUCENE-1376,12403669,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,04/Sep/08 10:10,11/Oct/08 12:49,30/Sep/19 08:38,04/Sep/08 11:14,2.3,,,,,,,,,,2.4,,,,core/index,,,0,,,,"I was seeing an intermittant failure, only on a Windows instance running inside VMWare, of TestIndexWriter.testAddIndexOnDiskFull.

It is happening because the while loop that checks for merge exceptions that had occurred during optimize fails to catch the case where all the BG optimize merges completed (or hit exceptions) before the while loop begins.  IE, all BG threads finished before the FG thread advanced to the while loop.  In that case the code fails to check if there were any exceptions.

The fix is straightforward: change the while loop so that it always checks, at least once, whether there were exceptions.",,,,,,,,,,,,,,,,"04/Sep/08 10:11;mikemccand;LUCENE-1376.patch;https://issues.apache.org/jira/secure/attachment/12389489/LUCENE-1376.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12375,,,Thu Sep 04 11:14:23 UTC 2008,New,,,,,,,"0|i04xdr:",26673,,,,,,,,,"04/Sep/08 10:11;mikemccand;Attached patch.  I will commit shortly.","04/Sep/08 11:14;mikemccand;Committed revision 691964.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add IndexCommit.getTimestamp method,LUCENE-1375,12403665,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,04/Sep/08 09:35,11/Oct/08 12:49,30/Sep/19 08:38,04/Sep/08 09:40,2.4,,,,,,,,,,2.4,,,,core/index,,,0,,,,Convenience method for getDirectory().fileModified(getSegmentsFileName()).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12376,,,Thu Sep 04 09:40:44 UTC 2008,New,,,,,,,"0|i04xdz:",26674,,,,,,,,,"04/Sep/08 09:38;mikemccand;Changes are trivial; I plan to commit shortly.","04/Sep/08 09:40;mikemccand;Committed revision 691952.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
replace Vector with ArrayList in Queries,LUCENE-1346,12400941,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,yseeley@gmail.com,yseeley@gmail.com,24/Jul/08 18:33,11/Oct/08 12:49,30/Sep/19 08:38,28/Jul/08 13:59,,,,,,,,,,,2.4,,,,,,,0,,,,Replace Vector with ArrayList in Queries.  This can make a difference in heavily concurrent scenarios when Query objects are examined or compared (e.g. used as cache keys).,,,,,,,,,,,,,,,,"24/Jul/08 18:36;yseeley@gmail.com;LUCENE-1346.patch;https://issues.apache.org/jira/secure/attachment/12386817/LUCENE-1346.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12402,,,Mon Jul 28 13:59:16 UTC 2008,New,,,,,,,"0|i04xjr:",26700,,,,,,,,,"24/Jul/08 18:36;yseeley@gmail.com;Attaching patch that changes Vector to ArrayList in PhraseQuery, MultiPhraseQuery, and in BooleanQuery.Weight.

I doubt there was ever any intent for these query classes to be partially thread safe.","28/Jul/08 13:59;yseeley@gmail.com;committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Term improvement,LUCENE-1334,12400128,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,dmsmith,dmsmith,11/Jul/08 17:25,11/Oct/08 12:49,30/Sep/19 08:38,19/Aug/08 10:40,2.3.1,,,,,,,,,,2.4,,,,core/index,,,0,,,,"Term is designed for reuse of the supplied filter, to minimize intern().

One of the common use patterns is to create a Term with the txt field being an empty string.

To simplify this pattern and to document it's usefulness, I suggest adding a constructor:
public Term(String fld)
with the obvious implementation
and use it throughout core and contrib as a replacement.

",all,,,,,,,,,,,,,,,"11/Jul/08 20:43;dmsmith;LUCENE-1334.txt;https://issues.apache.org/jira/secure/attachment/12385901/LUCENE-1334.txt",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-08-19 10:36:47.895,,,false,,,,,,,,,,,,,,,12414,,,Tue Aug 19 10:40:47 UTC 2008,New,,,,,,,"0|i04xmf:",26712,,,,,,,,,"11/Jul/08 20:43;dmsmith;patch for the issue","19/Aug/08 10:36;mikemccand;Patch looks good, thanks DM!  I'll commit shortly.","19/Aug/08 10:40;mikemccand;Thanks DM!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove synchronization in SegmentReader.isDeleted,LUCENE-1329,12399721,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,jasonrutherglen,jasonrutherglen,07/Jul/08 13:12,11/Oct/08 12:49,30/Sep/19 08:38,23/Aug/08 13:47,2.3.1,,,,,,,,,,2.4,,,,core/index,,,0,,,,Removes SegmentReader.isDeleted synchronization by using a volatile deletedDocs variable on Java 1.5 platforms.  On Java 1.4 platforms synchronization is limited to obtaining the deletedDocs reference.,,,,,,,,,,,,,,,,"19/Aug/08 13:06;mikemccand;LUCENE-1329.patch;https://issues.apache.org/jira/secure/attachment/12388522/LUCENE-1329.patch","08/Aug/08 19:27;mikemccand;LUCENE-1329.patch;https://issues.apache.org/jira/secure/attachment/12387845/LUCENE-1329.patch","07/Jul/08 13:13;jasonrutherglen;lucene-1329.patch;https://issues.apache.org/jira/secure/attachment/12385397/lucene-1329.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-07-08 10:30:20.618,,,false,,,,,,,,,,,,,,,12419,,,Sat Aug 23 13:47:41 UTC 2008,New,Patch Available,,,,,,"0|i04xnj:",26717,,,,,,,,,"07/Jul/08 13:13;jasonrutherglen;lucene-1329.patch
","08/Jul/08 10:30;mikemccand;Jason, have you run any scale tests (w/ many threads) to confirm whether volatile is faster than using synchronized, for this case?  I'm especially curious on what happens with JRE 1.4, since with this patch it is now synchronized and volatile.

I think we should, at least in addition but perhaps instead, create a way to open a read-only IndexReader.  This way no synchronization nor volatile would be necessary when accessing deletedDocs.","08/Jul/08 13:20;sanne.grinovero;Adding a readonly IndexReader would be really great, I'm contributing some code to Hibernate Search (integration of Lucene and Hibernate) and that
project could really benefit from that.","08/Jul/08 14:20;yseeley@gmail.com;bq. I think we should, at least in addition but perhaps instead, create a way to open a read-only IndexReader. 

Right... a volatile is still ""half"" a synchronized in many ways, and gets more expensive as you add more cores.  IAFAIK It's also something you won't see with a profiler because it involves cache flushes, not explicit high level blocking.
","08/Jul/08 14:30;yseeley@gmail.com;Once we have a read-only IndexReader, if we still want the deleting-reader then we could  also weaken the semantics of deleteDocument such that applications would need to synchronize themselves to guarantee visibility to another thread.

We could safely do this for a deleting-reader by pre-allocating the BitVector objects, thus eliminating the possibility of a thread seeing a partially constructed object.","08/Aug/08 19:27;mikemccand;I took a first cut at creating an explicit read only IndexReader
(attached), which is an alternative to the first patch here.

I added ""boolean readOnly"" to 3 of the IndexReader open methods, and
created ReadOnlySegmentReader and ReadOnlyMultiSegmentReader.  The
classes are trivial -- they subclass the parent and just override
acquireWriteLock (to throw an exception) and isDeleted.

reopen() also preserves readOnly-ness, and I fixed merging to open readOnly
IndexReaders.

bq. We could safely do this for a deleting-reader by pre-allocating the BitVector objects, thus eliminating the possibility of a thread seeing a partially constructed object.

I didn't do this one yet ... it makes me a bit nervous because it
means that people who just do IndexReader.open on an index with no
deletions pay the RAM cost upfront of allocating the BitVector.

Really, I think we want to default IndexReader.open to be
readOnly=true (which we can't do until 3.0) at which point doing the
above seems safer since you'd have to go out of your way to open a
non-readOnly IndexReader.
","08/Aug/08 19:37;yseeley@gmail.com;bq. I didn't do this one yet ... it makes me a bit nervous because it means that people who just do IndexReader.open on an index with no deletions pay the RAM cost upfront of allocating the BitVector.

Right, which is why I said it was for a ""deleting-reader"" (which presumes the existence of read-only-readers).
","19/Aug/08 10:14;mikemccand;I'd like to get this (read-only IndexReader) into 2.4.","19/Aug/08 13:06;mikemccand;Attached new rev of this patch.  I think it's ready to commit.  I'll wait a few days.

Changes:

  * Updated javadocs.

  * Stated clearly that in 3.0 the default for readOnly will switch from false to true.

  * Factored out IndexReader.READ_ONLY_DEFAULT so we have one clear place to change from false to true, in 3.0.","22/Aug/08 09:31;eksdev;Mike, did someone measure what this brings? 

This practically reduces need to have many IndexReader-s in MT setup when Index is used in read only case.




","22/Aug/08 09:55;mikemccand;bq. Mike, did someone measure what this brings? 

I don't think so -- I haven't yet tested how much of a bottleneck this was / how much it helps that isDeleted is no longer synchronized.

bq. This practically reduces need to have many IndexReader-s in MT setup when Index is used in read only case.

I *really* want to get Lucene to this point, but I fear LUCENE-753 may still stand in the way since many threads can pile up when accessing the same file.  Sadly, an optimized index exacerbates the situation (the polar opposite of what you'd expect when optimizing an index).  On every platform except Windows, this patch combined with NIOFSDirectory ought to solve all known search-time bottlenecks.","22/Aug/08 11:40;eksdev;ok, I see, thanks. 
At least, It resolves an issue completely for RAM based indexes.

We have seen performance drop for RAM based index when we switched to MT setup with shared IndexReader, I am not yet sure what is the reason for it,  problems in our code or this is indeed related to lucene. I am talking about 25-30% drop on 3 Threads on 4-Core CPU.  Must measure it properly...

","23/Aug/08 13:47;mikemccand;I just committed this.  Thanks Jason!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add IndexCommit.isOptimized method,LUCENE-1325,12399443,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,02/Jul/08 12:36,11/Oct/08 12:49,30/Sep/19 08:38,04/Jul/08 09:43,2.1,2.2,2.3,2.3.1,,,,,,,2.4,,,,core/index,,,0,,,,"Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200807.mbox/%3C69de18140807010347s6269fea5r12c3212e0ec0a12a@mail.gmail.com%3E",,,,,,,,,,,,,,,,"02/Jul/08 17:37;mikemccand;LUCENE-1325.patch;https://issues.apache.org/jira/secure/attachment/12385131/LUCENE-1325.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12423,,,Wed Jul 02 17:37:48 UTC 2008,New,,,,,,,"0|i04xof:",26721,,,,,,,,,"02/Jul/08 17:37;mikemccand;Attached patch.  I plan to commit in a day or so.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultiReader should make a private copy of the subReaders array,LUCENE-1323,12399360,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,01/Jul/08 12:31,11/Oct/08 12:49,30/Sep/19 08:38,01/Jul/08 15:31,1.9,2.0.0,2.1,2.2,2.3,2.3.1,2.3.2,2.4,2.9,3.0,2.4,,,,core/index,,,0,,,,"Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200806.mbox/%3C88F3F6A4-FBFB-43DF-890D-DB5F0D9A2461@gmail.com%3E

Because MultiReader just holds a reference to the array that was passed in, it's possible to hit scary exceptions (that look like index corruption) if that array is later altered eg by reopening some of the readers.

The fix is trivial: just make a private copy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12425,,,Tue Jul 01 15:31:36 UTC 2008,New,,,,,,,"0|i04xov:",26723,,,,,,,,,"01/Jul/08 15:31;mikemccand;Thanks Sascha!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove synchronization in CompoundFileReader,LUCENE-1322,12399273,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,jasonrutherglen,jasonrutherglen,30/Jun/08 13:10,11/Oct/08 12:49,30/Sep/19 08:38,02/Jul/08 11:57,2.3.1,,,,,,,,,,2.4,,,,core/index,,,0,,,,"Currently there is what seems to be unnecessary synchronization in CompoundFileReader.  This is solved by cloning the base IndexInput.  Synchronization in low level IO classes creates lock contention on highly multi threaded Lucene installations, so much so that in many cases the CPU utilization never reaches the maximum without using something like ParallelMultiSearcher.",,,,,,,,,,,,,,,,"01/Jul/08 20:57;jasonrutherglen;lucene-1322.patch;https://issues.apache.org/jira/secure/attachment/12385052/lucene-1322.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-06-30 13:58:38.162,,,false,,,,,,,,,,,,,,,12426,,,Wed Jul 02 11:57:58 UTC 2008,New,,,,,,,"0|i04xp3:",26724,,,,,,,,,"30/Jun/08 13:58;rengels@ix.netcom.com;This comment is very misleading.

Since there is IO involved, any operation can block, which will lower CPU utilization.

You just need to make sure that the locks control different physical resources.","01/Jul/08 20:57;jasonrutherglen;lucene-1322.patch

CSIndexInput.clone() added that clones the base IndexInput removing the synchronization overhead from CompoundFileReader.","02/Jul/08 11:57;mikemccand;Thanks Jason!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jar manifest should not contain ${user.name} of the person building,LUCENE-1294,12397009,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,gsingers,gsingers,gsingers,28/May/08 17:47,11/Oct/08 12:49,30/Sep/19 08:38,30/May/08 11:11,,,,,,,,,,,2.4,,,,,,,0,,,,"Not sure if it is a big deal, but I don't particularly like that my user id for my build machine is in the manifest of the JAR that I constructed.  It's a stretch, security-wise, I know, but I don't see how it serves any useful purpose.  We have signatures/logs/SVN tags so we know who built the particular item w/o needing to know what their local user account name is.

The fix is:

{code}
Index: common-build.xml
===================================================================
--- common-build.xml    (revision 661027)
+++ common-build.xml    (working copy)
@@ -281,7 +281,7 @@
                <attribute name=""Implementation-Title"" value=""org.apache.lucene""/>
                <!-- impl version can be any string -->
                <attribute name=""Implementation-Version""
-                          value=""${version} ${svnversion} - ${user.name} - ${DSTAMP} ${TSTAMP}""/>
+                          value=""${version} ${svnversion} - ${DSTAMP} ${TSTAMP}""/>
                <attribute name=""Implementation-Vendor""
                           value=""The Apache Software Foundation""/>
                <attribute name=""X-Compile-Source-JDK"" 
{code} ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-05-31 03:12:22.912,,,false,,,,,,,,,,,,,,,12454,,,Sat May 31 03:32:29 UTC 2008,Patch Available,,,,,,,"0|i04xvb:",26752,,,,,,,,,"31/May/08 03:12;hossman;FWIW: I included it originally because i've definitely seen value in the past when you can look at an artifact and ask ""who built this? where did it come from?"" ... maybe not so much for official releases but i've been in enough positions where organizations build their own artifacts from source and later you want to know who built it so you can double check with them how they did it (because you think they screwed something up)

I believe maven actually forcibly puts the build user in every manifest file, so it's not like we were in bleeding edge territory or anything.","31/May/08 03:32;cutting;I too always felt this a feature, albeit a minor one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove System.out left in SpanHighlighter code,LUCENE-1277,12395065,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,markrmiller@gmail.com,markrmiller@gmail.com,30/Apr/08 11:25,11/Oct/08 12:49,30/Sep/19 08:38,01/May/08 18:55,2.4,,,,,,,,,,2.4,,,,modules/highlighter,,,0,,,,A System.out debug was left in the code when a Query is not supported by the SpanHighlighter. This issue simply removes it.,,,,,,,,,,,,,,,,"30/Apr/08 11:26;markrmiller@gmail.com;SpanHighlighter-RemovSysOut.patch;https://issues.apache.org/jira/secure/attachment/12381178/SpanHighlighter-RemovSysOut.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12471,,,2008-04-30 11:25:28.0,New,Patch Available,,,,,,"0|i04xz3:",26769,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary assert in org.apache.lucene.index.DocumentsWriterThreadState.trimFields(),LUCENE-1247,12392483,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,ddillard,ddillard,27/Mar/08 19:29,11/Oct/08 12:49,30/Sep/19 08:38,28/Mar/08 09:04,2.3.1,2.3.2,,,,,,,,,2.4,,,,core/index,,,0,,,,"In org.apache.lucene.index.DocumentsWriterThreadState.trimFields() is the following code:

      if (fp.lastGen == -1) {
        // This field was not seen since the previous
        // flush, so, free up its resources now

        // Unhash
        final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;
        DocumentsWriterFieldData last = null;
        DocumentsWriterFieldData fp0 = fieldDataHash[hashPos];
        while(fp0 != fp) {
          last = fp0;
          fp0 = fp0.next;
        }
        assert fp0 != null;

The assert at the end is not necessary as fp0 cannot be null.  The first line in the above code guarantees that fp is not null by the time the while loop is hit.  The while loop is exited when fp0 and fp are equal.  Since fp is not null then fp0 cannot be null when the while loop is exited, thus the assert is guaranteed to never occur.

This was detected by FindBugs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-03-28 09:04:44.985,,,false,,,,,,,,,,,,,,,12501,,,Fri Mar 28 09:04:44 UTC 2008,New,,,,,,,"0|i04y5r:",26799,,,,,,,,,"28/Mar/08 09:04;mikemccand;Thanks David, I just removed this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
small speedups to bulk merging,LUCENE-1242,12392088,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,21/Mar/08 22:03,11/Oct/08 12:49,30/Sep/19 08:38,24/Mar/08 18:44,2.3,,,,,,,,,,2.4,,,,core/index,,,0,,,,"The bulk merging code, for stored fields & term vectors, was calling isDeleted twice for each deleted doc.

Patch also changes DocumentsWriter to use IndexWriter.message for its infoStream messages.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12505,,,2008-03-21 22:03:12.0,New,,,,,,,"0|i04y6v:",26804,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
have Hits implement Iterable,LUCENE-872,12368460,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Invalid,,koenhandekyn,koenhandekyn,02/May/07 13:04,15/Aug/08 00:00,30/Sep/19 08:38,15/Aug/08 00:00,2.1,,,,,,,,,,,,,,core/search,,,0,,,,for compatibilty with the enhanced for loop it is required that the Hits class implements the interface Iterable. no further code changes required as the method iterator required from the interface already is present.,java 1.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-05-02 22:54:01.276,,,false,,,,,,,,,,,,,,,12870,,,Fri Aug 15 00:00:32 UTC 2008,New,,,,,,,"0|i050hb:",27175,,,,,,,,,"02/May/07 22:54;dmsmith555;This will need to wait until Java 1.4.2 is no longer our target language.","03/May/07 06:37;markh;I have some reservations about making it even easier for new Lucene users to write inefficient code.

Iterating all the way across Hits is normally a poor way of achieving something in Lucene because of the repeated querying going on in the background.
New users already fall into this trap too often and adding this feature will undoubtedly lure more people into writing bad ""for"" loops.

","15/Aug/08 00:00;markrmiller@gmail.com;Hits is deprecated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add setIndexReader in IndexSearcher ,LUCENE-1315,12398884,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Duplicate,,anthonyu,anthonyu,24/Jun/08 01:24,26/Jul/08 09:50,30/Sep/19 08:38,26/Jul/08 09:50,2.3.2,,,,,,,,,,,,,,core/search,,,0,,,,"Adds a setter for the ""private IndexReader reader"" member in IndexSearcher.  Needed to in order to be able reload the reader underlying a remote searcher.",,,,,,,,,,,,,,,,"11/Jul/08 22:00;anthonyu;LUCENE-1315.patch;https://issues.apache.org/jira/secure/attachment/12385906/LUCENE-1315.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-07-26 09:50:04.86,,,false,,,,,,,,,,,,,,,12433,,,Sat Jul 26 09:50:04 UTC 2008,New,Patch Available,,,,,,"0|i04xqn:",26731,,,,,,,,,"24/Jun/08 01:25;anthonyu;Patch attached.","11/Jul/08 22:00;anthonyu;Renamed the patch file to standard name as per http://wiki.apache.org/lucene-java/HowToContribute.","26/Jul/08 09:50;mikemccand;Dup of LUCENE-1203.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make retrieveTerms(int docNum) public in MoreLikeThis,LUCENE-1295,12397012,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,gsingers,gsingers,gsingers,28/May/08 18:25,02/Jun/08 12:29,30/Sep/19 08:38,02/Jun/08 12:29,,,,,,,,,,,,,,,modules/other,,,0,,,,"It would be useful if 
{code}
private PriorityQueue retrieveTerms(int docNum) throws IOException {
{code}

were public, since it is similar in use to 
{code}
public PriorityQueue retrieveTerms(Reader r) throws IOException {
{code}

It also seems useful to add 
{code}
public String [] retrieveInterestingTerms(int docNum) throws IOException{
{code}
to mirror the one that works on Reader.

",,,,,,,,,,,,,,,,"28/May/08 18:28;gsingers;LUCENE-1295.patch;https://issues.apache.org/jira/secure/attachment/12382952/LUCENE-1295.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-05-29 05:02:35.263,,,false,,,,,,,,,,,,,,,12453,,,Mon Jun 02 12:29:21 UTC 2008,New,,,,,,,"0|i04xv3:",26751,,,,,,,,,"28/May/08 18:28;gsingers;I'll commit in a day or two.","29/May/08 05:02;otis;Perque no.  I see MLT is full of tabs, should you feel like fixing the formating.
","29/May/08 14:31;gsingers;{quote}
Perque no.
{quote}

Huh?

{quote}
I see MLT is full of tabs, should you feel like fixing the formating.
{quote}

Yeah, I noticed that too, and it is quite egregious, but I thought we avoided formatting changes, but I am happy to make an exception here.  ","31/May/08 04:26;otis;I think cosmetic changes are OK if:
* they are not mixed with functional changes
* there are no patches for the cleaned-up class(es) in JIRA

In this case I see only a couple of MLT issues, all of which look like we can take care of them quickly, and then somebody can clean up a little if we feel like it.  Anyhow...
","02/Jun/08 12:29;gsingers;Committed revision 662413.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up old JIRA issues with unknown component,LUCENE-1111,12385350,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Later,,michaelbusch,michaelbusch,30/Dec/07 18:31,24/May/08 22:38,30/Sep/19 08:38,24/May/08 22:38,,,,,,,,,,,2.3,,,,,,,0,,,,"A list of all JIRA issues with unknown component that haven't been updated in 2007:

   *	 LUCENE-749  	 ChainedFilter does not work well in the event of filters in ANDNOT   
   *	LUCENE-712 	Build with GCJ fail 
   *	LUCENE-535 	JEDirectory delete issue 
   *	LUCENE-488 	adding docs with large (binary) fields of 5mb causes OOM regardless of heap size 
   *	LUCENE-474 	High Frequency Terms/Phrases at the Index level 
   *	LUCENE-472 	Some fixes to let gcj 3.4.2 build lucene using ant gcj target 
   *	LUCENE-458 	Merging may create duplicates if the JVM crashes half way through
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12634,,,Sat May 24 22:38:39 UTC 2008,New,,,,,,,"0|i04z07:",26936,,,,,,,,,"24/May/08 22:38;michaelbusch;Reopen these issues or open new issues in then 2.4 release process.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Clean up old JIRA issues in component ""Store""",LUCENE-1110,12385348,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Later,,michaelbusch,michaelbusch,30/Dec/07 18:28,24/May/08 22:38,30/Sep/19 08:38,24/May/08 22:38,,,,,,,,,,,2.3,,,,core/store,,,0,,,,"A list of all JIRA issues in component ""Store"" that haven't been updated in 2007:

   *	 LUCENE-737  	 Provision of encryption/decryption services API to support Field.Store.Encrypted   
   *	LUCENE-674 	Error in FSDirectory if java.io.tmpdir incorrectly specified 
   *	LUCENE-547 	Directory implementation for Applets 
   *	LUCENE-519 	NIO FS implementation to avoid synchronization 
   *	LUCENE-487 	Database as a lucene index target 
   *	LUCENE-484 	Java Content Repository Directory Implementation 
   *	LUCENE-414 	Java NIO patch against Lucene 1.9 
   *	LUCENE-357 	Implementation of Directory to check integrity of index 
   *	LUCENE-283 	[PATCH] MMapDirectory for huge index files 
   *	LUCENE-151 	[PATCH] Clonable RAMDirectory 
   *	LUCENE-149 	[PATCH] URLDirectory implementation 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12635,,,Sat May 24 22:38:39 UTC 2008,New,,,,,,,"0|i04z0f:",26937,,,,,,,,,"24/May/08 22:38;michaelbusch;Reopen these issues or open new issues in then 2.4 release process.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Clean up old JIRA issues in component ""Search""",LUCENE-1109,12385347,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Later,,michaelbusch,michaelbusch,30/Dec/07 18:26,24/May/08 22:38,30/Sep/19 08:38,24/May/08 22:38,,,,,,,,,,,2.3,,,,core/search,,,0,,,,"A list of all JIRA issues in component ""Search"" that haven't been updated in 2007:

   *	 LUCENE-737  	 Provision of encryption/decryption services API to support Field.Store.Encrypted   
   *	LUCENE-711 	BooleanWeight should size the weights Vector correctly 
   *	LUCENE-691 	Bob Carpenter's FuzzyTermEnum refactoring 
   *	LUCENE-671 	Hashtable based Document 
   *	LUCENE-668 	Incremental Search 
   *	LUCENE-663 	New feature rich higlighter for Lucene. 
   *	LUCENE-655 	field queries does not work as expected 
   *	LUCENE-640 	[PATCH] Performance improvement for doNext() of ConjunctionScorer 
   *	LUCENE-628 	Intermittent FileNotFoundException for .fnm when using rsync 
   *	LUCENE-582 	Don't throw TooManyClauses exception 
   *	LUCENE-548 	Sort bug using ParallelMultiSearcher 
   *	LUCENE-538 	Using WildcardQuery with MultiSearcher, and Boolean MUST_NOT clause 
   *	LUCENE-533 	SpanQuery scoring: SpanWeight lacks a recursive traversal of the query tree 
   *	LUCENE-524 	Current implementation of fuzzy and wildcard queries inappropriately implemented as Boolean query rewrites 
   *	LUCENE-522 	SpanFuzzyQuery 
   *	LUCENE-517 	norm compression breaks ranking for small fields 
   *	LUCENE-507 	CLONE -[PATCH] remove unused variables 
   *	LUCENE-504 	FuzzyQuery produces a ""java.lang.NegativeArraySizeException"" in PriorityQueue.initialize if I use Integer.MAX_VALUE as BooleanQuery.MaxClauseCount 
   *	LUCENE-502 	TermScorer caches values unnecessarily 
   *	LUCENE-495 	Suggested Patches to MultiPhraseQuery and QueryTermExtractor (for use with HighLighter) 
   *	LUCENE-476 	BooleanQuery add public method that returns number of clauses this query 
   *	LUCENE-466 	Need QueryParser support for BooleanQuery.minNrShouldMatch 
   *	LUCENE-459 	Warnings needed for sorting on non existant or non indexed fields 
   *	LUCENE-445 	Contrib.: Thread-safe DelayCloseIndexSearcher. 
   *	LUCENE-434 	Lucene database bindings 
   *	LUCENE-427 	PrefixQuery.extractTerms() missing 
   *	LUCENE-423 	thread pool implementation of parallel queries 
   *	LUCENE-421 	Numeric range searching with large value sets 
   *	LUCENE-420 	[PATCH] Contribution: Did you mean... 
   *	LUCENE-411 	[PATCH] BitSetQuery, FastPrefixQuery, FastWildcardQuery and FastQueryParser 
   *	LUCENE-406 	sort missing string fields last 
   *	LUCENE-379 	Contribution: Efficient Sorting of DateField/DateTools Encoded Timestamp Long Values 
   *	LUCENE-329 	Fuzzy query scoring issues 
   *	LUCENE-320 	[PATCH] Increases visibility of methods/classes from protected/package level to public 
   *	LUCENE-314 	[PATCH] Filter for one/ multiple refinements search: filter the search with anterior hits 
   *	LUCENE-308 	add ""isClosed()"" method to IndexSearcher 
   *	LUCENE-271 	Query.toString() and Query.toString(String field) not escaping special characters 
   *	LUCENE-124 	Fuzzy Searches do not get a boost of 0.2 as stated in ""Query Syntax"" doc 
   *	LUCENE-42 	[PATCH] Phonetic Search capability 
   *	LUCENE-38 	RangeQuery without lower term and inclusive=false skips blank fields 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12636,,,Sat May 24 22:38:39 UTC 2008,New,,,,,,,"0|i04z0n:",26938,,,,,,,,,"24/May/08 22:38;michaelbusch;Reopen these issues or open new issues in then 2.4 release process.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Clean up old JIRA issues in component ""QueryParser""",LUCENE-1108,12385346,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Later,,michaelbusch,michaelbusch,30/Dec/07 18:24,24/May/08 22:38,30/Sep/19 08:38,24/May/08 22:38,,,,,,,,,,,2.3,,,,core/queryparser,,,0,,,,"A list of all JIRA issues in component ""QueryParser"" that haven't been updated in 2007:

   *	 LUCENE-682  	 QueryParser with Locale Based Operators (French included)   
   *	LUCENE-553 	MultiFieldDisjunctionMaxQueryParser 
   *	LUCENE-375 	fish*~ parses to PrefixQuery - should be a parse exception 
   *	LUCENE-373 	Query parts ending with a colon are handled badly 
   *	LUCENE-278 	Fuzziness for date searches 
   *	LUCENE-218 	Query Parser flags clauses with explicit OR as required when followed by explicit AND. 
   *	LUCENE-182 	[PATCH] SmartDateFormat for QueryParser 
   *	LUCENE-83 	ESCAPING BUG \(abc\) and \(a*c\) in v1.2 
   *	LUCENE-79 	QueryParser: time not supported in date ranges 
   *	LUCENE-72 	[PATCH] Query parser inconsistency when using terms to exclude. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12637,,,Sat May 24 22:38:39 UTC 2008,New,,,,,,,"0|i04z0v:",26939,,,,,,,,,"24/May/08 22:38;michaelbusch;Reopen these issues or open new issues in then 2.4 release process.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Clean up old JIRA issues in component ""Index""",LUCENE-1106,12385344,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Later,,michaelbusch,michaelbusch,30/Dec/07 18:19,24/May/08 22:38,30/Sep/19 08:38,24/May/08 22:38,,,,,,,,,,,2.3,,,,core/index,,,0,,,,"A list of all JIRA issues in component ""Index"" that haven't been updated in 2007:

   *	LUCENE-737  	 Provision of encryption/decryption services API to support Field.Store.Encrypted   
   *	LUCENE-705 	CompoundFileWriter should pre-set its file length 
   *	LUCENE-685 	Extract interface from IndexWriter 
   *	LUCENE-671 	Hashtable based Document 
   *	LUCENE-652 	Compressed fields should be ""externalized"" (from Fields into Document) 
   *	LUCENE-639 	[PATCH] Slight performance improvement for readVInt() of IndexInput 
   *	LUCENE-606 	Change behavior of ParallelReader.document(int) 
   *	LUCENE-602 	[PATCH] Filtering tokens for position and term vector storage 
   *	LUCENE-600 	ParallelWriter companion to ParallelReader 
   *	LUCENE-570 	Expose directory on IndexReader 
   *	LUCENE-552 	NPE during mergeSegments 
   *	LUCENE-532 	[PATCH] Indexing on Hadoop distributed file system 
   *	LUCENE-518 	document field lengths count analyzer synonym overlays 
   *	LUCENE-517 	norm compression breaks ranking for small fields 
   *	LUCENE-508 	SegmentTermEnum.next() doesn't maintain prevBuffer at end 
   *	LUCENE-506 	Optimize Memory Use for Short-Lived Indexes (Do not load TermInfoIndex if you know the queries ahead of time) 
   *	LUCENE-505 	MultiReader.norm() takes up too much memory: norms byte[] should be made into an Object 
   *	LUCENE-402 	addition of a previous() method to TermEnum 
   *	LUCENE-401 	[PATCH] fixes for gcj target. 
   *	LUCENE-382 	[PATCH] Document update contrib (Play with term postings or .. to a easy way to update) 
   *	LUCENE-362 	[PATCH] Extension to binary Fields that allows fixed byte buffer 
   *	LUCENE-336 	[PATCH] Add ability to specify the segment name when optimizing an index 
   *	LUCENE-325 	[PATCH] new method expungeDeleted() added to IndexWriter 
   *	LUCENE-211 	[Patch] replace DocumentWriter with InvertedDocument for performance 
   *	LUCENE-112 	[PATCH] Add an IndexReader implementation that frees resources when idle and refreshes itself when stale 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-01-13 15:20:49.829,,,false,,,,,,,,,,,,,,,12639,,,Sat May 24 22:38:39 UTC 2008,New,,,,,,,"0|i04z1b:",26941,,,,,,,,,"13/Jan/08 15:20;gsingers;I've gone through LUCENE-602 above.

I think 602 can be marked ""won't fix"", but will wait to hear from Chuck on it.  
","24/May/08 22:38;michaelbusch;Reopen these issues or open new issues in then 2.4 release process.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Clean up old JIRA issues in component ""Examples""",LUCENE-1105,12385343,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Later,,michaelbusch,michaelbusch,30/Dec/07 18:16,24/May/08 22:38,30/Sep/19 08:38,24/May/08 22:38,,,,,,,,,,,2.3,,,,modules/examples,,,0,,,,"A list of all JIRA issues in component ""Examples"" that haven't been updated in 2007:

   *	 LUCENE-591  	 Add meta keywords to HTMLParser   
   *	LUCENE-590 	Demo HTML parser gives incorrect summaries when title is repeated as a heading 
   *	LUCENE-589 	Demo HTML parser doesn't work for international documents 
   *	LUCENE-259 	HTML Parser doesn't decode character references in attributes 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12640,,,Sat May 24 22:38:39 UTC 2008,New,,,,,,,"0|i04z1j:",26942,,,,,,,,,"24/May/08 22:38;michaelbusch;Reopen these issues or open new issues in then 2.4 release process.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Correct 2 minor javadoc mistakes in core, javadoc.access=private",LUCENE-1060,12382811,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,otis,paul.elschot@xs4all.nl,paul.elschot@xs4all.nl,19/Nov/07 18:35,17/May/08 01:48,30/Sep/19 08:38,17/May/08 01:48,,,,,,,,,,,,,,,general/javadocs,,,0,,,,Patches Token.java and TermVectorsReader.java,,,,,,,,,,,,,,,,"19/Nov/07 18:36;paul.elschot@xs4all.nl;javadocs20071119.patch;https://issues.apache.org/jira/secure/attachment/12369803/javadocs20071119.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-05-17 01:48:31.134,,,false,,,,,,,,,,,,,,,12685,,,Sat May 17 01:48:31 UTC 2008,New,Patch Available,,,,,,"0|i04zbj:",26987,,,,,,,,,"17/May/08 01:48;otis;Thank you Paul.

Committed revision 657278.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SpellChecker not working because of stale IndexSearcher,LUCENE-865,12367645,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,otis,raikoe,raikoe,20/Apr/07 08:18,14/May/08 06:16,30/Sep/19 08:38,14/May/08 06:16,2.1,,,,,,,,,,,,,,core/search,,,0,,,,"The SpellChecker unit test did not work, because of a stale IndexReader and IndexSearcher instance after calling indexDictionary(Dictionary).",,,,,,,,,,,,,,,,"20/Apr/07 08:20;raikoe;LUCENE-865 SpellChecker.patch;https://issues.apache.org/jira/secure/attachment/12355891/LUCENE-865+SpellChecker.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-05-14 06:16:50.906,,,false,,,,,,,,,,,,,,,12877,,,Wed May 14 06:16:50 UTC 2008,New,Patch Available,,,,,,"0|i050iv:",27182,,,,,,,,,"20/Apr/07 08:20;raikoe;Added Patch","14/May/08 06:16;otis;This was already fixed in SpellChecker.java - I can see the same type of code is there in the current version of SpellChecker.java.  Also, all spellchecker unit tests pass now.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
addIndexesNoOptimize should not enforce maxMergeDocs/maxMergeSize limit,LUCENE-1254,12392717,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,31/Mar/08 09:21,08/May/08 19:47,30/Sep/19 08:38,01/Apr/08 06:59,2.1,2.2,2.3,,,,,,,,2.3.2,2.4,,,core/index,,,0,,,,"If you pass an index that has a segment > maxMergeDocs or maxMergeSize
to addIndexesNoOptimize, it throws an IllegalArgumentException.

But this check isn't reasonable because segment merging can easily
produce segments over these sizes since those limits apply to each
segment being merged, not to the final size of the segment produced.

So if you set maxMergeDocs to X, build up and index, then try to add
that index to another index that also has maxMergeDocs X, you can
easily hit the exception.

I think it's being too pedantic; I plan to just remove the checks for
sizes.",,,,,,,,,,,,,,,,"31/Mar/08 09:37;mikemccand;LUCENE-1254.patch;https://issues.apache.org/jira/secure/attachment/12378937/LUCENE-1254.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12494,,,Mon Mar 31 09:37:10 UTC 2008,New,,,,,,,"0|i04y47:",26792,,,,,,,,,"31/Mar/08 09:37;mikemccand;Attached patch.  I plan to commit in a day or two and backport to 2.3.2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standard tokenizer with punctuation output,LUCENE-889,12370256,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,karl.wettin,karl.wettin,25/May/07 10:37,12/Apr/08 18:17,30/Sep/19 08:38,12/Apr/08 18:17,2.1,,,,,,,,,,,,,,,,,0,,,,"This patch adds punctuation (comma, period, question mark and exclamation point)  tokens as output from the StandardTokenizer, and filters them out in the StandardFilter.

(I needed them for text classification reasons.)",,,,,,,,,,,,,,,,"25/May/07 10:39;karl.wettin;standard.patch;https://issues.apache.org/jira/secure/attachment/12358216/standard.patch","25/May/07 10:39;karl.wettin;test.patch;https://issues.apache.org/jira/secure/attachment/12358217/test.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2007-05-25 13:57:08.979,,,false,,,,,,,,,,,,,,,12853,,,Sat Apr 12 18:17:46 UTC 2008,New,Patch Available,,,,,,"0|i050dj:",27158,,,,,,,,,"25/May/07 10:39;karl.wettin;standard.patch is root src/java/org/apache/lucene/analysis
test.patch is root src/test/org/apache/lucene/analysis

I'm sorry about the non-trunk patch. My local copy of Lucene is a bit messed up.","25/May/07 13:57;ehatcher;This patch concerns me.  This changes default behavior in a very basic and commonly used piece of Lucene.  At the very least this should be made entirely optional and off by default.  

Thoughts?","25/May/07 15:30;karl.wettin;Erik Hatcher [25/May/07 06:57 AM]
> This patch concerns me. This changes default behavior
> in a very basic and commonly used piece of Lucene. At 
> the very least this should be made entirely optional and
> off by default.
>
> Thoughts? 

It is off by default. The punctuation comes out from the tokenizer, but the StandardAnalyzer uses a StandardFilter, and the StandardFilter will filter out the punctuation tokens. In order to get the punctuation, one needs to use a plain StandardTokenizer.","25/May/07 18:14;hossman;> In order to get the punctuation, one needs to use a plain StandardTokenizer.

I believe that is Erik's point. StandardTokenizer is a public class that many people use directly (specifically: every one who has ever posted a question about changing the behavior of StandardAnalyzer and been given the stock answer ""write your own Analyzer that uses the same Tokenizer and change/adds the list of TokenFilters.","25/May/07 19:50;karl.wettin;Hoss Man [25/May/07 11:14 AM]
> > In order to get the punctuation, one needs to use a plain StandardTokenizer.
>
> I believe that is Erik's point. StandardTokenizer is a public class that many
> people use directly (specifically: every one who has ever posted a question
> about changing the behavior of StandardAnalyzer and been given the stock
> answer ""write your own Analyzer that uses the same Tokenizer and
> change/adds the list of TokenFilters.

Aha. My JavaCC-skills aren't that great. I'll look in to it.

I presume something like 

    isTokenizingPuctuation() && token = <PUNCTUATION> |

is possible.
","12/Apr/08 18:17;karl.wettin;artifact",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EdgeNGram* documentation improvement,LUCENE-1236,12391523,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,gsingers,kawai,kawai,15/Mar/08 06:21,15/Mar/08 18:04,30/Sep/19 08:38,15/Mar/08 18:04,,,,,,,,,,,,,,,modules/other,,,0,,,,"To clarify what ""edge"" means, I added some description. That edge means the beggining edge of a term or ending edge of a term.",,,,,,,,,,,,,,,,"15/Mar/08 06:21;kawai;EdgeNGram.patch;https://issues.apache.org/jira/secure/attachment/12377967/EdgeNGram.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-03-15 18:02:14.479,,,false,,,,,,,,,,,,,,,12511,,,Sat Mar 15 18:04:21 UTC 2008,New,Patch Available,,,,,,"0|i04y8f:",26811,,,,,,,,,"15/Mar/08 18:02;gsingers;Hi Hiroaki,

Thanks for the patch.  I will apply the doc changes, but please don't combine other functionality into a patch (my guess is you still had some of the other NGram patches applied)

Thanks,
Grant","15/Mar/08 18:04;gsingers;committed, minus the >1024 clause.  Also removed existing author tags.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible hidden exception on SegmentInfos commit,LUCENE-1214,12390653,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,markrmiller@gmail.com,markrmiller@gmail.com,10/Mar/08 12:42,12/Mar/08 19:22,30/Sep/19 08:38,12/Mar/08 19:22,2.3.1,,,,,,,,,,,,,,core/index,,,0,,,,"I am not sure if this is that big of a deal, but I just ran into it and thought I might mention it.

SegmentInfos.commit removes the Segments File if it hits an exception. If it cannot remove the Segments file (because its not there or on Windows something has a hold of it), another Exception is thrown about not being able to delete the Segments file. Because of this, you lose the first exception, which might have useful info, including why the segments file might not be there to delete.

- Mark",,,,,,,,,,,,,,,,"10/Mar/08 16:46;mikemccand;LUCENE-1214.patch;https://issues.apache.org/jira/secure/attachment/12377534/LUCENE-1214.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-03-10 13:06:49.654,,,false,,,,,,,,,,,,,,,12532,,,Mon Mar 10 21:56:59 UTC 2008,New,,,,,,,"0|i04ydb:",26833,,,,,,,,,"10/Mar/08 13:06;mikemccand;Good catch Mark.  It seems like we should ignore any exception while trying to delete the partially written segments_N file, and throw the original exception.  I'll do that.

How did you hit these two exceptions?","10/Mar/08 16:46;mikemccand;Attached patch.  All tests pass.  I plan to commit in a day or so.","10/Mar/08 21:56;markrmiller@gmail.com;I am still trying to work that out...some craziness that started after I updated Lucene to trunk, but also made other fundamental changes, and windows vista may be haunting me too...

The gist of it is that Lucene is failing when it tries to create an index file (creates the directory fine). I don't think its Lucene related at the moment, but I havnt gotten to the bottom of it either.

Oddly, if I stop using the NoLockFactory (I manually manage a single Writer), things work...still digging though.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
use isBinary cached variable instead of instanceof in Field,LUCENE-1217,12390735,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,eksdev,eksdev,11/Mar/08 09:23,12/Mar/08 16:57,30/Sep/19 08:38,12/Mar/08 10:10,,,,,,,,,,,,,,,core/other,,,0,,,,"Field class can hold three types of values, 
See: AbstractField.java  protected Object fieldsData = null; 

currently, mainly RTTI (instanceof) is used to determine the type of the value stored in particular instance of the Field, but for binary value we have mixed RTTI and cached variable ""boolean isBinary"" 

This patch makes consistent use of cached variable isBinary.

Benefit: consistent usage of method to determine run-time type for binary case  (reduces chance to get out of sync on cached variable). It should be slightly faster as well.

Thinking aloud: 
Would it not make sense to maintain type with some integer/byte""poor man's enum"" (Interface with a couple of constants)
code:java{
public static final interface Type{
public static final byte BOOLEAN = 0;
public static final byte STRING = 1;
public static final byte READER = 2;
....
}
}

and use that instead of isBinary + instanceof? ",,,,,,,,,,LUCENE-1219,,,,,,"11/Mar/08 09:25;eksdev;LUCENE-1217.patch;https://issues.apache.org/jira/secure/attachment/12377589/LUCENE-1217.patch","11/Mar/08 21:09;eksdev;Lucene-1217-take1.patch;https://issues.apache.org/jira/secure/attachment/12377639/Lucene-1217-take1.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2008-03-11 18:26:37.181,,,false,,,,,,,,,,,,,,,12529,,,Wed Mar 12 16:57:16 UTC 2008,New,Patch Available,,,,,,"0|i04ycn:",26830,,,,,,,,,"11/Mar/08 18:26;mikemccand;Patch looks good.  I will commit shortly.  Thanks Eks Dev.

{quote}
Would it not make sense to maintain type with some integer/byte""poor man's enum"" (Interface with a couple of constants)
{quote}

Or we could wait until Java 5 (3.0) and use real enums?

Or ... maybe we should have subclasses of Field (TextField, BinaryField,
ReaderField, TokenStreamField) which override the corresponding method
(and the base Field.java would still implement these methods but
return null)?  Though this would be a rather large change...","11/Mar/08 20:35;eksdev;thanks fof looking into it!
Subclassing now with backwards compatibility would be clumsy, I was thinking about it but could not find clean way to make it.

>>Or we could wait until Java 5 (3.0) and use real enums?
yes, that is ultimate solution, but my line of thoughts was that ""poor man's enum""->java 5 enum migration would be trivial later... but do not change working code kicks-in here :)  ","11/Mar/08 20:48;mikemccand;Actually seeing a test failure with this:

    [junit] Testcase: testLazyFields(org.apache.lucene.index.TestFieldsReader): FAILED
    [junit] bytes is null and it shouldn't be
    [junit] junit.framework.AssertionFailedError: bytes is null and it shouldn't be
    [junit]     at org.apache.lucene.index.TestFieldsReader.testLazyFields(TestFieldsReader.java:132)

","11/Mar/08 21:08;eksdev;hah, this bug just  justified this patch :) 
sorry,  I should have run tests before... nothing is trivial enough.   
 The problem was indeed isBinary that went out of sync in LazyField, new patch follows ","11/Mar/08 21:09;eksdev;new patch, fixes isBinary status in LazyField","11/Mar/08 22:49;mikemccand;OK the new patch passes all tests -- thanks!

One unrelated thing I noticed: it looks like you can get a binary LazyField and then ask for its stringValue(), and vice-versa.  Ie we are failing to check in binaryValue() that the field is in fact binary even though when we create the LazyField we know whether it is.  I'll open a separate issue for this.","12/Mar/08 16:57;cutting;fix typo that's been bugging me",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If setConfig(Config config) is called in resetInputs(), you can turn term vectors off and on by round",LUCENE-1209,12390517,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,doronc,markrmiller@gmail.com,markrmiller@gmail.com,08/Mar/08 02:21,09/Mar/08 16:56,30/Sep/19 08:38,09/Mar/08 16:56,2.4,,,,,,,,,,,,,,modules/benchmark,,,0,,,,"I want to be able to run one benchmark that tests things using term vectors and not using term vectors.

Currently this is not easy because you cannot specify term vectors per round.

While you do have to create a new index per round, this automation is preferable to me in comparison to running two separate tests.

If it doesn't affect anything else, it would be great to have setConfig(Config config) called in BasicDocMaker.resetInputs(). This would keep the term vector options up to date per round if you reset.

- Mark",,,,,,,,,,,,,,,,"09/Mar/08 16:31;doronc;reset_config.patch;https://issues.apache.org/jira/secure/attachment/12377484/reset_config.patch","09/Mar/08 15:24;doronc;reset_config.patch;https://issues.apache.org/jira/secure/attachment/12377482/reset_config.patch","08/Mar/08 02:21;markrmiller@gmail.com;reset_config.patch;https://issues.apache.org/jira/secure/attachment/12377415/reset_config.patch",,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2008-03-08 21:16:50.667,,,false,,,,,,,,,,,,,,,12537,,,Sun Mar 09 16:56:37 UTC 2008,Patch Available,,,,,,,"0|i04yef:",26838,,,,,,,,,"08/Mar/08 21:16;doronc;Config maintains properties by round, so this should do the trick: 

{code}
doc.term.vector=tvf:true:false
{code}

It sets term-vectors to true in round 0, false in round 1, true in round 2, etc.
Also, a column is added to the reports with the value of this property ('tvf'). 

Unless you already tried this and it didn't work?
","08/Mar/08 22:57;markrmiller@gmail.com;It seems to me that its not working right. Everything that is set in public void setConfig(Config config) is only set once for me, not per round. That is unless I apply the above patch. This means that I cannot seem to set tokenizing, storing, or termvectors per round.

From what I can tell it is because setConfig is only called once, and so only the first value is every read for those properties. The patch above puts set config in the resetInputs method which does get called per round. Not sure if that is the best fix, but I know cannot currently set those per round and have anything but the first setting take effect.

- Mark","09/Mar/08 08:01;doronc;Mark you are right that setConfig is called just once, at start.
At least for setting properties by round this should be sufficient. 
I wonder why this doesn't work for you.

I tried with this one:

{code}
compound=true

analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
directory=RamDirectory

doc.stored=true
doc.tokenized=true
doc.term.vector=termVec:false:true
doc.add.log.step=10

doc.maker=org.apache.lucene.benchmark.byTask.feeds.SimpleDocMaker
task.max.depth.log=1

{

    { ""Populate""
        CreateIndex
        { AddDoc > : 50
        Optimize
        CloseIndex
    >

    ResetSystemErase
    NewRound

} : 2

RepSumByName
RepSelectByPref Populate
{code}

And got this output:
{code}
 Working Directory: work
 Running algorithm from: conf\termVecByRound.alg
 ------------> config properties:
 analyzer = org.apache.lucene.analysis.standard.StandardAnalyzer
 compound = true
 directory = RamDirectory
 doc.add.log.step = 10
 doc.maker = org.apache.lucene.benchmark.byTask.feeds.SimpleDocMaker
 doc.stored = true
 doc.term.vector = termVec:false:true
 doc.tokenized = true
 task.max.depth.log = 1
 work.dir = work
 -------------------------------
 ------------> algorithm:
 Seq {
     Seq_2 {
         Populate {
             CreateIndex
             Seq_50 {
                 AddDoc
             > * 50
             Optimize
             CloseIndex
         >
         ResetSystemErase
         NewRound
     } * 2
     RepSumByName
     RepSelectByPref Populate
 }
 
 ------------> starting task: Seq
 ------------> starting task: Seq_2
 --> 0.1 sec: main processed (add) 10 docs
 --> 0.1 sec: main processed (add) 20 docs
 --> 0.11 sec: main processed (add) 30 docs
 --> 0.11 sec: main processed (add) 40 docs
 --> 0.11 sec: main processed (add) 50 docs
 ------------> SimpleDocMaker statistics (0): 
 num docs added since last inputs reset:                   50
 total bytes added since last inputs reset:             42,150
 
 
 
 --> Round 0-->1:   doc.term.vector:false-->true
 
 --> 0 sec: main processed (add) 60 docs
 --> 0 sec: main processed (add) 70 docs
 --> 0 sec: main processed (add) 80 docs
 --> 0 sec: main processed (add) 90 docs
 --> 0 sec: main processed (add) 100 docs
 ------------> SimpleDocMaker statistics (1): 
 num docs added since last inputs reset:                   50
 total bytes added since last inputs reset:             42,150
 
 
 
 --> Round 1-->2:   doc.term.vector:true-->false
 
 
 ------------> Report Sum By (any) Name (2 about 3 out of 4)
 Operation   round termVec   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
 Seq_2           0   false        1          106        530.0        0.20       639,912      5,177,344
 Populate        -       -        2           53        706.7        0.15       839,552      5,177,344
 
 
 ------------> Report Select By Prefix (Populate) (2 about 2 out of 4)
 Operation   round termVec   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
 Populate        0   false        1           53        378.6        0.14       858,080      5,177,344
 Populate -  -   1 -  true -  -   1 -  -  -   53 -  - 5,300.0 -  -   0.01 -  -  821,024 -  - 5,177,344
 
 ####################
 ###  D O N E !!! ###
 ####################
{code}

Note in particular this line:
{code}
[java] --> Round 0-->1:   doc.term.vector:false-->true 
{code}

Note that a *NewRound* command is required in order for the round number to change. 
{code}
    NewRound
{code}

A possible cause for error is that the property definition parsing requires a property name prefix for multi-valued properties.
So this would not work as expected:
{code}
doc.term.vector=false:true
{code}

But this will work:
{code}
doc.term.vector=termVec:false:true
{code}

If it still doesn't work for you, can you post here the algorithm?","09/Mar/08 13:43;markrmiller@gmail.com;My algorithm is below.

I see ""Round 0-->1:   doc.term.vector:false-->true"" as well...however if I put a debug print on what is returned from public boolean get (String name, boolean dflt), it is only ever called once for ""doc.term.vector"" as well as the other guys in setConfig.

More importantly, lets say I set it to true:false....if I look at the work/index directory on the second run, there are certainly term vectors. Thats how I noticed this to begin with...I was looking at the index and saw the term vector files on every round. Its possible I have something messed up, but every time I run through everything again and it really does not seem to be working. If I set term vectors to false:true, they are never made in any round.

>>Mark you are right that setConfig is called just once, at start.
>>At least for setting properties by round this should be sufficient.
>>I wonder why this doesn't work for you. 

I think this admits the problem right? The get property for everything in setConfig is only called once...that loads up the ""false:true"", returns false, and sets up ""true"" to be returned on the next call...the next time you call get on Config you will get the ""true""...but there is no next time. Its only done once...so it shows up right in the output ""Round 0-->1:   doc.term.vector:false-->true"", but its only every called once and so only loads false.

- Mark


{code}
ram.flush.mb=flush:32:32
compound=false

analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
directory=FSDirectory

doc.stored=true
doc.tokenized=tok:false:true
doc.term.vector=vec:true:false
doc.term.vector.offsets=tvo:false:true
doc.term.vector.positions=tvp:false:true
doc.add.log.step=2000

docs.dir=reuters-out

doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker

query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker

# task at this depth or less would print when they start
task.max.depth.log=2

log.queries=true
# -------------------------------------------------------------------------------------

{ ""Rounds""
      
    ResetSystemErase

        CreateIndex
        { ""MAddDocs"" AddDoc(60) } : 20000
        Optimize
        CloseIndex
  
    OpenReader
      { ""SrchTrvRetNewRdr"" SearchTravRet(10) > : 1000
    CloseReader
    OpenReader
      { ""SearchHlgtSameRdr"" SearchTravRetHighlight(size[20],highlight[20],mergeContiguous[true],maxFrags[0],fields[body]) > : 1000

    CloseReader

    RepSumByPref SearchHlgtSameRdr

    NewRound

} : 2

RepSumByNameRound
RepSumByName
RepSumByPrefRound MAddDocs
{code}","09/Mar/08 15:22;doronc;Ok I can see it now, you're right. 
So all doc maker per rounds settings were ignored - first round settings were used. 
I am updating TestPerfTasksLogic.testIndexWriterSettings() to catch this bug.
Thanks for catching this,
Doron","09/Mar/08 15:24;doronc;same fix + test case that fails without the fix.","09/Mar/08 16:31;doronc;QualityTest fails with previous patch, exposing a related bug in ReutersDocMaker,
of not reseting files list at call to setConfig(), Was not required before, but now since
setConfig is called more than once must clear the list of collected files.
Attached file fixes this and all benchmark tests pass.","09/Mar/08 16:56;doronc;Committed, thanks Mark!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Highlighter Documentation updates,LUCENE-1132,12386267,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,gsingers,gsingers,gsingers,15/Jan/08 13:47,29/Jan/08 13:35,30/Sep/19 08:38,29/Jan/08 13:35,,,,,,,,,,,,,,,modules/other,,,0,,,,Various places in the Highlighter documentation refer to bytes (i.e. SimpleFragmenter) when it should be chars.  See http://www.gossamer-threads.com/lists/lucene/java-user/56986,,,,,,,,,,,,,,,,"27/Jan/08 17:23;gsingers;LUCENE-1132.patch;https://issues.apache.org/jira/secure/attachment/12374156/LUCENE-1132.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12613,,,Tue Jan 29 13:35:29 UTC 2008,,,,,,,,"0|i04yvj:",26915,,,,,,,,,"27/Jan/08 17:23;gsingers;Update for highlighter.   Deprecates byte related stuff in favor of chars in the naming of variable and accessors, even though the underlying functionality is still the same.","29/Jan/08 13:35;gsingers;Committed revision 616305.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow overriding the specification version in MANIFEST.MF,LUCENE-1123,12385787,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,08/Jan/08 16:53,25/Jan/08 03:24,30/Sep/19 08:38,08/Jan/08 20:09,,,,,,,,,,,2.3,,,,general/build,,,0,,,,"The specification version in MANIFEST.MF should only consist of
digits. When we e. g. build a release candidate with a version like
2.3-rc1 then we have to specify a different specification version.

See related discussion:
http://www.gossamer-threads.com/lists/lucene/java-dev/56611

",,,,,,,,,,,,,,,,"08/Jan/08 16:55;michaelbusch;lucene-1123.patch;https://issues.apache.org/jira/secure/attachment/12372725/lucene-1123.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12622,,,Tue Jan 08 20:09:18 UTC 2008,New,,,,,,,"0|i04yxj:",26924,,,,,,,,,"08/Jan/08 16:55;michaelbusch;This simple patch allows overriding the specification version:

 ant -Dversion=2.3-rc1 -Dspec.version=2.3 clean dist dist-src

I'm planning to commit this today.","08/Jan/08 20:09;michaelbusch;Committed to trunk and 2.3 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Clean up old JIRA issues in component ""Other""",LUCENE-1107,12385345,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,michaelbusch,michaelbusch,30/Dec/07 18:21,25/Jan/08 03:24,30/Sep/19 08:38,18/Jan/08 14:21,,,,,,,,,,,2.3,,,,core/other,,,0,,,,"A list of all JIRA issues in component ""Other"" that haven't been updated in 2007:

   *	 LUCENE-746  	 Incorrect error message in AnalyzingQueryParser.getPrefixQuery   
   *	LUCENE-644 	Contrib: another highlighter approach 
   *	LUCENE-574 	support for vjc java compiler, also known as J# 
   *	LUCENE-471 	gcj ant target doesn't work on windows 
   *	LUCENE-434 	Lucene database bindings 
   *	LUCENE-254 	[PATCH] pseudo-relevance feedback enhancement 
   *	LUCENE-180 	[PATCH] Language guesser contribution 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-01-18 14:21:10.526,,,false,,,,,,,,,,,,,,,12638,,,Fri Jan 18 14:21:10 UTC 2008,New,,,,,,,"0|i04z13:",26940,,,,,,,,,"18/Jan/08 14:21;gsingers;I've gone through all of these and closed the ones I think are appropriate to close.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simple toString() for BooleanFilter,LUCENE-1049,12382249,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,jcalabrese,jcalabrese,10/Nov/07 02:05,25/Jan/08 03:24,30/Sep/19 08:38,17/Nov/07 20:17,,,,,,,,,,,2.3,,,,modules/other,,,0,,,,"While working with BooleanFilter I wanted a basic toString() for debugging.

This is what I came up.  It works ok for me.",,,,,,,,,,,,,,,,"10/Nov/07 17:03;jcalabrese;patch3.txt;https://issues.apache.org/jira/secure/attachment/12369306/patch3.txt",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-11-10 02:32:53.474,,,false,,,,,,,,,,,,,,,12696,,,Sat Nov 17 20:17:52 UTC 2007,New,Patch Available,,,,,,"0|i04zdz:",26998,,,,,,,,,"10/Nov/07 02:06;jcalabrese;Attached patch","10/Nov/07 02:32;ehatcher;Jason - the patch looks like it is generated backwards (minus signs, not plusses).  ","10/Nov/07 16:00;jcalabrese;Oops, here's a corrected patch","10/Nov/07 17:03;jcalabrese;Wrapped the whole toString with BooleanFilter to match the toString()'s for other filters","17/Nov/07 20:17;markh;Committed in revision 595996
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes a handful of misspellings/mistakes in changes.txt,LUCENE-1031,12380767,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,markrmiller@gmail.com,markrmiller@gmail.com,19/Oct/07 12:07,25/Jan/08 03:24,30/Sep/19 08:38,19/Oct/07 13:02,2.3,,,,,,,,,,2.3,,,,core/other,,,0,,,,There are a handful of misspellings/mistakes in changes.txt. This patch fixes them. Avoided the one or two British to English conversions <g>,,,,,,,,,,,,,,,,"19/Oct/07 12:07;markrmiller@gmail.com;changes.txt.patch;https://issues.apache.org/jira/secure/attachment/12368007/changes.txt.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-10-19 12:58:38.943,,,false,,,,,,,,,,,,,,,12714,,,Fri Oct 19 13:02:01 UTC 2007,New,Patch Available,,,,,,"0|i04zhz:",27016,,,,,,,,,"19/Oct/07 12:58;mikemccand;Sheesh, we committers really can't spell!!

Thanks Mark!

I'll commit.","19/Oct/07 13:02;mikemccand;Fixed -- thanks Mark!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove TermVectorsWriter (it's no longer used),LUCENE-984,12376253,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,17/Aug/07 16:54,25/Jan/08 03:24,30/Sep/19 08:38,18/Sep/07 09:55,2.3,,,,,,,,,,2.3,,,,core/index,,,0,,,,"We should remove TermVectorsWriter: it's no longer used now that
DocumentsWriter writes the term vectors directly to the index.",,,,,,,,,,,,,,,,"14/Sep/07 18:12;mikemccand;LUCENE-984.patch;https://issues.apache.org/jira/secure/attachment/12365886/LUCENE-984.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12759,,,Fri Sep 14 18:12:43 UTC 2007,New,,,,,,,"0|i04zsf:",27063,,,,,,,,,"14/Sep/07 18:12;mikemccand;Attached patch.

It turns out we still use TermVectorsWriter for segment merging, so I
kept it but optimized it for that specific use case of bulk-adding of
term vectors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FilterIndexReader should overwrite isOptimized(),LUCENE-970,12374992,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,30/Jul/07 19:36,25/Jan/08 03:24,30/Sep/19 08:38,30/Jul/07 22:30,,,,,,,,,,,2.3,,,,core/search,,,0,,,,A call of FilterIndexReader.isOptimized() results in a NPE because FilterIndexReader does not overwrite isOptimized().,,,,,,,,,,,,,,,,"30/Jul/07 19:51;michaelbusch;lucene-970.patch;https://issues.apache.org/jira/secure/attachment/12362805/lucene-970.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12773,,,Mon Jul 30 19:51:49 UTC 2007,New,Patch Available,,,,,,"0|i04zvj:",27077,,,,,,,,,"30/Jul/07 19:51;michaelbusch;Trivial patch. I'm planning to commit this shortly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check for boundary conditions in FieldInfos,LUCENE-939,12372177,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,22/Jun/07 01:30,25/Jan/08 03:23,30/Sep/19 08:38,24/Jul/07 03:05,,,,,,,,,,,2.3,,,,,,,0,,,,"In FieldInfos there are three methods in which we don't check for
boundary conditions but catch e. g. an IndexOutOfBoundsException
or a NPE. I think this isn't good code style and is probably not
even faster than checking explicitly.

""Exceptions should not be used to alter the flow of a program as 
part of normal execution.""

Also this can be irritating when you're trying to debug an 
IndexOutOfBoundsException that is thrown somewhere else in your
program and you place a breakpoint on that exception.

The three methods are:

  public int fieldNumber(String fieldName) {
    try {
      FieldInfo fi = fieldInfo(fieldName);
      if (fi != null)
        return fi.number;
    }
    catch (IndexOutOfBoundsException ioobe) {
      return -1;
    }
    return -1;
  }
  

  public String fieldName(int fieldNumber) {
    try {
      return fieldInfo(fieldNumber).name;
    }
    catch (NullPointerException npe) {
      return """";
    }
  }
  
  
  public FieldInfo fieldInfo(int fieldNumber) {
    try {
      return (FieldInfo) byNumber.get(fieldNumber);
    }
    catch (IndexOutOfBoundsException ioobe) {
      return null;
    }
  }",,,,,,,,,,,,,,,,"23/Jul/07 03:32;michaelbusch;lucene-939.patch;https://issues.apache.org/jira/secure/attachment/12362317/lucene-939.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12803,,,Mon Jul 23 03:32:33 UTC 2007,New,,,,,,,"0|i0502f:",27108,,,,,,,,,"23/Jul/07 03:32;michaelbusch;This patch removes the catch clauses and adds explicit checks
for the boundary cases. 

In fieldInfo(int fieldNumber) we only have to check if 
fieldNumber>=0 because of one special case: the very first 
term in the dictionary is always an empty term """":"""". That's
why -1 is stored as field number for this term in the 
dictionary. If we could avoid storing the empty term then we
could also get rid of these checks in FieldInfos. When I 
have some time I'll look into that and add a separate issue.

All tests pass with this patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Clean up old JIRA issues in component ""Analysis""",LUCENE-1104,12385342,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,michaelbusch,michaelbusch,30/Dec/07 18:13,12/Jan/08 23:14,30/Sep/19 08:38,12/Jan/08 23:14,,,,,,,,,,,2.3,,,,modules/analysis,,,0,,,,"A list of all JIRA issues in component ""Analysis"" that haven't been updated in 2007:

   *	 LUCENE-760  	 Spellchecker could/should use n-gram tokenizers instead of rolling its own n-gramming   
   *	LUCENE-677 	Italian Analyzer 
   *	LUCENE-571 	StandardTokenizer parses decimal number as <HOST> 
   *	LUCENE-566 	Esperanto Analyzer 
   *	LUCENE-559 	Turkish Analyzer for Lucene 
   *	LUCENE-494 	Analyzer for preventing overload of search service by queries with common terms in large indexes 
   *	LUCENE-424 	[PATCH] Submissiom form simple Romanian Analyzer 
   *	LUCENE-417 	StandardTokenizer has problems with comma-separated values 
   *	LUCENE-400 	NGramFilter -- construct n-grams from a TokenStream 
   *	LUCENE-396 	[PATCH] Add position increment back into StopFilter 
   *	LUCENE-387 	Contrib: Main memory based SynonymMap and SynonymTokenFilter 
   *	LUCENE-321 	[PATCH] Submissiom of my Tswana Analyzer 
   *	LUCENE-233 	[PATCH] analyzer refactoring based on CVS HEAD from 6/21/2004 
   *	LUCENE-210 	[PATCH] Never write an Analyzer again 
   *	LUCENE-205 	[PATCH] Patches for RussianAnalyzer 
   *	LUCENE-185 	[PATCH] Thai Analysis Enhancement 
   *	LUCENE-152 	[PATCH] KStem for Lucene 
   *	LUCENE-82 	[PATCH] HTMLParser: IOException: Pipe closed 

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-01-12 23:14:23.366,,,false,,,,,,,,,,,,,,,12641,,,Sat Jan 12 23:14:23 UTC 2008,New,,,,,,,"0|i04z1r:",26943,,,,,,,,,"12/Jan/08 23:14;gsingers;I think the remaining 3 issues are reasonable to keep open",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
contrib/Highlighter javadoc example needs to be updated,LUCENE-1114,12385381,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,gsingers,gsingers,gsingers,31/Dec/07 20:22,02/Jan/08 15:31,30/Sep/19 08:38,02/Jan/08 15:31,,,,,,,,,,,,,,,modules/other,,,0,,,,"The Javadoc package.html example code is outdated, as it still uses QueryParser.parse.  

http://lucene.zones.apache.org:8080/hudson/job/Lucene-Nightly/javadoc/contrib-highlighter/index.html",,,,,,,,,,,,,,,,"01/Jan/08 18:41;gsingers;LUCENE-1114.patch;https://issues.apache.org/jira/secure/attachment/12372380/LUCENE-1114.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12631,,,Wed Jan 02 15:31:39 UTC 2008,,,,,,,,"0|i04yzj:",26933,,,,,,,,,"31/Dec/07 21:18;gsingers;It also only demonstrates using the Analyzer to get the tokenStream, and not term vectors (TermSources)","01/Jan/08 18:41;gsingers;First draft at a bit of an update to the package javadocs","02/Jan/08 15:31;gsingers;committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Token.DEFAULT_TYPE public,LUCENE-1080,12384019,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,gsingers,gsingers,gsingers,05/Dec/07 20:15,06/Dec/07 03:34,30/Sep/19 08:38,06/Dec/07 03:34,,,,,,,,,,,,,,,modules/analysis,,,0,,,,"Make Token.DEFAULT_TYPE public so that TokenFilters using the reusable Token model have a way of setting the type back to the default.

No patch necessary.  I will commit soon.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12665,,,Thu Dec 06 03:34:14 UTC 2007,New,,,,,,,"0|i04z73:",26967,,,,,,,,,"06/Dec/07 03:34;gsingers;Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexReader.FieldOption has incomplete Javadocs,LUCENE-921,12371202,,Wish,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,michaelbusch,michaelbusch,08/Jun/07 00:51,27/Nov/07 01:08,30/Sep/19 08:38,27/Nov/07 01:08,,,,,,,,,,,,,,,general/javadocs,,,0,,,,IndexReader.FieldOption has no javadocs at all.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-11-26 23:34:35.271,,,false,,,,,,,,,,,,,,,12821,,,Tue Nov 27 01:08:31 UTC 2007,New,,,,,,,"0|i0506f:",27126,,,,,,,,,"26/Nov/07 23:34;lucenebugs@danielnaber.de;I added some javadoc comments. Not much, but I think this can be closed.","27/Nov/07 01:08;michaelbusch;Cool! Thanks Daniel, keep going! ;)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexModifier has incomplete Javadocs,LUCENE-920,12371200,,Wish,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,michaelbusch,michaelbusch,08/Jun/07 00:49,26/Nov/07 22:57,30/Sep/19 08:38,26/Nov/07 22:57,,,,,,,,,,,,,,,general/javadocs,,,0,,,,"A lot of public and protected members of org.apache.lucene.index.IndexModifier 
don't have javadocs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-11-26 22:51:24.858,,,false,,,,,,,,,,,,,,,12822,,,Mon Nov 26 22:57:22 UTC 2007,New,,,,,,,"0|i0506n:",27127,,,,,,,,,"26/Nov/07 22:51;lucenebugs@danielnaber.de;I think this bug can be closed, as IndexModifier is deprecated.","26/Nov/07 22:57;michaelbusch;I agree.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
better explain output,LUCENE-1066,12383172,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,lucenebugs@danielnaber.de,lucenebugs@danielnaber.de,25/Nov/07 13:20,25/Nov/07 22:40,30/Sep/19 08:38,25/Nov/07 22:40,2.3,,,,,,,,,,2.3,,,,core/query/scoring,,,0,,,,Very simple patch that slightly improves output of idf: show both docFreq and numDocs.,,,,,,,,,,,,,,,,"25/Nov/07 13:21;lucenebugs@danielnaber.de;explain-output.diff;https://issues.apache.org/jira/secure/attachment/12370157/explain-output.diff",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12679,,,Sun Nov 25 22:40:24 UTC 2007,New,Patch Available,,,,,,"0|i04za7:",26981,,,,,,,,,"25/Nov/07 22:40;lucenebugs@danielnaber.de;applied",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Searcher class should have an abstract declaration of doc(int n, FieldSelector)",LUCENE-999,12378287,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Invalid,gsingers,gsingers,gsingers,14/Sep/07 13:49,14/Sep/07 23:56,30/Sep/19 08:38,14/Sep/07 23:56,,,,,,,,,,,,,,,core/search,,,0,,,,"Add:
abstract public Document doc(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException;

to Searcher, since it is defined by Searchable anyway, which Searcher implements.  This would allows people using Searcher to have access to the FieldSelector mechanism.  

If there are no objections, I will commit this change by Monday, as I don't think it will break anything, because all derived classes already have to implement it anyway. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-09-14 21:47:26.846,,,false,,,,,,,,,,,,,,,12745,,,Fri Sep 14 23:56:46 UTC 2007,,,,,,,,"0|i04zp3:",27048,,,,,,,,,"14/Sep/07 21:47;yseeley@gmail.com;All ""Searchable"" already have access to the FieldSelector  mechanism since it's in the interface... am I misunderstanding your reason for adding this declaration?","14/Sep/07 23:56;gsingers;yeah, sorry for the noise.  Don't know what I was thinking.  I thought it would help with http://www.gossamer-threads.com/lists/lucene/java-user/52904 but I am now guessing this person is using an older version of Lucene
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Searcher code creating Hits is somewhat messy,LUCENE-775,12360461,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,,karl.wettin,karl.wettin,13/Jan/07 13:10,28/Jul/07 07:46,30/Sep/19 08:38,28/Jul/07 07:46,,,,,,,,,,,,,,,,,,0,,,,"This patch makes sure all Hits-resulting queries sent to Searcher pass though the same methods, rather than an ad hoc Hits call per method call. Did it so it would be easier for me to implement this decorated searcher cache of mine.

I could not find any implementations overriding the methods I set final, so I think it is allright.",,,,,,,,,,,,,,,,"13/Jan/07 13:11;karl.wettin;searcher.diff;https://issues.apache.org/jira/secure/attachment/12348902/searcher.diff",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-01-15 07:14:20.377,,,false,,,,,,,,,,,,,,,12978,,,Sat Jul 28 07:46:05 UTC 2007,New,Patch Available,,,,,,"0|i05133:",27273,,,,,,,,,"15/Jan/07 07:14;hossman;I agree that's messy and should be cleaned up .. while we're at it the Hits constructor has just as much redundency.

i don't think it's safe to make those methods final though ... there may not be any subclasses in the tree, but that's no garuntee there aren't users with their own subclasses (consider all the people who might have written a proxy cache like you are but needed to override all of those methods to do so)

can you explain this...

+  /** Sub class ad hoc IndexReader coupling */
+  protected IndexSearcher() {
+  }

""reader"" is package protected, so any out of package subclass that uses this constructor is pretty much screwed right? .. how is would this constructor be useful?

","15/Jan/07 09:36;karl.wettin;> can you explain this...
> 
> +  /** Sub class ad hoc IndexReader coupling */
> +  protected IndexSearcher() {
> +  }
> 
> ""reader"" is package protected, so any out of package subclass that uses this
>  constructor is pretty much screwed right? .. how is would this constructor be useful?

Sorry, that is an old artifact from a hack, not ment for the patch.  I'll refresh the patch.

> i don't think it's safe to make those methods final though ... there may not be any 
> subclasses in the tree, but that's no garuntee there aren't users with their own subclasses
> (consider all the people who might have written a proxy cache like you are but needed 
> to override all of those methods to do so) 

I personally  think they should change their code to override only the non-final method call. Anything else would be overkill. At least that is what I concluded while making these changes, hence the finals. But if it is final or not does not matter much to me. It is all good as long all Hits-returining method calls pass by that last non-final method in this patch.","28/Jul/07 07:46;karl.wettin;More nasty old code I'm getting ridth of.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document Vector->ArrayList,LUCENE-959,12373801,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,yseeley@gmail.com,yseeley@gmail.com,13/Jul/07 20:42,16/Jul/07 18:47,30/Sep/19 08:38,16/Jul/07 18:47,,,,,,,,,,,,,,,,,,0,,,,"Document Vector should be changed to ArrayList.
Document is not advertised to be thread safe, and it's doubtful that anyone modifies a Document from multiple threads.",,,,,,,,,,,,,,,,"13/Jul/07 20:44;yseeley@gmail.com;Document.patch;https://issues.apache.org/jira/secure/attachment/12361817/Document.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12783,,,Mon Jul 16 18:47:21 UTC 2007,New,,,,,,,"0|i04zxz:",27088,,,,,,,,,"13/Jul/07 20:44;yseeley@gmail.com;Trivial patch attached.
Any concerns?","16/Jul/07 18:47;yseeley@gmail.com;committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
replace text from an online collection (used in few test cases) with text that is surely 100% free.,LUCENE-946,12372705,,Task,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,doronc,doronc,doronc,29/Jun/07 17:45,30/Jun/07 05:45,30/Sep/19 08:38,30/Jun/07 05:44,,,,,,,,,,,,,,,,,,0,,,,"Text from an online firstaid collection (firstaid . ie . eu . org)  is used as arbitrary text for test documents creation, in:
   o.a.l.analysis.Analyzer.FunctionTestSetup.DOC_TEXT_LINES
   o.a.l.benchmark.byTask.feeds.SimpleDocMaker.DOC_TEXT

I once got this text from Project Gutenberg and was sure that it is free. But now the referred Web site does not seem to respond, and I can no more find that firstaid eBook in the Project Gutenberg site.

Since it doesn't matter what text we use there, I will just replace that with some of my own words...",,,,,,,,,,,,,,,,"29/Jun/07 18:38;doronc;lucene-946.patch;https://issues.apache.org/jira/secure/attachment/12360835/lucene-946.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12796,,,Sat Jun 30 05:45:21 UTC 2007,Patch Available,,,,,,,"0|i0500v:",27101,,,,,,,,,"30/Jun/07 05:44;doronc;Fixed.","30/Jun/07 05:45;doronc;Thanks Steven for pointing this out!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo on query parser syntax web page.,LUCENE-936,12372086,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,hossman,kganong,kganong,21/Jun/07 01:39,26/Jun/07 04:35,30/Sep/19 08:38,26/Jun/07 04:35,2.2,,,,,,,,,,,,,,general/website,,,0,,,,"On the web page http://lucene.apache.org/java/docs/queryparsersyntax.html#N10126 the text says:

""To search for documents that must contain ""jakarta"" and may contain ""lucene"" use the query:""

The example says:

+jakarta apache

The problem:
The example uses apache where the text says lucene.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-06-26 04:35:38.481,,,false,,,,,,,,,,,,,,,12806,,,Tue Jun 26 04:35:38 UTC 2007,New,,,,,,,"0|i05033:",27111,,,,,,,,,"26/Jun/07 04:35;hossman;thanks for spotting this...

Committed revision 550680.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some files are missing the license headers,LUCENE-931,12371297,,Wish,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,09/Jun/07 00:23,19/Jun/07 08:14,30/Sep/19 08:38,09/Jun/07 06:11,,,,,,,,,,,2.2,,,,general/javadocs,,,0,,,,"Jukka provided the following list of files that are missing the license headers.
In addition there might be other files (like build scripts) that don't have the headers.

src/java/org/apache/lucene/document/MapFieldSelector.java
src/java/org/apache/lucene/search/PrefixFilter.java
src/test/org/apache/lucene/TestHitIterator.java
src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java
src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java
src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
src/test/org/apache/lucene/index/TestFieldInfos.java
src/test/org/apache/lucene/index/TestIndexFileDeleter.java
src/test/org/apache/lucene/index/TestIndexWriter.java
src/test/org/apache/lucene/index/TestIndexWriterDelete.java
src/test/org/apache/lucene/index/TestIndexWriterLockRelease.java
src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
src/test/org/apache/lucene/index/TestNorms.java
src/test/org/apache/lucene/index/TestParallelTermEnum.java
src/test/org/apache/lucene/index/TestSegmentTermEnum.java
src/test/org/apache/lucene/index/TestTerm.java
src/test/org/apache/lucene/index/TestTermVectorsReader.java
src/test/org/apache/lucene/search/TestRangeQuery.java
src/test/org/apache/lucene/search/TestTermScorer.java
src/test/org/apache/lucene/store/TestBufferedIndexInput.java
src/test/org/apache/lucene/store/TestWindowsMMap.java
src/test/org/apache/lucene/store/_TestHelper.java
src/test/org/apache/lucene/util/_TestUtil.java
contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/FeedNotFoundException.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/ComponentType.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/RegistryException.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/storage/lucenestorage/StorageAccountWrapper.java
contrib/gdata-server/src/core/src/test/org/apache/lucene/gdata/storage/lucenestorage/TestModifiedEntryFilter.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/AtomUriElementTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMEntryImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMFeedImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMGenereatorImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMSourceImplTest.java
contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
contrib/javascript/queryConstructor/luceneQueryConstructor.js
contrib/javascript/queryEscaper/luceneQueryEscaper.js
contrib/javascript/queryValidator/luceneQueryValidator.js
contrib/queries/src/java/org/apache/lucene/search/BooleanFilter.java
contrib/queries/src/java/org/apache/lucene/search/BoostingQuery.java
contrib/queries/src/java/org/apache/lucene/search/FilterClause.java
contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery.java
contrib/queries/src/java/org/apache/lucene/search/TermsFilter.java
contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThisQuery.java
contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
contrib/snowball/src/java/net/sf/snowball/Among.java
contrib/snowball/src/java/net/sf/snowball/SnowballProgram.java
contrib/snowball/src/java/net/sf/snowball/TestApp.java
contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/BooleanQueryTst.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/ExceptionQueryTst.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/SingleFieldTestDb.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test01Exceptions.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test02Boolean.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test03Distance.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/Syns2Index.java
",,,,,,,,,,,,,,,,"09/Jun/07 03:41;michaelbusch;lucene-931.patch;https://issues.apache.org/jira/secure/attachment/12359312/lucene-931.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-06-09 08:39:25.39,,,false,,,,,,,,,,,,,,,12811,,,Sat Jun 09 08:39:25 UTC 2007,New,,,,,,,"0|i05047:",27116,,,,,,,,,"09/Jun/07 03:41;michaelbusch;This patch adds the license header to all files listed above and
to the ant build files.","09/Jun/07 06:11;michaelbusch;Committed to trunk & 2.2 branch. Thanks, Jukka!","09/Jun/07 08:39;jukkaz;Nice, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexWriter has incomplete Javadocs,LUCENE-924,12371207,,Wish,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,michaelbusch,michaelbusch,08/Jun/07 01:06,19/Jun/07 08:14,30/Sep/19 08:38,09/Jun/07 14:35,,,,,,,,,,,2.2,,,,general/javadocs,,,0,,,,A couple of getter methods in IndexWriter have no javadocs.,,,,,,,,,,,,,,,,"08/Jun/07 16:31;mikemccand;LUCENE-924.patch;https://issues.apache.org/jira/secure/attachment/12359291/LUCENE-924.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-06-08 07:57:23.256,,,false,,,,,,,,,,,,,,,12818,,,Fri Jun 08 16:31:06 UTC 2007,New,,,,,,,"0|i0505r:",27123,,,,,,,,,"08/Jun/07 07:57;mikemccand;I can take this one.","08/Jun/07 16:31;mikemccand;All javadoc changes to add missing docs in IndexWriter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should SegmentTermPositionVector be public?,LUCENE-923,12371205,,Wish,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,08/Jun/07 01:01,19/Jun/07 08:14,30/Sep/19 08:38,09/Jun/07 02:59,,,,,,,,,,,2.2,,,,core/index,,,0,,,,"I'm wondering why SegmentTermPositionVector is public. It implements the public
interface TermPositionVector. Should we remove ""public""?",,,,,,,,,,,,,,,,"09/Jun/07 02:54;michaelbusch;lucene-923.patch;https://issues.apache.org/jira/secure/attachment/12359309/lucene-923.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-06-08 01:33:55.503,,,false,,,,,,,,,,,,,,,12819,,,Sat Jun 09 02:59:03 UTC 2007,New,,,,,,,"0|i0505z:",27124,,,,,,,,,"08/Jun/07 01:33;gsingers;+1","09/Jun/07 02:54;michaelbusch;There were no objections against making SegmentTermPositionVector
package-private, so I will go ahead and commit this patch soon.","09/Jun/07 02:59;michaelbusch;Committed to trunk & 2.2 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TermVectorOffsetInfo has incomplete Javadocs,LUCENE-922,12371204,,Wish,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Duplicate,gsingers,michaelbusch,michaelbusch,08/Jun/07 00:57,19/Jun/07 08:14,30/Sep/19 08:38,09/Jun/07 03:08,,,,,,,,,,,2.2,,,,general/javadocs,,,0,,,,org.apache.lucene.index.TermVectorOffsetInfo has no javadocs at all.,,,,,,,,,,,,LUCENE-918,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-06-08 01:55:07.907,,,false,,,,,,,,,,,,,,,12820,,,Sat Jun 09 03:08:28 UTC 2007,New,,,,,,,"0|i05067:",27125,,,,,,,,,"08/Jun/07 01:55;gsingers;Will provide a patch for both of them","09/Jun/07 03:08;michaelbusch;Grant submitted a patch for this on LUCENE-918.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultSkipListReader should not be public,LUCENE-919,12371199,,Wish,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,08/Jun/07 00:45,19/Jun/07 08:14,30/Sep/19 08:38,08/Jun/07 03:30,,,,,,,,,,,2.2,,,,core/index,,,0,,,,"There's no need for org.apache.lucene.index.DefaultSkipListReader to be public.
This class hasn't been released yet, so we should fix this before 2.2.",,,,,,,,,,,,,,,,"08/Jun/07 01:18;michaelbusch;lucene-919.patch;https://issues.apache.org/jira/secure/attachment/12359230/lucene-919.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12823,,,Fri Jun 08 03:30:06 UTC 2007,New,,,,,,,"0|i0506v:",27128,,,,,,,,,"08/Jun/07 03:30;michaelbusch;Committed to trunk & 2.2 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadoc improvements for Payload class,LUCENE-917,12371196,,Wish,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,08/Jun/07 00:38,19/Jun/07 08:14,30/Sep/19 08:38,09/Jun/07 04:56,,,,,,,,,,,2.2,,,,general/javadocs,,,0,,,,Some methods in org.apache.lucene.index.Payload don't have javadocs,,,,,,,,,,,,,,,,"09/Jun/07 03:04;michaelbusch;lucene-917.patch;https://issues.apache.org/jira/secure/attachment/12359310/lucene-917.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12825,,,Sat Jun 09 04:56:43 UTC 2007,New,,,,,,,"0|i0507b:",27130,,,,,,,,,"09/Jun/07 04:56;michaelbusch;Committed to trunk & 2.2 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add/change warning comments in the javadocs of Payload APIs,LUCENE-910,12370971,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,05/Jun/07 22:08,19/Jun/07 08:14,30/Sep/19 08:38,16/Jun/07 23:26,,,,,,,,,,,2.2,,,,general/javadocs,,,0,,,,"Since the payload API is still experimental we should change the comments
in the javadocs similar to the new search/function package.",,,,,,,,,,,,,,,,"16/Jun/07 23:17;michaelbusch;lucene-910-2.patch;https://issues.apache.org/jira/secure/attachment/12359949/lucene-910-2.patch","06/Jun/07 02:54;michaelbusch;lucene-910.patch;https://issues.apache.org/jira/secure/attachment/12359018/lucene-910.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12832,,,Sat Jun 16 23:26:52 UTC 2007,Patch Available,,,,,,,"0|i0508v:",27137,,,,,,,,,"06/Jun/07 02:57;michaelbusch;Committed.","16/Jun/07 23:15;michaelbusch;I forgot to change the comment in one class.","16/Jun/07 23:26;michaelbusch;Committed to trunk & 2.2 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Demo targets for running the demo,LUCENE-909,12370970,,Task,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,doronc,doronc,doronc,05/Jun/07 21:57,19/Jun/07 08:14,30/Sep/19 08:38,05/Jun/07 23:57,,,,,,,,,,,2.2,,,,,,,0,,,,"Now that the demo build targets are working and build the jar/war, it may be useful for users to also be able to run the demo with something like 'ant run-demo'. This complements existing docs/demo.html.",,,,,,,,,,,,,,,,"05/Jun/07 22:21;doronc;lucene-909.patch;https://issues.apache.org/jira/secure/attachment/12358996/lucene-909.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-06-05 22:41:49.514,,,false,,,,,,,,,,,,,,,12833,,,Tue Jun 05 23:57:35 UTC 2007,Patch Available,,,,,,,"0|i05093:",27138,,,,,,,,,"05/Jun/07 22:21;doronc;The patch:

1)  adds 4 targets to demo's build file:
     - demo-index-html -   Run html indexing demo (index the javadocs).
     - demo-index-text -     Run text indexing demo (index the sources of the demo).
     - demo-search-html - Run interactive search demo.
     - demo-search-text -  Run interactive search demo.

2) changes the search demo interactive print statements (which ask user input) to println(), so that the request for input shows also when the demo runs from ant (ant buffers the output until it detects eol, found no way around this). 

3) fixes a minor search demo bug, so that it now exists nicely when entering an empty query string.","05/Jun/07 22:27;doronc;In (3) above it should be ""exits"" (not ""exists""...)
I will commit this if there are no objections.
","05/Jun/07 22:41;michaelbusch;I just tried it out. Works great, Doron!

+1 for committing.

Minor question: Should ant clean also remove the directories
""demo-html-dir"" and ""demo-text-dir""?","05/Jun/07 23:12;doronc;Good idea, I'll add that, thanks!","05/Jun/07 23:57;doronc;committed, including Michael's suggestion ('clean' deletes demo index dirs).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Demo and contrib jars should contain NOTICE.TXT and LICENSE.TXT,LUCENE-907,12370947,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,05/Jun/07 17:45,19/Jun/07 08:14,30/Sep/19 08:38,05/Jun/07 19:54,,,,,,,,,,,2.2,,,,general/build,,,0,,,,"We should include NOTICE.TXT and LICENSE.TXT not only in the core jar but also
in the demo and contrib jars.",,,,,,,,,,,,,,,,"05/Jun/07 19:05;michaelbusch;lucene-907.patch;https://issues.apache.org/jira/secure/attachment/12358989/lucene-907.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12835,,,Tue Jun 05 19:54:57 UTC 2007,New,,,,,,,"0|i0509j:",27140,,,,,,,,,"05/Jun/07 19:05;michaelbusch;With this patch the two files are included in the META-INF dir of
the demo jar, the demo war and the contrib jars.

I will commit this soon.","05/Jun/07 19:54;michaelbusch;Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calculate MD5 checksums in target <dist-all>,LUCENE-904,12370800,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,04/Jun/07 05:15,19/Jun/07 08:14,30/Sep/19 08:38,05/Jun/07 01:19,,,,,,,,,,,2.2,,,,general/build,,,0,,,,"Trivial patch that extends the ant target <dist-all> to calculate
the MD5 checksums for the dist files.",,,,,,,,,,,,,,,,"05/Jun/07 00:00;michaelbusch;lucene-904.patch;https://issues.apache.org/jira/secure/attachment/12358909/lucene-904.patch","04/Jun/07 05:15;michaelbusch;lucene-904.patch;https://issues.apache.org/jira/secure/attachment/12358806/lucene-904.patch",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2007-06-04 05:38:33.511,,,false,,,,,,,,,,,,,,,12838,,,Tue Jun 05 01:19:10 UTC 2007,New,Patch Available,,,,,,"0|i050a7:",27143,,,,,,,,,"04/Jun/07 05:38;hossman;FWIW ... i put something like this in the solr build.xml a while back, but discovered the format ant's <checksum> task uses by default is really trivial and doesn't work when people use ""md5sum -c"" to try and check the sums.  (<checksum> has a ""format"" attribute now, but that wasn't added until ~Dec2006, so I think it requires ant 1.7)

the way we solved the problem was with the <solr-checksum> macro added to our build.xml in this commit...
http://svn.apache.org/viewvc/lucene/solr/trunk/build.xml?r1=483882&r2=484780

background...
http://www.nabble.com/%22correct%22-format-for-the-md5-files--tf2779495.html#a7754724","04/Jun/07 19:54;michaelbusch;> doesn't work when people use ""md5sum -c"" to try and check the sums.  

True, I just checked it. md5sum -c emits

  ""no properly formatted MD5 checksum lines found""

for the .md5 files <checksum> produces without the ""format"" attribute.  

> (<checksum> has a ""format"" attribute now, but that wasn't added until 
> ~Dec2006, so I think it requires ant 1.7)

Yes it requires 1.7.

> the way we solved the problem was with the <solr-checksum> macro added
> to our build.xml in this commit...

Looks good! Mind me stealing the macro?","04/Jun/07 20:28;hossman;You can't ""steal"" the macro, but it is Licensed under the ASL v2.0 (and more explicitly: when i wrote it and committed it to the Solr repository I Licensed it to the ASF for inclusion in ASF works as per the Apache Software License §5)","04/Jun/07 21:40;michaelbusch;Well, yes that's what I meant. As long as code is licensed to the ASF for inclusion in ASF works it can be copied from project A to project B, as long as both A and B are licensed under the same ASL, right?","04/Jun/07 22:41;hossman;(for the record: in my last comment i was attempting to be facetious ... i clearly failed)

IANAL but i but as i understand it you are correct, if not then a lot of apache projects are already in a lot of trouble.","04/Jun/07 22:56;michaelbusch;Actually my ""steal"" sentence was supposed to be facetious too :-) I guess we have to improve our communication (<-- this is supposed to be funny, too)

Alright, I will submit a new patch shortly.","05/Jun/07 00:00;michaelbusch;OK, here is the patch with the macro. And indeed, ""md5sum -c"" works
fine with the .md5 files.

I'm planning on committing this patch soon.","05/Jun/07 01:19;michaelbusch;Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor improvement to JavaDoc for ScoreDocComparator,LUCENE-807,12363116,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,pcowan,pcowan,19/Feb/07 01:26,19/Jun/07 08:14,30/Sep/19 08:38,15/Mar/07 08:48,2.1,,,,,,,,,,2.2,,,,general/javadocs,,,1,,,,"About to attach a very small patch for ScoreDocComparator which broadens the contract of compare(ScoreDoc, ScoreDoc) to follow the same semantics as java.util.Comparator.compare() -- allow any integer to be returned, rather than specifically -1/0/-1.

Note that this behaviour must already be acceptable; the anonymous ScoreDocComparators returned by FieldSortedHitQueue.comparatorStringLocale() already return the result of Collator.compare(), which is not tied to this -1/0/1 restriction.",,,,,,,,,,,,,,,,"19/Feb/07 01:27;pcowan;ScoreDocComparator.patch;https://issues.apache.org/jira/secure/attachment/12351474/ScoreDocComparator.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-03-15 08:48:55.226,,,false,,,,,,,,,,,,,,,12933,,,Thu Mar 15 08:48:55 UTC 2007,New,Patch Available,,,,,,"0|i050vz:",27241,,,,,,,,,"19/Feb/07 01:27;pcowan;Amends javadoc.
","15/Mar/07 08:48;michaelbusch;Committed. Thank you, Paul!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interface TermFreqVector has incomplete Javadocs,LUCENE-918,12371197,,Wish,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,gsingers,michaelbusch,michaelbusch,08/Jun/07 00:41,13/Jun/07 01:14,30/Sep/19 08:38,13/Jun/07 01:14,,,,,,,,,,,2.2,,,,general/javadocs,,,0,,,,We should improve the Javadocs of org.apache.lucene.index.TermFreqVector,,,,,,,,,,,,,,,,"08/Jun/07 02:05;gsingers;LUCENE_918_922.patch;https://issues.apache.org/jira/secure/attachment/12359233/LUCENE_918_922.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-06-08 02:05:24.488,,,false,,,,,,,,,,,,,,,12824,,,Wed Jun 13 01:14:48 UTC 2007,,,,,,,,"0|i05073:",27129,,,,,,,,,"08/Jun/07 02:05;gsingers;Patch for both LUCENE_918 and 922 since they are both related to term vectors and both are relatively straightforward.","13/Jun/07 01:14;gsingers;Committed revision 546696
and Committed revision 546694",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix for small syntax omission in TermQuery documentation,LUCENE-890,12370278,,Task,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,wls,wls,25/May/07 14:26,25/May/07 16:08,30/Sep/19 08:38,25/May/07 16:08,,,,,,,,,,,,,,,general/javadocs,,,0,,,,"A coding example, which could be cut'n'paste by a user, has unbalanced parenthesis.

This fix corrects the documentation, making no changes to functionality, only readability.",,,,,,,,,,,,,,,,"25/May/07 14:28;wls;DocumentationFix.patch;https://issues.apache.org/jira/secure/attachment/12358245/DocumentationFix.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-05-25 16:08:10.603,,,false,,,,,,,,,,,,,,,12852,,,Fri May 25 16:08:10 UTC 2007,New,Patch Available,,,,,,"0|i050db:",27157,,,,,,,,,"25/May/07 14:28;wls;Simply update to the search package.html file.","25/May/07 16:08;otis;Fixed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A number of documentation fixes for the search package summary,LUCENE-891,12370279,,Task,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,wls,wls,25/May/07 15:02,25/May/07 16:07,30/Sep/19 08:38,25/May/07 16:07,,,,,,,,,,,,,,,general/javadocs,,,0,,,,"Improves readability and clarity, corrects some basic English, makes some example text even more clear, and repairs typos.",,,,,,,,,,,,,,,,"25/May/07 15:02;wls;DocumentationFixes.patch;https://issues.apache.org/jira/secure/attachment/12358249/DocumentationFixes.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-05-25 16:07:35.375,,,false,,,,,,,,,,,,,,,12851,,,Fri May 25 16:07:35 UTC 2007,New,Patch Available,,,,,,"0|i050d3:",27156,,,,,,,,,"25/May/07 15:02;wls;Addresses a number of minor issues with the search package summary.","25/May/07 16:07;otis;Fixed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"QueryParser escaping/parsin issue with strings starting/ending with ||",LUCENE-881,12369552,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,smarjan,smarjan,17/May/07 11:19,23/May/07 04:55,30/Sep/19 08:38,23/May/07 04:55,2.1,2.2,,,,,,,,,,,,,core/queryparser,,,0,,,,"There is a problem with query parser when search string starts/ends with ||.  When string contains || in the middle like 'something || something' everything runs without a problem.

Part of code: 
  searchText = QueryParser.escape(searchText);
  QueryParser parser = null;
  parser = new QueryParser(fieldName, new CustomAnalyser());
  parser.parse(searchText);

CustomAnalyser class extends Analyser. Here is the only redefined method: 

    @Override
    public TokenStream tokenStream(String fieldName, Reader reader) {
      return new PorterStemFilter( (new StopAnalyzer()).tokenStream(fieldName, reader));
    }

I have tested this on Lucene 2.1 and latest source I have checked-out from SVN (Revision 538867) and in both cases parsing exception was thrown.

Part of Stack Trace (Lucene - SVN checkout - Revision 538867):
Cannot parse 'someting ||': Encountered ""<EOF>"" at line 1, column 11.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
 org.apache.lucene.queryParser.ParseException: Cannot parse 'someting ||': Encountered ""<EOF>"" at line 1, column 11.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:150)


Part of Stack Trace (Lucene 2.1):
Cannot parse 'something ||': Encountered ""<EOF>"" at line 1, column 12.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
 org.apache.lucene.queryParser.ParseException: Cannot parse 'something ||': Encountered ""<EOF>"" at line 1, column 12.
Was expecting one of:
    <NOT> ...
    ""+"" ...
    ""-"" ...
    ""("" ...
    ""*"" ...
    <QUOTED> ...
    <TERM> ...
    <PREFIXTERM> ...
    <WILDTERM> ...
    ""["" ...
    ""{"" ...
    <NUMBER> ...
    
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:149)


","MAC OS X 10.4.7, J2SE 5.0 Release 4",,,,,,,,,,,,,,,"18/May/07 17:34;michaelbusch;lucene-881.patch;https://issues.apache.org/jira/secure/attachment/12357650/lucene-881.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-05-17 16:29:57.347,,,false,,,,,,,,,,,,,,,12861,,,Wed May 23 04:55:46 UTC 2007,New,,,,,,,"0|i050fb:",27166,,,,,,,,,"17/May/07 16:29;michaelbusch;The problem here is that QueryParser.escape() does not escape | and &. This should be easy to fix, I'll submit a patch soon.","17/May/07 16:43;yseeley@gmail.com;Sorry, I don't quite understand the problem.  Could someone provide an actual query string that should work but doesn't?  ""||"" is reserved since it means OR, AFAIK.","17/May/07 17:18;michaelbusch;You are right Yonik, || is reserved.

The QueryParser itself works correctly:

""|| test ||"" yields a ParseException, which is correct because in this case || means OR
""\|\| test \|\|"" yields ""|| test ||"", this is correct, too, because the two | are escaped


The problem here is the escape() method:

  /**
   * Returns a String where those characters that QueryParser
   * expects to be escaped are escaped by a preceding <code>\</code>.
   */
  public static String escape(String s);

It escapes chars like +, -, ! and so on. Example:

escape(""++ test ++"") yields ""\+\+ test \+\+""

but

escape(""|| test ||"") yields ""|| test ||"".

I believe to be consistent escape() should escape the two chars | and & as well, no?","17/May/07 18:10;yseeley@gmail.com;> escape() should escape the two chars | and & as well, no?

Agree.","18/May/07 17:34;michaelbusch;Patch with additional unit tests.

All tests pass.","23/May/07 04:55;michaelbusch;I just committed this patch. Thank you for finding this bug, Slobodan!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make FSIndexInput and FSIndexOutput inner classes of FSDirectory,LUCENE-869,12367795,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,23/Apr/07 04:38,25/Apr/07 08:51,30/Sep/19 08:38,25/Apr/07 08:51,,,,,,,,,,,,,,,core/store,,,0,,,,"I would like make FSIndexInput and FSIndexOutput protected, static, inner classes of FSDirectory. Currently these classes are located in the same source file as FSDirectory, which means that classes outside the store package can not extend them.

I don't see any performance impacts or other side effects of this trivial patch. All unit tests pass.",,,,,,,,,,,,,,,,"23/Apr/07 04:40;michaelbusch;lucene-869.patch;https://issues.apache.org/jira/secure/attachment/12356020/lucene-869.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-04-24 18:53:32.412,,,false,,,,,,,,,,,,,,,12873,,,Wed Apr 25 08:51:20 UTC 2007,New,,,,,,,"0|i050hz:",27178,,,,,,,,,"23/Apr/07 04:40;michaelbusch;Attaching the patch file. If nobody objects I'm going to commit this soon.","24/Apr/07 18:53;cutting;Heh.  These classes precede the addition of nested classes to Java.  They're nine years old!  +1","25/Apr/07 08:51;michaelbusch;Funny, I didn't even know that nested classes weren't in Java forever... 

I just committed this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Term's equals() throws ClassCastException if passed something other than a Term,LUCENE-828,12364423,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,pcowan,pcowan,08/Mar/07 05:13,16/Mar/07 15:23,30/Sep/19 08:38,16/Mar/07 15:23,2.1,,,,,,,,,,,,,,,,,0,,,,"Term.equals(Object) does a cast to Term without checking if the other object is a Term.

It's unlikely that this would ever crop up but it violates the implied contract of Object.equals().",,,,,,,,,,,,,,,,"08/Mar/07 05:15;pcowan;termequals.patch;https://issues.apache.org/jira/secure/attachment/12352901/termequals.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-03-16 15:23:18.432,,,false,,,,,,,,,,,,,,,12912,,,Fri Mar 16 15:23:18 UTC 2007,New,Patch Available,,,,,,"0|i050rb:",27220,,,,,,,,,"08/Mar/07 05:15;pcowan;Amendment to .equals() to deal with non-Term objects.

Also includes standard performance optimisation in the case that the two objects are identity-equal, and a test case (the last assertion of which fails on current trunk, but passes after patch is applies to Term).","16/Mar/07 15:23;otis;Patch applied, thanks Paul!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extended javadocs in spellchecker,LUCENE-786,12361333,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,otis,karl.wettin,karl.wettin,26/Jan/07 14:56,03/Mar/07 10:55,30/Sep/19 08:38,02/Mar/07 18:29,2.0.0,,,,,,,,,,,,,,general/javadocs,,,0,,,,"Added some javadocs that explains why the spellchecker does not work as one might expect it to.

http://www.nabble.com/SpellChecker%3A%3AsuggestSimilar%28%29-Question-tf3118660.html#a8640395

> Without having looked at the code for a long time, I think the problem is what the
> lucene scoring consider to be best. First the grams are searched, resulting in a number
> of hits. Then the edit-distance is calculated on each hit. ""Genetics"" is appearently the
> third most similar hit according to Lucene, but the best according to Levenshtein.
>
> I.e. Lucene does not use edit-distance as similarity. You need to get a bunch of best hits
> in order to find the one with the smallest edit-distance.

I took a look at the code, and my assessment seems to be right.",,,,,,,,,,,,,,,,"26/Jan/07 14:59;karl.wettin;spellcheck_javadocs.diff;https://issues.apache.org/jira/secure/attachment/12349692/spellcheck_javadocs.diff",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-03-02 18:29:40.544,,,false,,,,,,,,,,,,,,,12967,,,Sat Mar 03 10:55:38 UTC 2007,New,Patch Available,,,,,,"0|i0510n:",27262,,,,,,,,,"26/Jan/07 14:59;karl.wettin;patch root is trunk/contrib/spellcheck","02/Mar/07 18:29;otis;Applied, merci Karl.","03/Mar/07 10:55;karl.wettin;
It might be noteworthy that the spell checker will gather numSug * 10 hits from the a priori corpus. I suppose that number (10) was something the original author came up with when testing. In most cases it is seems to be good enough. In my refactor I've introduced a method parameter for the factor. This is probably a better looking solution than telling the user to increase numSug, as numSug saves a few clock ticks when not adding a suggestion to the priority list.

The javadocs should probaly state something like that instead.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use WeakHashMap instead of Hashtable in FSDirectory,LUCENE-776,12360470,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Won't Fix,mikemccand,hibou,hibou,13/Jan/07 17:02,27/Feb/07 18:10,30/Sep/19 08:38,14/Jan/07 22:14,2.0.0,,,,,,,,,,2.1,,,,,,,0,,,,"I was just reading the FSDirectory java code, then I found this :

  /** This cache of directories ensures that there is a unique Directory
   * instance per path, so that synchronization on the Directory can be used to
   * synchronize access between readers and writers.
   *
   * This should be a WeakHashMap, so that entries can be GC'd, but that would
   * require Java 1.2.  Instead we use refcounts...
   */
  private static final Hashtable DIRECTORIES = new Hashtable();

Since Lucene is now requiring at least 1.2 (for ThreadLocal for instance, which is using BTW some WeakHashMap), maybe it is time to change ?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-01-14 12:07:05.148,,,false,,,,,,,,,,,,,,,12977,,,Tue Feb 27 18:10:39 UTC 2007,New,,,,,,,"0|i0512v:",27272,,,,,,,,,"14/Jan/07 12:07;mikemccand;Good catch!  And thanks for opening this.

But: how would we actually use WeakHashMap here?

With WeakHashMap, it's the keys that are the weak reference, so we'd
need to have our FSDirectory instance be the keys (I think?).  But
then I don't see how we could cleanly ""look up"" a given key to see if
we already have an FSDirectory instance for the incoming canonical
path?

Maybe instead we could keep the current Hashtable, but instead of
using the FSDirectory instance directly as the value, wrap it in a
WeakReference (and unwap on looking it up, later)?

One other small concern is: the current ""reference counting"" approach
has nice ""immediacy"".  Meaning once the final close() occurs on the
FSDirectory instance, it is immediately removed from the DIRECTORIES
hashtable.  Whereas, with a WeakReference, it would generally be a
much slower thing (I think?) because it's up to GC to decide when it
will actually be removed once only weak references remain?

Though: even with the ""immediacy"" of reference counting, we still must
then wait for GC to actually clean up the FSDirectory instance we had
removed.  Worse, if that path is re-opened we would create another
FSDirectory instance (ie more garbage to be collected).  So, since we
must wait for GC anyway with our current reference counting solution,
I guess it is in fact better to have a WeakReference so that if a
given path is re-used, and it hasn't been GC'd yet, we would re-use
it?
","14/Jan/07 18:05;karl.wettin;Michael McCandless [14/Jan/07 04:07 AM]
> Good catch! And thanks for opening this.
>
> But: how would we actually use WeakHashMap here?
>
> With WeakHashMap, it's the keys that are the weak reference, so we'd
> need to have our FSDirectory instance be the keys (I think?). But
> then I don't see how we could cleanly ""look up"" a given key to see if
> we already have an FSDirectory instance for the incoming canonical
> path? 

Not sure if this helps, but I just posted a SoftReferenceMap<K,V>.java in issue 550. Need to take a look at the license though, as it is based on something I found on the net. URL is in the javadoc. ","14/Jan/07 19:24;hibou;I think you've describe the problem completely Michael. When submitting this issue, I thought that the weak object in a WeakHashMap was the value of the map. So it appears that it is not done for that. About your last though, it is accurate because I think that most of the time, Lucene-based application are opening their directories at the same place.
My turn of though : we might have an issue if the table holds some reference that are not yet GCed. A directory is closed, ""manually"" cleaned up, and reopened with a different lock factory : this will fail with the IOException because of the still cached directory, conflicting because of its different lock factory. So the current design might be the best one in fact.","14/Jan/07 22:10;mikemccand;Karl, thanks for the offer, but we're still on JDK 1.4.x so far, so we
can't use that SoftReferenceMap<K,V> directly just yet (but it sounds
neat!).

Oh good point Nicolas.  If we make this change it's actually a
semantic difference in the API in that previously you would get a
brand new FSDirectory but with this change you would get a ""recycled""
one.  I agree it's safer to not do this.  I will update the comment to
remove the tantalizing reference to a WeakHashMap and then resolve.
Thanks!
","14/Jan/07 22:14;mikemccand;Just updated the javadoc for FSDirectory.DIRECTORIES.","27/Feb/07 18:10;mikemccand;Closing all issues that were resolved for 2.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document the temporary free space requirements of IndexWriter methods,LUCENE-764,12359726,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,mikemccand,mikemccand,mikemccand,03/Jan/07 19:43,27/Feb/07 18:10,30/Sep/19 08:38,05/Jan/07 18:30,2.1,,,,,,,,,,2.1,,,,core/index,,,0,,,,"Just opening an issue to track fixes to javadocs around Directory
space usage of optimize(), addIndexes(*), addDocument.

This came out of a recent thread on the users list around unexpectedly
high temporary disk usage during optimize():

  http://www.gossamer-threads.com/lists/lucene/java-user/43475

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,12988,,,Tue Feb 27 18:10:39 UTC 2007,New,,,,,,,"0|i0515j:",27284,,,,,,,,,"27/Feb/07 18:10;mikemccand;Closing all issues that were resolved for 2.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
finalize()-methods of FSDirectory.FSIndexInput and FSDirectory.FSIndexOutput try to close already closed file,LUCENE-669,12349375,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,buschmic,buschmic,07/Sep/06 07:46,27/Feb/07 18:10,30/Sep/19 08:38,30/Nov/06 00:08,,,,,,,,,,,2.1,,,,core/store,,,1,,,,"Hi all,

I found a small problem in FSDirectory: The finalize()-methods of FSDirectory.FSIndexInput and FSDirectory.FSIndexOutput try to close the underlying file. This is not a problem unless the file has been closed before by calling the close() method. If it has been closed before, the finalize method throws an IOException saying that the file is already closed. Usually this IOException would go unnoticed, because the GarbageCollector, which calls finalize(), just eats it. However, if I use the Eclipse debugger the execution of my code will always be suspended when this exception is thrown.

Even though this exception probably won't cause problems during normal execution of Lucene, the code becomes cleaner if we apply this small patch. Might this IOException also have a performance impact, if it is thrown very frequently?

I attached the patch which applies cleanly on the current svn HEAD. All testcases pass and I verfied with the Eclipse debugger that the IOException is not longer thrown.",,,,,,,,,,,,,,,,"26/Nov/06 06:56;michaelbusch;FSDirectory_close_file2.patch;https://issues.apache.org/jira/secure/attachment/12345670/FSDirectory_close_file2.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2006-11-10 16:07:07.0,,,false,,,,,,,,,,,,,,,13083,,,Tue Feb 27 18:10:34 UTC 2007,Patch Available,,,,,,,"0|i051qn:",27379,,,,,,,,,"10/Nov/06 16:07;mikemccand;This patch looks good to me.  It still applies cleanly to the current [Nov 10 2006] svn head, and I think there's very little risk.  I think it makes sense to guard against double-closing.","10/Nov/06 23:31;otis;Looks fine to me.

Maybe change this a bit:
   public void close() throws IOException {
-    if (!isClone)
-      file.close();
+    if (!isClone) {
+      if (file != null) {
+        file.close();
+        file = null;
+      }
+    }
   }

That if (file != null) block could be replaced with closeFile() call, I think.

Also, what was closing the file when you run this in Eclipse?","11/Nov/06 00:35;michaelbusch;The method closeFile() belongs to FSDirectory.FSIndexOutput, so I can't call it in FSDirectory.FSIndexInput.close(). (This is hard to see if you just look at the patch file). 

I added the method closeFile() to FSDirectory.FSIndexOutput, because the behaviour of finalize() and close() is slightly different: finalize() simply closes the file, whereas close() calls super.close() first and closes the file then. I didn't want to change this behavior, thus I can't just call close() from finalize().

But now I am actually wondering if this behavior is correct. super.close() triggers a flush of the buffer. So in the current Lucene code, FSDirectory.FSIndexOutput.close() triggers a flush, but FSDirectory.FSIndexOutput.finalize() doesn't. Shouldn't we call flush also inside finalize() surrounded by try/catch?","26/Nov/06 06:56;michaelbusch;Since the new testcase TestStressIndexing has been added as part of the lockless commits I see this test failing regularly. In ten runs it failed four times with the following exception:

    [junit] Testsuite: org.apache.lucene.index.TestStressIndexing
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 33.338 sec
    [junit] ------------- Standard Output ---------------
    [junit] java.io.IOException: The handle is invalid.
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] java.io.IOException: The handle is invalid.
    [junit] at java.io.RandomAccessFile.close0(Native Method)
    [junit] at java.io.RandomAccessFile.close(RandomAccessFile.java:573)
    [junit] at org.apache.lucene.store.FSIndexInput.close(FSDirectory.java:537)
    [junit] at org.apache.lucene.index.CompoundFileReader.close(CompoundFileReader.java:111)
    [junit] at org.apache.lucene.index.SegmentReader.doClose(SegmentReader.java:252)
    [junit] at org.apache.lucene.index.IndexReader.close(IndexReader.java:637)
    [junit] at org.apache.lucene.index.MultiReader.doClose(MultiReader.java:235)
    [junit] at org.apache.lucene.index.IndexReader.close(IndexReader.java:637)
    [junit] at org.apache.lucene.search.IndexSearcher.close(IndexSearcher.java:74)
    [junit] at org.apache.lucene.index.TestStressIndexing$SearcherThread.run(TestStressIndexing.java:101)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testStressIndexAndSearching(org.apache.lucene.index.TestStressIndexing):	FAILED
    [junit] hit unexpected exception in search2
    [junit] junit.framework.AssertionFailedError: hit unexpected exception in search2
    [junit] at org.apache.lucene.index.TestStressIndexing.runStressTest(TestStressIndexing.java:140)
    [junit] at org.apache.lucene.index.TestStressIndexing.testStressIndexAndSearching(TestStressIndexing.java:161)
    [junit] Test org.apache.lucene.index.TestStressIndexing FAILED
	
It appears to be the same problem for which I opened this issue. After applying the patch I did not see the test failing anymore (I ran it about 20 times...)

I attach a new version of this patch. Now FSDirectory.FSIndexOutput.finalize() simply calls close() which triggers a flush of the buffer. I don't see a reason not do that. Anybody does?

All unit tests pass with the new patch.","26/Nov/06 11:40;mikemccand;Hmmm.  Michael, how does the exception in this unit test tie into this issue?  Ie, I thought this issue was that only finalize would be doing a double-close?  I'm confused how the two are connected (it's awesome that your patch fixes this, but I'd like to understand why!).","28/Nov/06 08:25;michaelbusch;Mike,

you are right, it is very weird that 1) this problem happens and 2) my patch fixes it. It took me quite a while to figure out what the real problem is but I think I have at least a guess now. 

The problem again only happens on an IBM JVM. It happens rarely with your unmodified TestStressIndexing. If I change this testcase so that is does not perform the stress test on a RAMDirectory but only on a FSDirectory it happens almost always. This is VERY weird, because I did not change the stress test at all... I just commented the lines

//    // First in a RAM directory:
//    Directory directory = new RAMDirectory();
//    runStressTest(directory);
//    directory.close();

and suddenly every test fails with the IOException ""The handle is invalid"". So that makes me think it has something to do with garbage collection and the finalize() methods.

Now if I just comment out the finalize() method of FSDirectory.FSIndexOutput the test passes. This is even stranger, because the IOException occurred in FSDirectory.FSIndexInput.close(), not in FSIndexOutput, so in a different class which uses hence a different file descriptor.

So I checked how java.io.FileDescriptor is implemented: It simply wraps an int value. I can't see how these int values are computed, because the open() method of RandomAccessFile is native. 

So I believe the following happens:
(1) FSIndexOutput uses a FileDescriptor with value x
(2) FSIndexOutput.close() is called, so the underlying file is being closed
(3) A new FSIndexInput instance is created by a searcher thread. This opens a RandomAccessFile. Because FileDescriptor x is not in use anymore, x is used as the value for the new FileDescriptor.
(4) Now garbage collection kicks in. It removes the old instance of FSIndexOutput for which close() has been called already. So the garbage collector calls finalize() which calls RandomAccessFile.close() again which still uses the descriptor with value x. So this call of close() actually closes the file used by the IndexInput instance created in (3).
(5) FSIndexInput.close() is called and tries to close the file which has been closed already in (4) and thus the IOException occurs.

So it seems to me that the IBM JVM makes file descriptor values available after a file has been closed, whereas Sun waits until the FileDescriptor instance is destroyed. This might be a bug in the JVM, but since this patch is very simple we could just use it to be on the safe side.

Do you think this makes sense? Or does anybody have a better idea why commenting out the finalize() method in FSIndexOutput prevents FSIndexInput.close() from throwing the IOException? ","28/Nov/06 17:38;mikemccand;Michael, which OS are you seeing the exception on?  I'm trying to repro on Linux w/ IBM's JVM 1.5.0 with no success.","28/Nov/06 19:00;michaelbusch;I'm seeing the problem on Windows XP SP2, IBM JVM 1.5 SR3. I'm running the tests in eclipse.","28/Nov/06 20:11;mikemccand;OK I will try to repro.

In the meantime, I like your theory above!  It seems very plausible that the 2nd close (during finalize) could [incorrectly] close what was in fact a newly opened descriptor (in use elsewhere).  This also means this bug is more serious that I had thought (I thought it would just throw exceptions up to the GC).

One way to be sure this theory is true is to instrument the finalize() to see that indeed it called close for the second time, and, the close succeeded (instead of throwing the original exception you saw).  Ie, if this event occurs and then corresponds to the above exception in the TestStressIndexing unit test, then we've got this explained, and, it's quite serious since in production this could in theory result in errant IOExceptions like the one above.","29/Nov/06 06:09;michaelbusch;Mike,

I tried to add some debug to FSIndexOutput and made another observation which makes this issue even odder! I added a boolean variable to FSIndexOutput with the name isOpen. I don't do anything with this variable, I just change the close() method from:

  public void close() throws IOException {
    super.close();
    file.close();
  }

to 

  public void close() throws IOException {
    super.close();
    file.close();
    isOpen = false;
  }

and suddenly the problem disappears! Now I change close() to

  public void close() throws IOException {
    super.close();
    isOpen = false;
    file.close();
  }

and the IOException occurs again. Notice that I don't use isOpen anywhere else in the code. So it seems that the problem only occurs if file.close() is the last instruction in close(). I bet this is a JVM bug, maybe the compiler makes some kind of optimization (maybe early freeing up the resources of the method's context that are on the stack while the last instruction of the method is being executed). So I'm not completely sure what the real problem is, but I'm pretty sure it is a JVM bug.
","29/Nov/06 23:28;mikemccand;Ugh!  This bug is clearly a heisenbug.

OK, I can also reproduce this on Windows when I use the IBM 1.5.0 JRE.
I can't repro with the Sun 1.5.0_07 JRE.

When I apply your patch, the IOException goes away.

Furthermore, my best efforts to get a standalone test to show the
error have failed.  I don't understand what precise tickling is
required to get the IOException to happen.

Finally, I found this spooky very recent thread on java-user that
looks very much like this error (and was never resolved):

    http://www.gossamer-threads.com/lists/lucene/java-user/40357

I think in this case it was on Sun's JRE.  So I tried the test using
""java -server"" but it didn't fail.  Sigh.

My conclusion is: double-closing a RandomAccessFile is dangerous!  And
quite possibly can cause problems for ""real"" use cases (ie, not just
when testing under Eclipse).

I will commit this patch.

I made a couple of tiny changes: changed the name to ""isOpen"" and
moved up the ""isOpen = true"" to be right after the file is actually
opened.

I also like the change to flush the buffer on finalize (if the file is
still open).

Thanks Michael!
","30/Nov/06 00:46;michaelbusch;Wow that was a tough one!

Thanks for trying so hard to reproduce it, Mike. And thanks for committing, the small changes you made to my patch sound good to me!","06/Feb/07 16:23;rnewson;
The close() method in RandomAccessFile is defined not to throw IOException if it's merely closed twice. The bug here is with the IBM JDK and not Lucene. 

This stanza;

final RandomAccessFile raf = new RandomAccessFile(""/tmp/raf"", ""rw"");
		for (int i = 0; i < 1000; i++) {
			raf.close();
		}

should run fine everywhere, according to Javadocs;

file:///home/rnewson/Documents/jdk-1.5/api/java/io/Closeable.html#close()
""Closes this stream and releases any system resources associated with it. If the stream is already closed then invoking this method has no effect.""

This behavior was clarified with the introduction of the Closeable interface in 1.5, so perhaps IBM are not to blame for this. ","27/Feb/07 18:10;mikemccand;Closing all issues that were resolved for 2.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FuzzyQuery should not be final,LUCENE-657,12348039,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,steven_parkes,neunand,neunand,16/Aug/06 00:30,27/Feb/07 18:10,30/Sep/19 08:38,26/Oct/06 03:49,,,,,,,,,,,2.1,,,,core/search,,,1,,,,"I am trying to extend the FuzzyQuery to further filter the TermEnum. (I am indexing stem forms and original forms, but I only want to match original forms with a fuzzy term, otherwise I get to much noise). However, FuzzyQuery is a final class and I cannot extend it.  

As discussed in the mailing list (http://www.gossamer-threads.com/lists/lucene/java-dev/38756), we want to make the private variables and inner classes protected.

I am attaching a patch for FuzzyQuery.java that implements this. I ran all unit tests and they passed without errors.

Andreas.",all platforms,,,,,,,,,,,,,,,"25/Oct/06 18:07;steven_parkes;LUCENE-657.patch;https://issues.apache.org/jira/secure/attachment/12343642/LUCENE-657.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2006-10-19 22:41:56.0,,,false,,,,,,,,,,,,,,,13095,,,Tue Feb 27 18:10:34 UTC 2007,Patch Available,,,,,,,"0|i051tb:",27391,,,,,,,,,"19/Oct/06 22:41;steven_parkes;Anybody worried about this? Seems like a very nominal change. Any reason we would want to inhibit people from extended a query class like FuzzyQuery? ","20/Oct/06 21:27;yseeley@gmail.com;Making the class non-final is fine.

As far as changing members to protected, I'm not opposed to opening something up a little more so the very expert users don't have to copy the class (although that is often an option).  In return, those expert users shouldn't complain if the internals change ;-)","20/Oct/06 21:50;steven_parkes;Good point on the members.

There are public getter methods and the member are set from ctor, so the only reason for making them protected is to allow them to be set outside the ctor, right? Is that really necessary, to change them after construction?

I have a tendency to prefer protected setters over protected fields, but there is precedence for this, e.g, BooleanQuery.minNrShouldMatch, so I don't think that part is exceptional.","25/Oct/06 18:07;steven_parkes;This version of the patch omits making the member variables public. FuzzyQuery is no longer final and some of the nested classes are made protected, but the member variables have getters so shouldn't need to be made public.","26/Oct/06 03:49;otis;Applied in the trunk, thanks.","27/Feb/07 18:10;mikemccand;Closing all issues that were resolved for 2.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant test won't run in 'out of the box' installation,LUCENE-787,12361405,,Bug,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,mkrogemann,mkrogemann,28/Jan/07 00:18,31/Jan/07 22:27,30/Sep/19 08:38,31/Jan/07 22:27,2.0.0,,,,,,,,,,,,,,,,,0,,,,one possible solution would be to remove 'lib' from the junit.classpath,,,,,,,,,,,LUCENE-717,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-01-31 22:27:49.971,,,false,,,,,,,,,,,,,,,12966,,,Wed Jan 31 22:27:49 UTC 2007,New,,,,,,,"0|i0510f:",27261,,,,,,,,,"31/Jan/07 22:27;hossman;already fixed in trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove usage of deprecated method Document.fields(),LUCENE-726,12356520,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,michaelbusch,michaelbusch,michaelbusch,24/Nov/06 01:38,27/Nov/06 04:05,30/Sep/19 08:38,27/Nov/06 04:05,,,,,,,,,,,2.1,,,,core/index,,,0,,,,"The classes DocumentWriter, FieldsWriter, and ParallelReader use the deprecated method Document.fields(). This simple patch changes these three classes to use Document.getFields() instead.

All unit tests pass.",,,,,,,,,,,,,,,,"24/Nov/06 01:40;michaelbusch;deprecation.patch;https://issues.apache.org/jira/secure/attachment/12345583/deprecation.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,13026,,,Mon Nov 27 04:05:30 UTC 2006,New,Patch Available,,,,,,"0|i051dz:",27322,,,,,,,,,"27/Nov/06 04:05;michaelbusch;Thanks Otis for committing this!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index File Format - Example for frequency file .frq is wrong,LUCENE-706,12354647,,Improvement,Resolved,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,gsingers,doronc,doronc,03/Nov/06 05:40,06/Nov/06 02:21,30/Sep/19 08:38,06/Nov/06 02:21,,,,,,,,,,,,,,,general/website,,,0,,,,"Reported by Johan Stuyts - http://www.nabble.com/Possible-documentation-error--p7012445.html - 

Frequency file example says: 

     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 
         15, 22, 3 

It should be: 

     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 
         15, 8, 3 


",not applicable,,,,,,,,,,,,,,,"03/Nov/06 05:51;doronc;file-format-frq-example.patch;https://issues.apache.org/jira/secure/attachment/12344262/file-format-frq-example.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2006-11-03 13:22:44.0,,,false,,,,,,,,,,,,,,,13046,,,Mon Nov 06 02:21:13 UTC 2006,New,Patch Available,,,,,,"0|i051if:",27342,,,,,,,,,"03/Nov/06 05:51;doronc;example fixed","03/Nov/06 13:22;gsingers;Just to double check the math:
From the Website:
""DocDelta determines both the document number and the frequency. In particular, DocDelta/2 is the difference between this document number and the previous document number (or zero when this is the first document in a TermFreqs). When DocDelta is odd, the frequency is one. When DocDelta is even, the frequency is read as another VInt.""

So, 15 is the correct start since 15 /2 as an int is 7 and the frequency is one.  Then the difference between doc 7 and 11 is 4, so the next value should be 8 (since DocDelta/2 = 11 - 7, which is even, meaning the frequency is the next VInt, in this case 3, so I would concur.

","03/Nov/06 18:41;doronc;Right - 

   15  =  2 * 7 + 1    --> doc 7 with freq 1 
    8   =  2 * (11 - 7) --> doc 11 with frequency > 1 
    3                --> frequency = 3 for doc 11 

.frq file actual content for similar case also agrees with that, it starts like this (Hex):

     0D 08  03 01 03 

(note: Hex:  0D = 15.)


","03/Nov/06 18:50;sarowe;Hex: 0D is NOT the same as decimal 15.  0Dh = 13d.  15d = 0Fh.","03/Nov/06 19:03;doronc;Right, sorry, copied that hex data from an .frq of an index with a different example, where the frequencies were 1 in doc 6 and 3 in doc 10, so there you would get 2 * 6 + 1 = 13.

For the correct example of freq 1 in doc 7 and 3 in doc 11 the .frq content is  0F 08 03  as it should be. 

(Meaning that the documentatin should still be fixed...;-)
","06/Nov/06 02:21;gsingers;Applied.  Thanks Johan and Doron.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refrase javadoc 1st sentence for IndexReader.deleteDocuments,LUCENE-684,12353212,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,paul.elschot@xs4all.nl,paul.elschot@xs4all.nl,15/Oct/06 10:31,15/Oct/06 14:22,30/Sep/19 08:38,15/Oct/06 14:22,,,,,,,,,,,2.1,,,,core/index,,,0,,,,,,,,,,,,,,,,,,,,"15/Oct/06 10:33;paul.elschot@xs4all.nl;IndexReader1.patch;https://issues.apache.org/jira/secure/attachment/12342953/IndexReader1.patch",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2006-10-15 14:22:35.0,,,false,,,,,,,,,,,,,,,13068,,,Sun Oct 15 14:22:35 UTC 2006,,,,,,,,"0|i051nb:",27364,,,,,,,,,"15/Oct/06 10:33;paul.elschot@xs4all.nl;Mention that the term should have been indexed for IndexReader.deleteDocuments to work.","15/Oct/06 14:22;lucenebugs@danielnaber.de;Thanks, patch applied.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Promote solr's PrefixFilter into Java Lucene's core,LUCENE-676,12350979,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,yseeley@gmail.com,vajda,vajda,25/Sep/06 19:25,05/Oct/06 21:10,30/Sep/19 08:38,05/Oct/06 21:10,2.1,,,,,,,,,,2.1,,,,core/search,,,1,,,,"Solr's PrefixFilter class is not specific to Solr and seems to be of interest to core lucene users (PyLucene in this case).
Promoting it into the Lucene core would be helpful.",,,,,,,,,,,,,,,,"25/Sep/06 19:27;vajda;PrefixFilter.java;https://issues.apache.org/jira/secure/attachment/12341662/PrefixFilter.java","26/Sep/06 17:50;vajda;TestPrefixFilter.java;https://issues.apache.org/jira/secure/attachment/12341735/TestPrefixFilter.java",,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2006-09-26 07:07:33.0,,,false,,,,,,,,,,,,,,,13076,,,Thu Oct 05 21:10:25 UTC 2006,,,,,,,,"0|i051p3:",27372,,,,,,,,,"25/Sep/06 19:27;vajda;Attached is a version of PrefixFilter that could be added to the Lucene core as submitted by Yura Smolsky, a PyLucene user.","26/Sep/06 07:07;hossman;Even though i use PrefixFilter on a daily basis in Solr, and i am confident of it's correctness, I don't think anything should be commited/promoted to the Lucene code base without some Unit Tests.

(PrefixFilter is exercised by a few tests in the Solr code base at the moment but they aren't portable because they go through the SolrCore)","26/Sep/06 17:50;vajda;Here is another attachment by Yura providing the request unit test.","05/Oct/06 21:10;yseeley@gmail.com;The test was incorrect.  I fixed it, added some more tests, and committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixed Spelling mailinglist.xml,LUCENE-649,12347796,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,simonwillnauer,simonwillnauer,11/Aug/06 15:20,16/Aug/06 18:32,30/Sep/19 08:38,16/Aug/06 18:32,2.1,,,,,,,,,,,,,,general/website,,,0,,,,"Just fixed some spelling in the mailinglist.xml in /java/trunk/xdocs



",,,,,,,,,,,,,,,,"11/Aug/06 15:20;simonwillnauer;ASF.LICENSE.NOT.GRANTED--mailinglist_xml.diff;https://issues.apache.org/jira/secure/attachment/12338707/ASF.LICENSE.NOT.GRANTED--mailinglist_xml.diff",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2006-08-16 18:32:05.0,,,false,,,,,,,,,,,,,,,13103,,,Wed Aug 16 18:32:05 UTC 2006,,,,,,,,"0|i051v3:",27399,,,,,,,,,"16/Aug/06 18:32;lucenebugs@danielnaber.de;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
results.jsp in luceneweb.war uses unknown parse-Method,LUCENE-630,12346265,,Bug,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,philipreimer,philipreimer,19/Jul/06 15:55,19/Jul/06 18:28,30/Sep/19 08:38,19/Jul/06 18:28,2.0.0,,,,,,,,,,2.1,,,,modules/examples,,,0,,,,"results.jsp in luceneweb.war demo throws JasperException:

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)

I think, the code in line 81 of results.jsp should maybe look like the following ?

QueryParser qp = new QueryParser(""contents"", analyzer);
query = qp.parse(queryString);","Windows XP Pro and Linux (Ubuntu 6.06 TLS)
Tomcat 5.5
Sun Java 1.5_07",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2006-07-19 18:28:54.0,,,false,,,,,,,,,,,,,,,13123,,,Wed Jul 19 18:28:54 UTC 2006,,,,,,,,"0|i051zb:",27418,,,,,,,,,"19/Jul/06 16:22;philipreimer;This refers to the results.jsp coming with the war in the binary distribution (lucene-2.0.0.zip and lucene-2.0.0.tar.gz)","19/Jul/06 18:28;lucenebugs@danielnaber.de;This has been fixed some time ago (after the 2.0 release).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove superfluous comment in MMapDirectory.java,LUCENE-513,12329956,,Improvement,Closed,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,Fixed,,paul.elschot@xs4all.nl,paul.elschot@xs4all.nl,08/Mar/06 15:47,17/Mar/06 07:51,30/Sep/19 08:38,17/Mar/06 07:49,2.0.0,,,,,,,,,,2.0.0,,,,core/store,,,0,,,,"See title, and I prefer my name to be removed from the source code.",,,,,,,,,,,,,,,,"08/Mar/06 15:48;paul.elschot@xs4all.nl;MMapDirPatch1.txt;https://issues.apache.org/jira/secure/attachment/12323916/MMapDirPatch1.txt",,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2006-03-17 07:49:46.0,,,false,,,,,,,,,,,,,,,13237,,,Fri Mar 17 07:49:46 UTC 2006,,,,,,,,"0|i052on:",27532,,,,,,,,,"17/Mar/06 07:49;otis;Applied.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
